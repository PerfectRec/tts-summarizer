[
  {
    "page": 1,
    "text": "This article was downloaded by: [73.109.109.53] On: 03 May 2022, At: 10:30\nPublisher: Institute for Operations Research and the Management Sciences (INFORMS)\nINFORMS is located in Maryland, USA\n  MARKETING                               Marketing Science\n  SCIENCE\n                                          Publication details, including instructions for authors and subscription information:\n                                          http://pubsonline.informs.org\n\n                                          Reputation Inflation\n                                          Apostolos Filippas, John J. Horton, Joseph M. Golden\n\nTo cite this article:\nApostolos Filippas, John J. Horton, Joseph M. Golden (2022) Reputation Inflation. Marketing Science\n    Published online in Articles in Advance 03 May 2022\n. https://doi.org/10.1287/mksc.2022.1350\n\nFull terms and conditions of use: https://pubsonline.informs.org/Publications/Librarians-Portal/PubsOnLine-Terms-and-\nConditions\n    This article may be used only for the purposes of research, teaching, and/or private study. Commercial use\nor systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher\napproval, unless otherwise noted. For more information, contact permissions@informs.org.\n    The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness\nfor a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or\ninclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or\nsupport of claims made of that product, publication, or service.\n    Copyright © 2022, INFORMS\n\nPlease scroll down for article—it is on subsequent pages\nCforms\nWith 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.)\nand analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual\nprofessionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to\ntransform strategic visions and achieve better outcomes.\nFor more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",
    "md": "# Reputation Inflation\n\n# Apostolos Filippas, John J. Horton, Joseph M. Golden\n\nTo cite this article:\n\nApostolos Filippas, John J. Horton, Joseph M. Golden (2022) Reputation Inflation. Marketing Science Published online in Articles in Advance 03 May 2022. https://doi.org/10.1287/mksc.2022.1350\n\nFull terms and conditions of use: https://pubsonline.informs.org/Publications/Librarians-Portal/PubsOnLine-Terms-and-Conditions\n\nThis article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org.\n\nThe Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service.\n\nCopyright © 2022, INFORMS\n\nPlease scroll down for article—it is on subsequent pages\n\nWith 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.\n\nFor more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",
    "images": [
      {
        "name": "img_p0_1.png",
        "height": 389,
        "width": 291,
        "x": 21.6,
        "y": 75.68999400000001,
        "original_width": 179,
        "original_height": 239
      },
      {
        "name": "img_p0_2.png",
        "height": 73,
        "width": 283,
        "x": 21.6,
        "y": 586.5870269999999,
        "original_width": 497,
        "original_height": 127
      }
    ],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Reputation Inflation",
        "md": "# Reputation Inflation",
        "bBox": {
          "x": 172,
          "y": 155,
          "w": 143.2,
          "h": 16
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Apostolos Filippas, John J. Horton, Joseph M. Golden",
        "md": "# Apostolos Filippas, John J. Horton, Joseph M. Golden",
        "bBox": {
          "x": 172,
          "y": 179,
          "w": 238.2100000000001,
          "h": 10
        }
      },
      {
        "type": "text",
        "value": "To cite this article:\n\nApostolos Filippas, John J. Horton, Joseph M. Golden (2022) Reputation Inflation. Marketing Science Published online in Articles in Advance 03 May 2022. https://doi.org/10.1287/mksc.2022.1350\n\nFull terms and conditions of use: https://pubsonline.informs.org/Publications/Librarians-Portal/PubsOnLine-Terms-and-Conditions\n\nThis article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org.\n\nThe Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service.\n\nCopyright © 2022, INFORMS\n\nPlease scroll down for article—it is on subsequent pages\n\nWith 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.\n\nFor more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",
        "md": "To cite this article:\n\nApostolos Filippas, John J. Horton, Joseph M. Golden (2022) Reputation Inflation. Marketing Science Published online in Articles in Advance 03 May 2022. https://doi.org/10.1287/mksc.2022.1350\n\nFull terms and conditions of use: https://pubsonline.informs.org/Publications/Librarians-Portal/PubsOnLine-Terms-and-Conditions\n\nThis article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org.\n\nThe Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service.\n\nCopyright © 2022, INFORMS\n\nPlease scroll down for article—it is on subsequent pages\n\nWith 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.\n\nFor more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",
        "bBox": {
          "x": 21,
          "y": 87.05796560411312,
          "w": 564.8299999999998,
          "h": 16
        }
      }
    ]
  },
  {
    "page": 2,
    "text": "                                                                                                                                     MARKETING SCIENCEArticles in Advance, pp. 1–13\n              http://pubsonline.informs.org/journal/mksc                                                                ISSN 0732-2399 (print), ISSN 1526-548X      (online)\n\n              Reputation Inflation\n              Apostolos Filippas,a, * John J. Horton,b,c Joseph M. Goldend\n              a Gabelli School of Business, Fordham University, New York, New York 10023; b MIT Sloan School of Management, Massachusetts Institute of\n              Technology, Cambridge, Massachusetts 02139;c National Bureau of Economic Research, Cambridge, Massachusetts 02138; d PerfectRec.com\n              *Corresponding author\n              Contact: apostolosfilippas@gmail.com,        https://orcid.org/0000-0003-4593-3333 (AF); john.joseph.horton@gmail.com,\n                 https://orcid.org/0000-0001-5426-0156 (JJH); jgolden9@gmail.com,              https://orcid.org/0000-0002-2068-3676 (JMG)\n\n              Received: October 4, 2019                     Abstract.    We show that average buyer ratings of sellers have grown substantially more\n              Revised: April 22, 2020; October 28, 2020;    positive over time in five online marketplaces. Although this increase could by explained\n              June 8, 2021                                  by (i) marketplace improvements that increased rater satisfaction, it could also be caused\n              Accepted: September 17, 2021                  by (ii) “reputation inflation,” with raters giving higher ratings without being more satis-\n              Published Online in Articles in Advance:      fied. We present a method to decompose the growth in average ratings into components\n              May 3, 2022                                   attributable to these two reasons. Using this method in one marketplace where we have\n              https://doi.org/10.1287/mksc.2022.1350        extensive transaction-level data, we find that much of the observed increase in ratings is\n                                                            attributable to reputation inflation. We discuss the negative informational implications of\n              Copyright: © 2022 INFORMS                     reputation inflation and consider the likely causes.\n                                                            History: Avi Goldfarb served as the senior editor and Dina Mayzlin served as associate editor for this article.\n                                                            Funding: A. Filippas gratefully acknowledges the support of the Alexander S. Onassis Public Benefit\n                                                               Foundation.\n                                                            Supplemental Material:  Data and the online appendix are available at https://doi.org/10.1287/mksc.\n                                                               2022.1350.\n\n              Keywords:       online marketplaces • reputation systems • market design\n\n1. Introduction\nScores of various kinds—credit scores, school grades,\nrestaurant and         film star reviews, restaurant hygiene\nscores,    Better     Business      Bureau      ratings—have         long\nbeen important sources of information for market par-\nticipants. A large literature documents the economic\n\nimportance of such scores (Resnick et al. 2000, 2006;\nJin and Leslie 2003; Ghose et al. 2014; Mayzlin et al.\n2014; Luca 2016; Luca and Zervas 2016), as well as\nsome of their limitations (Dellarocas and Wood 2008,\nTadelis and Zettelmeyer 2015, Tadelis 2016, Hu et al.\n2017). As more of economic and social life has become\ncomputer mediated, opportunities to generate and apply\nnew kinds of scores—particularly in marketplace con-\ntexts—have proliferated, as has the number of individu-\nals and businesses subject to these reputation systems\n(Levin 2011, Hall and Krueger 2018, Katz and Krueger\n2019).     Designing       effective     reputation       systems      has\nbecome a first-order question in the digital economy.\n   In online marketplaces, reputations are typically\ncalculated from numerical feedback scores left by past\ntrading partners. As many have noted, the distribu-\ntion of these feedback scores in various online market-\nplaces     seems      implausibly       rosy.     For    example,      the\nmedian seller on eBay has a score of 100% positive\nfeedback ratings, and the tenth percentile is 98.21%\npositive feedback ratings (Nosko and Tadelis 2015).\nOn Uber and Lyft, it is widely known that anything\nless than     five stars is considered bad feedback: Athey\net al. (2019) found that nearly 90% of UberX Chicago\ntrips in early 2017 had a perfect               five-star rating. We\nshow in this paper that 85% of rated workers in an\nonline labor market—which we call our focal market-\nplace—received a perfect rating in recent years.1\n                                                                    How-\never, we also show that feedback scores did not start out\nthis positively skewed: the fraction of workers receiving\na perfect    five-star rating grew from 33% to 85% in just\nsix years. Increasing average feedback scores in market-\nplaces seem to be commonplace: we collected data from\nfive different online marketplaces, and each exhibits a\nmarked increase in average feedback scores over time.\n   Rising feedback scores can be caused by two dis-\ntinct—but not mutually exclusive—reasons: (1) raters\nare becoming more satisfied or (2) raters are rating\nhigher despite not being more satisfied. The first pos-\nsibility—more satisfied raters giving higher scores—is\ndue to improvements in market fundamentals, such\nas   better     marketplace        features,     better     cohorts      of\nbuyers/sellers joining the platform (or low-quality\nbuyers/sellers exiting the platform), and lower-priced\nproducts. Improvements are obviously welcome, but\nwith a fixed rating scale, they can lead to pooling of\nfeedback scores at the highest possible score. The sec-\nond possibility—raters giving higher scores despite\n\n                                                                                            1",
    "md": "# Reputation Inflation\n\nApostolos Filippas, *  John J. Horton,  Joseph M. Goldend\n\na Gabelli School of Business, Fordham University, New York, New York 10023; b MIT Sloan School of Management, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139; c National Bureau of Economic Research, Cambridge, Massachusetts 02138; d PerfectRec.com\n\n*Corresponding author\n\nContact: apostolosfilippas@gmail.com, https://orcid.org/0000-0003-4593-3333 (AF); john.joseph.horton@gmail.com, https://orcid.org/0000-0001-5426-0156 (JJH); jgolden9@gmail.com, https://orcid.org/0000-0002-2068-3676 (JMG)\n\nReceived: October 4, 2019\n\nRevised: April 22, 2020; October 28, 2020; June 8, 2021\n\nAccepted: September 17, 2021\n\nPublished Online in Articles in Advance: May 3, 2022\n\nhttps://doi.org/10.1287/mksc.2022.1350\n\nCopyright: © 2022 INFORMS\n\nHistory: Avi Goldfarb served as the senior editor and Dina Mayzlin served as associate editor for this article.\n\nFunding: A. Filippas gratefully acknowledges the support of the Alexander S. Onassis Public Benefit Foundation.\n\nSupplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mksc.2022.1350.\n\nKeywords: online marketplaces • reputation systems • market design\n\n# 1. Introduction\n\nScores of various kinds—credit scores, school grades, restaurant and film star reviews, restaurant hygiene scores, Better Business Bureau ratings—have long been important sources of information for market participants. A large literature documents the economic importance of such scores (Resnick et al. 2000, 2006; Jin and Leslie 2003; Ghose et al. 2014; Mayzlin et al. 2014; Luca 2016; Luca and Zervas 2016), as well as some of their limitations (Dellarocas and Wood 2008, Tadelis and Zettelmeyer 2015, Tadelis 2016, Hu et al. 2017). As more of economic and social life has become computer mediated, opportunities to generate and apply new kinds of scores—particularly in marketplace contexts—have proliferated, as has the number of individuals and businesses subject to these reputation systems (Levin 2011, Hall and Krueger 2018, Katz and Krueger 2019). Designing effective reputation systems has become a first-order question in the digital economy.\n\nIn online marketplaces, reputations are typically calculated from numerical feedback scores left by past trading partners. As many have noted, the distribution of these feedback scores in various online marketplaces seems implausibly rosy. For example, the median seller on eBay has a score of 100% positive feedback ratings, and the tenth percentile is 98.21% positive feedback ratings (Nosko and Tadelis 2015). On Uber and Lyft, it is widely known that anything less than five stars is considered bad feedback: Athey et al. (2019) found that nearly 90% of UberX Chicago trips in early 2017 had a perfect five-star rating. We show in this paper that 85% of rated workers in an online labor market—which we call our focal marketplace—received a perfect rating in recent years. However, we also show that feedback scores did not start out this positively skewed: the fraction of workers receiving a perfect five-star rating grew from 33% to 85% in just six years. Increasing average feedback scores in marketplaces seem to be commonplace: we collected data from five different online marketplaces, and each exhibits a marked increase in average feedback scores over time.\n\nRising feedback scores can be caused by two distinct—but not mutually exclusive—reasons: (1) raters are becoming more satisfied or (2) raters are rating higher despite not being more satisfied. The first possibility—more satisfied raters giving higher scores—is due to improvements in market fundamentals, such as better marketplace features, better cohorts of buyers/sellers joining the platform (or low-quality buyers/sellers exiting the platform), and lower-priced products. Improvements are obviously welcome, but with a fixed rating scale, they can lead to pooling of feedback scores at the highest possible score. The second possibility—raters giving higher scores despite not being more satisfied.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Reputation Inflation",
        "md": "# Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 116.87199999999996,
          "w": 150.03169763000002,
          "h": 15.9403
        }
      },
      {
        "type": "text",
        "value": "Apostolos Filippas, *  John J. Horton,  Joseph M. Goldend\n\na Gabelli School of Business, Fordham University, New York, New York 10023; b MIT Sloan School of Management, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139; c National Bureau of Economic Research, Cambridge, Massachusetts 02138; d PerfectRec.com\n\n*Corresponding author\n\nContact: apostolosfilippas@gmail.com, https://orcid.org/0000-0003-4593-3333 (AF); john.joseph.horton@gmail.com, https://orcid.org/0000-0001-5426-0156 (JJH); jgolden9@gmail.com, https://orcid.org/0000-0002-2068-3676 (JMG)\n\nReceived: October 4, 2019\n\nRevised: April 22, 2020; October 28, 2020; June 8, 2021\n\nAccepted: September 17, 2021\n\nPublished Online in Articles in Advance: May 3, 2022\n\nhttps://doi.org/10.1287/mksc.2022.1350\n\nCopyright: © 2022 INFORMS\n\nHistory: Avi Goldfarb served as the senior editor and Dina Mayzlin served as associate editor for this article.\n\nFunding: A. Filippas gratefully acknowledges the support of the Alexander S. Onassis Public Benefit Foundation.\n\nSupplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mksc.2022.1350.\n\nKeywords: online marketplaces • reputation systems • market design",
        "md": "Apostolos Filippas, *  John J. Horton,  Joseph M. Goldend\n\na Gabelli School of Business, Fordham University, New York, New York 10023; b MIT Sloan School of Management, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139; c National Bureau of Economic Research, Cambridge, Massachusetts 02138; d PerfectRec.com\n\n*Corresponding author\n\nContact: apostolosfilippas@gmail.com, https://orcid.org/0000-0003-4593-3333 (AF); john.joseph.horton@gmail.com, https://orcid.org/0000-0001-5426-0156 (JJH); jgolden9@gmail.com, https://orcid.org/0000-0002-2068-3676 (JMG)\n\nReceived: October 4, 2019\n\nRevised: April 22, 2020; October 28, 2020; June 8, 2021\n\nAccepted: September 17, 2021\n\nPublished Online in Articles in Advance: May 3, 2022\n\nhttps://doi.org/10.1287/mksc.2022.1350\n\nCopyright: © 2022 INFORMS\n\nHistory: Avi Goldfarb served as the senior editor and Dina Mayzlin served as associate editor for this article.\n\nFunding: A. Filippas gratefully acknowledges the support of the Alexander S. Onassis Public Benefit Foundation.\n\nSupplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mksc.2022.1350.\n\nKeywords: online marketplaces • reputation systems • market design",
        "bBox": {
          "x": 45,
          "y": 150.87199999999996,
          "w": 490.0389406999999,
          "h": 11.970200000000006
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "1. Introduction",
        "md": "# 1. Introduction",
        "bBox": {
          "x": 45,
          "y": 407.87199999999996,
          "w": 83.95349424,
          "h": 12
        }
      },
      {
        "type": "text",
        "value": "Scores of various kinds—credit scores, school grades, restaurant and film star reviews, restaurant hygiene scores, Better Business Bureau ratings—have long been important sources of information for market participants. A large literature documents the economic importance of such scores (Resnick et al. 2000, 2006; Jin and Leslie 2003; Ghose et al. 2014; Mayzlin et al. 2014; Luca 2016; Luca and Zervas 2016), as well as some of their limitations (Dellarocas and Wood 2008, Tadelis and Zettelmeyer 2015, Tadelis 2016, Hu et al. 2017). As more of economic and social life has become computer mediated, opportunities to generate and apply new kinds of scores—particularly in marketplace contexts—have proliferated, as has the number of individuals and businesses subject to these reputation systems (Levin 2011, Hall and Krueger 2018, Katz and Krueger 2019). Designing effective reputation systems has become a first-order question in the digital economy.\n\nIn online marketplaces, reputations are typically calculated from numerical feedback scores left by past trading partners. As many have noted, the distribution of these feedback scores in various online marketplaces seems implausibly rosy. For example, the median seller on eBay has a score of 100% positive feedback ratings, and the tenth percentile is 98.21% positive feedback ratings (Nosko and Tadelis 2015). On Uber and Lyft, it is widely known that anything less than five stars is considered bad feedback: Athey et al. (2019) found that nearly 90% of UberX Chicago trips in early 2017 had a perfect five-star rating. We show in this paper that 85% of rated workers in an online labor market—which we call our focal marketplace—received a perfect rating in recent years. However, we also show that feedback scores did not start out this positively skewed: the fraction of workers receiving a perfect five-star rating grew from 33% to 85% in just six years. Increasing average feedback scores in marketplaces seem to be commonplace: we collected data from five different online marketplaces, and each exhibits a marked increase in average feedback scores over time.\n\nRising feedback scores can be caused by two distinct—but not mutually exclusive—reasons: (1) raters are becoming more satisfied or (2) raters are rating higher despite not being more satisfied. The first possibility—more satisfied raters giving higher scores—is due to improvements in market fundamentals, such as better marketplace features, better cohorts of buyers/sellers joining the platform (or low-quality buyers/sellers exiting the platform), and lower-priced products. Improvements are obviously welcome, but with a fixed rating scale, they can lead to pooling of feedback scores at the highest possible score. The second possibility—raters giving higher scores despite not being more satisfied.",
        "md": "Scores of various kinds—credit scores, school grades, restaurant and film star reviews, restaurant hygiene scores, Better Business Bureau ratings—have long been important sources of information for market participants. A large literature documents the economic importance of such scores (Resnick et al. 2000, 2006; Jin and Leslie 2003; Ghose et al. 2014; Mayzlin et al. 2014; Luca 2016; Luca and Zervas 2016), as well as some of their limitations (Dellarocas and Wood 2008, Tadelis and Zettelmeyer 2015, Tadelis 2016, Hu et al. 2017). As more of economic and social life has become computer mediated, opportunities to generate and apply new kinds of scores—particularly in marketplace contexts—have proliferated, as has the number of individuals and businesses subject to these reputation systems (Levin 2011, Hall and Krueger 2018, Katz and Krueger 2019). Designing effective reputation systems has become a first-order question in the digital economy.\n\nIn online marketplaces, reputations are typically calculated from numerical feedback scores left by past trading partners. As many have noted, the distribution of these feedback scores in various online marketplaces seems implausibly rosy. For example, the median seller on eBay has a score of 100% positive feedback ratings, and the tenth percentile is 98.21% positive feedback ratings (Nosko and Tadelis 2015). On Uber and Lyft, it is widely known that anything less than five stars is considered bad feedback: Athey et al. (2019) found that nearly 90% of UberX Chicago trips in early 2017 had a perfect five-star rating. We show in this paper that 85% of rated workers in an online labor market—which we call our focal marketplace—received a perfect rating in recent years. However, we also show that feedback scores did not start out this positively skewed: the fraction of workers receiving a perfect five-star rating grew from 33% to 85% in just six years. Increasing average feedback scores in marketplaces seem to be commonplace: we collected data from five different online marketplaces, and each exhibits a marked increase in average feedback scores over time.\n\nRising feedback scores can be caused by two distinct—but not mutually exclusive—reasons: (1) raters are becoming more satisfied or (2) raters are rating higher despite not being more satisfied. The first possibility—more satisfied raters giving higher scores—is due to improvements in market fundamentals, such as better marketplace features, better cohorts of buyers/sellers joining the platform (or low-quality buyers/sellers exiting the platform), and lower-priced products. Improvements are obviously welcome, but with a fixed rating scale, they can lead to pooling of feedback scores at the highest possible score. The second possibility—raters giving higher scores despite not being more satisfied.",
        "bBox": {
          "x": 45,
          "y": 405.87199999999996,
          "w": 493.46753734,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 3,
    "text": "2\nnot being more satisfied—can be described as a kind\nof inflation. This inflation can also cause pooling at\nthe highest score, but is more worrisome because of\nits greater potential to reduce the informativeness and\nstability of the reputation system.\n   If reputation inflation can explain at least some of\nthe increase in average feedback scores, then we would\nexpect a gap to emerge between what raters rate and how\nthey actually feel about a transaction. To explore this pos-\nsibility, we use information obtained by the introduction\nof a parallel and experimental reputation system into our\nfocal marketplace. More specifically, a new feedback ques-\ntion asked employers to rate workers privately. This pri-\nvate feedback was not conveyed to the rated workers, nor\nmade public to future would-be employers. At the same\ntime, raters were still asked to give the status quo public\nfeedback, both written and numerical.\n   We show that raters were far more candid in private,\nwith substantial numbers reporting dissatisfaction pri-\nvately but still assigning a perfect five-star rating pub-\nlicly. This gap suggests that raters are reluctant to give\nnegative feedback publicly because they do not want\nto harm the ratees’ future prospects, either for altruistic\nreasons or because they fear retaliation of some kind.\nWe also show that average private feedback scores\nwere decreasing over the period they were collected,\nbut at the same time, average public feedback scores\nfor the   same transactions were increasing. This diver-\ngence provides some evidence of reputation inflation\non the platform, albeit over a short time window when\nthe private feedback question was asked.\n   To determine how much of the increase in average\nratings is attributable to reputation inflation, we intro-\nduce     a  method       for   decomposing        average      ratings\nincreases into the component that can be explained by\nchanges in satisfaction and the component that can-\nnot. To obtain a point estimate of the effect of infla-\ntion, the method requires an alternative measure of\nrater satisfaction not prone to inflation. Importantly, if\nthe alternative measure is also prone to inflation, the\nmethod yields a lower bound. The method consists of\nlearning the expected value for the actual numerical\nfeedback, conditional upon the alternative measure of\nrater    satisfaction.     Under      some     mild    assumptions\nlikely to be met in practice, this learned conditional\nexpectation function (CEF) can then be applied to new\ntransaction data, predicting what the average score\nshould be given the alternative feedback data. This\nallows one to net out the increase not attributable to\nchanges in marketplace fundamentals.\n   Alternative      measures       of  rater   satisfaction     might\nseem hard to come by, but one measure available in\nmany online marketplaces is the textual feedback that\naccompanies scores on the same transactions. For rea-\nsons we will discuss, textual feedback may be less\n                    Filippas, Horton, and Golden: Reputation Inflation\n            Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\nprone to inflationary pressures. Consistent with this\nview, we show in our focal marketplace that the same\nsentences systematically have higher associated numeri-\ncal feedback scores as time passes. For example, employ-\ners calling the work they received                “terrible”    would\nassign on average a public feedback score of 1.4 stars in\n2008, but they would instead assign 2.4 stars in 2015.\n   Using written feedback as our alternative measure of\nrater satisfaction, we      fit a model that predicts numerical\nfeedback from the text of written feedback. We find that\nmore than 50% of the increase in scores over a six-year\nperiod was due to inflation, with this result being robust\nacross different specifications and training sets. Insofar as\nwritten feedback is also subject to inflation, our approach\nunderstates the extent of reputation inflation. As numerical\nratings are often accompanied by written feedback, this\nmethod can be readily used in other contexts.\n   A natural question is whether the reputation inflation\nwe identify in our focal marketplace—and which likely\noccurs in other marketplaces—matters in practice for\nthe functioning of the reputation system. Although we\ndo not explore this question empirically, there are sev-\neral theoretical reasons why strong inflation in a system\nwith a     fixed, top-censored scale will cause a loss of\ninformation, analogous to the problems with grade\ninflation (Babcock 2010, Butcher et al. 2014).\n   Our key contribution is documenting the extent of\nreputation      inflation    in   a  large    online    marketplace\nby using an approach that accounts for changes in\nplatform fundamentals. Our long-run, whole-system\nperspective is possible because we use data spanning\nover a decade of the operations of the marketplace.\nAlthough we cannot perform the same decomposition\nin other marketplaces, we observe increasing average\nfeedback scores in every marketplace for which we\ncould obtain data, even though none of these market-\nplaces allows tit-for-tat rating behavior (Bolton et al.\n2013). Given that many online marketplaces share the\nsame features as our focal marketplace, this evidence\nsuggests that the problem is widespread.\n   The rest of this paper is organized as follows. Sec-\ntion 2 introduces the empirical setting and documents\nincreasing feedback scores over time across five online\nmarketplaces. Section 3 presents descriptive evidence\non the problem in our focal marketplace. Section 4\nintroduces our decomposition method and applies it.\nSection 5 discusses the causes and implications of rep-\nutation inflation. Section 6 concludes the paper.\n\n2. Empirical Context and Descriptive\n\n     Evidence of Rising Average Feedback\n     in Several Online Marketplaces\nOur focal market is a large online labor market (Horton\n2010). In online labor markets, employers hire workers to",
    "md": "# 2. Empirical Context and Descriptive Evidence of Rising Average Feedback in Several Online Marketplaces\n\nOur focal market is a large online labor market (Horton 2010). In online labor markets, employers hire workers to\n\nnot being more satisfied—can be described as a kind of inflation. This inflation can also cause pooling at the highest score, but is more worrisome because of its greater potential to reduce the informativeness and stability of the reputation system.\n\nIf reputation inflation can explain at least some of the increase in average feedback scores, then we would expect a gap to emerge between what raters rate and how they actually feel about a transaction. To explore this possibility, we use information obtained by the introduction of a parallel and experimental reputation system into our focal marketplace. More specifically, a new feedback question asked employers to rate workers privately. This private feedback was not conveyed to the rated workers, nor made public to future would-be employers. At the same time, raters were still asked to give the status quo public feedback, both written and numerical.\n\nWe show that raters were far more candid in private, with substantial numbers reporting dissatisfaction privately but still assigning a perfect five-star rating publicly. This gap suggests that raters are reluctant to give negative feedback publicly because they do not want to harm the ratees’ future prospects, either for altruistic reasons or because they fear retaliation of some kind. We also show that average private feedback scores were decreasing over the period they were collected, but at the same time, average public feedback scores for the same transactions were increasing. This divergence provides some evidence of reputation inflation on the platform, albeit over a short time window when the private feedback question was asked.\n\nTo determine how much of the increase in average ratings is attributable to reputation inflation, we introduce a method for decomposing average ratings increases into the component that can be explained by changes in satisfaction and the component that cannot. To obtain a point estimate of the effect of inflation, the method requires an alternative measure of rater satisfaction not prone to inflation. Importantly, if the alternative measure is also prone to inflation, the method yields a lower bound. The method consists of learning the expected value for the actual numerical feedback, conditional upon the alternative measure of rater satisfaction. Under some mild assumptions likely to be met in practice, this learned conditional expectation function (CEF) can then be applied to new transaction data, predicting what the average score should be given the alternative feedback data. This allows one to net out the increase not attributable to changes in marketplace fundamentals.\n\nAlternative measures of rater satisfaction might seem hard to come by, but one measure available in many online marketplaces is the textual feedback that accompanies scores on the same transactions. For reasons we will discuss, textual feedback may be less prone to inflationary pressures. Consistent with this view, we show in our focal marketplace that the same sentences systematically have higher associated numerical feedback scores as time passes. For example, employers calling the work they received “terrible” would assign on average a public feedback score of 1.4 stars in 2008, but they would instead assign 2.4 stars in 2015.\n\nUsing written feedback as our alternative measure of rater satisfaction, we fit a model that predicts numerical feedback from the text of written feedback. We find that more than 50% of the increase in scores over a six-year period was due to inflation, with this result being robust across different specifications and training sets. Insofar as written feedback is also subject to inflation, our approach understates the extent of reputation inflation. As numerical ratings are often accompanied by written feedback, this method can be readily used in other contexts.\n\nA natural question is whether the reputation inflation we identify in our focal marketplace—and which likely occurs in other marketplaces—matters in practice for the functioning of the reputation system. Although we do not explore this question empirically, there are several theoretical reasons why strong inflation in a system with a fixed, top-censored scale will cause a loss of information, analogous to the problems with grade inflation (Babcock 2010, Butcher et al. 2014).\n\nOur key contribution is documenting the extent of reputation inflation in a large online marketplace by using an approach that accounts for changes in platform fundamentals. Our long-run, whole-system perspective is possible because we use data spanning over a decade of the operations of the marketplace. Although we cannot perform the same decomposition in other marketplaces, we observe increasing average feedback scores in every marketplace for which we could obtain data, even though none of these marketplaces allows tit-for-tat rating behavior (Bolton et al. 2013). Given that many online marketplaces share the same features as our focal marketplace, this evidence suggests that the problem is widespread.\n\nThe rest of this paper is organized as follows. Section 2 introduces the empirical setting and documents increasing feedback scores over time across five online marketplaces. Section 3 presents descriptive evidence on the problem in our focal marketplace. Section 4 introduces our decomposition method and applies it. Section 5 discusses the causes and implications of reputation inflation. Section 6 concludes the paper.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "2. Empirical Context and Descriptive Evidence of Rising Average Feedback in Several Online Marketplaces",
        "md": "# 2. Empirical Context and Descriptive Evidence of Rising Average Feedback in Several Online Marketplaces",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 215.23483836000003,
          "h": 11.955100000000016
        }
      },
      {
        "type": "text",
        "value": "Our focal market is a large online labor market (Horton 2010). In online labor markets, employers hire workers to\n\nnot being more satisfied—can be described as a kind of inflation. This inflation can also cause pooling at the highest score, but is more worrisome because of its greater potential to reduce the informativeness and stability of the reputation system.\n\nIf reputation inflation can explain at least some of the increase in average feedback scores, then we would expect a gap to emerge between what raters rate and how they actually feel about a transaction. To explore this possibility, we use information obtained by the introduction of a parallel and experimental reputation system into our focal marketplace. More specifically, a new feedback question asked employers to rate workers privately. This private feedback was not conveyed to the rated workers, nor made public to future would-be employers. At the same time, raters were still asked to give the status quo public feedback, both written and numerical.\n\nWe show that raters were far more candid in private, with substantial numbers reporting dissatisfaction privately but still assigning a perfect five-star rating publicly. This gap suggests that raters are reluctant to give negative feedback publicly because they do not want to harm the ratees’ future prospects, either for altruistic reasons or because they fear retaliation of some kind. We also show that average private feedback scores were decreasing over the period they were collected, but at the same time, average public feedback scores for the same transactions were increasing. This divergence provides some evidence of reputation inflation on the platform, albeit over a short time window when the private feedback question was asked.\n\nTo determine how much of the increase in average ratings is attributable to reputation inflation, we introduce a method for decomposing average ratings increases into the component that can be explained by changes in satisfaction and the component that cannot. To obtain a point estimate of the effect of inflation, the method requires an alternative measure of rater satisfaction not prone to inflation. Importantly, if the alternative measure is also prone to inflation, the method yields a lower bound. The method consists of learning the expected value for the actual numerical feedback, conditional upon the alternative measure of rater satisfaction. Under some mild assumptions likely to be met in practice, this learned conditional expectation function (CEF) can then be applied to new transaction data, predicting what the average score should be given the alternative feedback data. This allows one to net out the increase not attributable to changes in marketplace fundamentals.\n\nAlternative measures of rater satisfaction might seem hard to come by, but one measure available in many online marketplaces is the textual feedback that accompanies scores on the same transactions. For reasons we will discuss, textual feedback may be less prone to inflationary pressures. Consistent with this view, we show in our focal marketplace that the same sentences systematically have higher associated numerical feedback scores as time passes. For example, employers calling the work they received “terrible” would assign on average a public feedback score of 1.4 stars in 2008, but they would instead assign 2.4 stars in 2015.\n\nUsing written feedback as our alternative measure of rater satisfaction, we fit a model that predicts numerical feedback from the text of written feedback. We find that more than 50% of the increase in scores over a six-year period was due to inflation, with this result being robust across different specifications and training sets. Insofar as written feedback is also subject to inflation, our approach understates the extent of reputation inflation. As numerical ratings are often accompanied by written feedback, this method can be readily used in other contexts.\n\nA natural question is whether the reputation inflation we identify in our focal marketplace—and which likely occurs in other marketplaces—matters in practice for the functioning of the reputation system. Although we do not explore this question empirically, there are several theoretical reasons why strong inflation in a system with a fixed, top-censored scale will cause a loss of information, analogous to the problems with grade inflation (Babcock 2010, Butcher et al. 2014).\n\nOur key contribution is documenting the extent of reputation inflation in a large online marketplace by using an approach that accounts for changes in platform fundamentals. Our long-run, whole-system perspective is possible because we use data spanning over a decade of the operations of the marketplace. Although we cannot perform the same decomposition in other marketplaces, we observe increasing average feedback scores in every marketplace for which we could obtain data, even though none of these marketplaces allows tit-for-tat rating behavior (Bolton et al. 2013). Given that many online marketplaces share the same features as our focal marketplace, this evidence suggests that the problem is widespread.\n\nThe rest of this paper is organized as follows. Section 2 introduces the empirical setting and documents increasing feedback scores over time across five online marketplaces. Section 3 presents descriptive evidence on the problem in our focal marketplace. Section 4 introduces our decomposition method and applies it. Section 5 discusses the causes and implications of reputation inflation. Section 6 concludes the paper.",
        "md": "Our focal market is a large online labor market (Horton 2010). In online labor markets, employers hire workers to\n\nnot being more satisfied—can be described as a kind of inflation. This inflation can also cause pooling at the highest score, but is more worrisome because of its greater potential to reduce the informativeness and stability of the reputation system.\n\nIf reputation inflation can explain at least some of the increase in average feedback scores, then we would expect a gap to emerge between what raters rate and how they actually feel about a transaction. To explore this possibility, we use information obtained by the introduction of a parallel and experimental reputation system into our focal marketplace. More specifically, a new feedback question asked employers to rate workers privately. This private feedback was not conveyed to the rated workers, nor made public to future would-be employers. At the same time, raters were still asked to give the status quo public feedback, both written and numerical.\n\nWe show that raters were far more candid in private, with substantial numbers reporting dissatisfaction privately but still assigning a perfect five-star rating publicly. This gap suggests that raters are reluctant to give negative feedback publicly because they do not want to harm the ratees’ future prospects, either for altruistic reasons or because they fear retaliation of some kind. We also show that average private feedback scores were decreasing over the period they were collected, but at the same time, average public feedback scores for the same transactions were increasing. This divergence provides some evidence of reputation inflation on the platform, albeit over a short time window when the private feedback question was asked.\n\nTo determine how much of the increase in average ratings is attributable to reputation inflation, we introduce a method for decomposing average ratings increases into the component that can be explained by changes in satisfaction and the component that cannot. To obtain a point estimate of the effect of inflation, the method requires an alternative measure of rater satisfaction not prone to inflation. Importantly, if the alternative measure is also prone to inflation, the method yields a lower bound. The method consists of learning the expected value for the actual numerical feedback, conditional upon the alternative measure of rater satisfaction. Under some mild assumptions likely to be met in practice, this learned conditional expectation function (CEF) can then be applied to new transaction data, predicting what the average score should be given the alternative feedback data. This allows one to net out the increase not attributable to changes in marketplace fundamentals.\n\nAlternative measures of rater satisfaction might seem hard to come by, but one measure available in many online marketplaces is the textual feedback that accompanies scores on the same transactions. For reasons we will discuss, textual feedback may be less prone to inflationary pressures. Consistent with this view, we show in our focal marketplace that the same sentences systematically have higher associated numerical feedback scores as time passes. For example, employers calling the work they received “terrible” would assign on average a public feedback score of 1.4 stars in 2008, but they would instead assign 2.4 stars in 2015.\n\nUsing written feedback as our alternative measure of rater satisfaction, we fit a model that predicts numerical feedback from the text of written feedback. We find that more than 50% of the increase in scores over a six-year period was due to inflation, with this result being robust across different specifications and training sets. Insofar as written feedback is also subject to inflation, our approach understates the extent of reputation inflation. As numerical ratings are often accompanied by written feedback, this method can be readily used in other contexts.\n\nA natural question is whether the reputation inflation we identify in our focal marketplace—and which likely occurs in other marketplaces—matters in practice for the functioning of the reputation system. Although we do not explore this question empirically, there are several theoretical reasons why strong inflation in a system with a fixed, top-censored scale will cause a loss of information, analogous to the problems with grade inflation (Babcock 2010, Butcher et al. 2014).\n\nOur key contribution is documenting the extent of reputation inflation in a large online marketplace by using an approach that accounts for changes in platform fundamentals. Our long-run, whole-system perspective is possible because we use data spanning over a decade of the operations of the marketplace. Although we cannot perform the same decomposition in other marketplaces, we observe increasing average feedback scores in every marketplace for which we could obtain data, even though none of these marketplaces allows tit-for-tat rating behavior (Bolton et al. 2013). Given that many online marketplaces share the same features as our focal marketplace, this evidence suggests that the problem is widespread.\n\nThe rest of this paper is organized as follows. Section 2 introduces the empirical setting and documents increasing feedback scores over time across five online marketplaces. Section 3 presents descriptive evidence on the problem in our focal marketplace. Section 4 introduces our decomposition method and applies it. Section 5 discusses the causes and implications of reputation inflation. Section 6 concludes the paper.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 238.47149919999993,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 4,
    "text": "             Filippas, Horton, and Golden: Reputation Inflation\n             Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS  3\n\nperform remote tasks, such as computer programming,\ngraphic design, and data entry. Online labor markets dif-\nfer in their scope and focus, but services provided by the\nplatform are similar to those provided by other peer-to-\npeer markets, and include maintaining job listings, arbi-\ntrating disputes, certifying worker skills, and, importantly,\nbuilding and maintaining reputation systems (Filippas\net al. 2020). Online markets offer a convenient setting for\nresearch because of the excellent measurement afforded in\nthe online setting (Horton et al. 2011, Horton and Tambe\n\n2015).\n\n2.1. How the Reputation System Functions in Our\n      Focal Marketplace\nIn our focal marketplace, when one party ends a con-\ntract, both parties are prompted to give feedback.2\nEmployers are asked to give both written feedback\n(e.g., “Paul did excellent work—I’d work with him\nagain”    or  “Ada is a great person to work for—her\ninstructions were always very clear”) and numerical\nfeedback. The numerical feedback is given on several\nweighted dimensions: skills (20%), quality of work\n(20%), availability (15%), adherence to schedule (15%),\ncommunication         (15%),    and    cooperation      (15%).    On\neach dimension, the rater gives a score on a one-to-\nfive-star scale.\n   The scores are aggregated according to the dimen-\nsion weights. A worker’s reputation at a moment in\ntime is the average of her scores on completed proj-\nects, weighted by the dollar value of each project. On\nthe worker profile, a lifetime score is shown as well as\na  “last 6 months”       score, which is more prominently\ndisplayed. Showing recent feedback is presumably\nthe   platform’s      response     to   the   opportunism        that\nbecomes possible once an employer or worker has\nobtained a      high,    hard-to-lower reputation           (Aperjis\nand Johari 2010, Liu 2011). Despite the aggregation of\nindividual scores into a reputation, the entire feedback\nhistory is available to interested parties for inspection.\nWorkers can view the feedback given to previous\nworkers     rated    by   an   employer,      and    the   feedback\nreceived by that employer from those same workers.\n   The reputation system could be characterized as\nstate-of-the-art, in the sense that direct tit-for-tat con-\nditioning is not possible (Dellarocas 2005, Bolton et al.\n2013, Fradkin et al. 2019). Both the employer and the\nworker have an initial 14-day period in which to leave\nfeedback. The platform does not reveal public feed-\nback    immediately,       but   rather    uses   a  double-blind\nprocess. If both parties leave feedback during the\n14-day feedback period, then the platform reveals\nboth   sets of feedback simultaneously.               If only one\nparty leaves feedback, then the platform reveals it at\nthe end of the feedback period. Thus, neither party\nlearns its own rating before leaving a rating for the\nother party. Leaving feedback is strongly encouraged,\nbut    not   compulsory.       These     encouragements         seem\neffective, in that over the history of the platform,\n81.8% of employers eligible to leave feedback have\nchosen to do so.\n\n2.2. Feedback Ratings Now and in the Past\nThe    distribution      of   employer-on-worker           feedback\nscores in our focal marketplace is highly right skewed.\nFigure 1(a) depicts the histogram of public feedback\nscores from January 1, 2014, to May 11, 2016, for con-\ntracts worth more than $10.3\n                                         Public feedback scores\nare between 1 and 5 stars, inclusive, and with incre-\nments of 0.25 stars. Each bar is labeled with the per-\ncentage of total observations falling in that bin, and\nthe dashed       line   shows the cumulative number of\nassignments with feedback less than or equal to the\nright limit of the bin it is above. More than 80% of the\nevaluations fall in the 4.75-to-5.00-star bin (1,339,071\nobservations). The average feedback pooled for the\nwhole sample shown in Figure 1(a) is 4.77.\n   Scores have not always been highly right skewed.\nFigure 1(b) shows the average monthly feedback over\ntime, for contracts ending within each month. There is\na clear increase in the feedback scores awarded on the\nplatform: the feedback score average has increased\nfrom 3.74 in the beginning of 2007 to 4.85 in May 2016.\nThe strongest period of increase was 2007, when aver-\nage feedback scores increased by about 0.53 stars.\n   The increase in average feedback could be the out-\ncome of raters giving less bad feedback, more good\nfeedback, or some combination thereof. Figure 1(c)\nshows the fractions of contracts having ratings within\ndifferent ranges, over time. In the early days of the\nplatform,     rating    assignments      were     reasonably      dis-\npersed, with completed contracts regularly receiving\nratings in the (0, 3]      range. Near the end of our data,\ncompleted contracts essentially never receive a rating\nin the (0, 3] range. Instead, there has been a dramatic\nincrease in the fraction of contracts getting exactly five\nstars: 33% of contracts received a             five-star rating at\nthe start of sample, compared with 85% at the end of\nthe sample.\n\n2.3. Evidence of Increasing Average Scores from\n       Several Online Marketplaces\nOur focal marketplace clearly shows an increase in\naverage ratings over time, but a natural question is\nwhether this kind of pattern is common in online mar-\nkets. To answer this question, we collected average\nfeedback score ratings from a number of online mar-\nketplaces. The average feedback scores for the various\nmarketplaces are shown in Figure 2. For some market-\nplaces    that   are   organized      by   geography,       we   also\nobtained city-specific data.\n   We observe an increase in average ratings over time\nthat mirrors the pattern that we found in our focal",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\nperform remote tasks, such as computer programming, graphic design, and data entry. Online labor markets differ in their scope and focus, but services provided by the platform are similar to those provided by other peer-to-peer markets, and include maintaining job listings, arbitrating disputes, certifying worker skills, and, importantly, building and maintaining reputation systems (Filippas et al. 2020). Online markets offer a convenient setting for research because of the excellent measurement afforded in the online setting (Horton et al. 2011, Horton and Tambe 2015).\n\n# 2.1. How the Reputation System Functions in Our Focal Marketplace\n\nIn our focal marketplace, when one party ends a contract, both parties are prompted to give feedback. Employers are asked to give both written feedback (e.g., “Paul did excellent work—I’d work with him again” or “Ada is a great person to work for—her instructions were always very clear”) and numerical feedback. The numerical feedback is given on several weighted dimensions: skills (20%), quality of work (20%), availability (15%), adherence to schedule (15%), communication (15%), and cooperation (15%). On each dimension, the rater gives a score on a one-to-five-star scale.\n\nThe scores are aggregated according to the dimension weights. A worker’s reputation at a moment in time is the average of her scores on completed projects, weighted by the dollar value of each project. On the worker profile, a lifetime score is shown as well as a “last 6 months” score, which is more prominently displayed. Showing recent feedback is presumably the platform’s response to the opportunism that becomes possible once an employer or worker has obtained a high, hard-to-lower reputation (Aperjis and Johari 2010, Liu 2011). Despite the aggregation of individual scores into a reputation, the entire feedback history is available to interested parties for inspection. Workers can view the feedback given to previous workers rated by an employer, and the feedback received by that employer from those same workers.\n\nThe reputation system could be characterized as state-of-the-art, in the sense that direct tit-for-tat conditioning is not possible (Dellarocas 2005, Bolton et al. 2013, Fradkin et al. 2019). Both the employer and the worker have an initial 14-day period in which to leave feedback. The platform does not reveal public feedback immediately, but rather uses a double-blind process. If both parties leave feedback during the 14-day feedback period, then the platform reveals both sets of feedback simultaneously. If only one party leaves feedback, then the platform reveals it at the end of the feedback period. Thus, neither party learns its own rating before leaving a rating for the other party. Leaving feedback is strongly encouraged, but not compulsory. These encouragements seem effective, in that over the history of the platform, 81.8% of employers eligible to leave feedback have chosen to do so.\n\n# 2.2. Feedback Ratings Now and in the Past\n\nThe distribution of employer-on-worker feedback scores in our focal marketplace is highly right skewed. Figure 1(a) depicts the histogram of public feedback scores from January 1, 2014, to May 11, 2016, for contracts worth more than $10. Public feedback scores are between 1 and 5 stars, inclusive, and with increments of 0.25 stars. Each bar is labeled with the percentage of total observations falling in that bin, and the dashed line shows the cumulative number of assignments with feedback less than or equal to the right limit of the bin it is above. More than 80% of the evaluations fall in the 4.75-to-5.00-star bin (1,339,071 observations). The average feedback pooled for the whole sample shown in Figure 1(a) is 4.77.\n\nScores have not always been highly right skewed. Figure 1(b) shows the average monthly feedback over time, for contracts ending within each month. There is a clear increase in the feedback scores awarded on the platform: the feedback score average has increased from 3.74 in the beginning of 2007 to 4.85 in May 2016. The strongest period of increase was 2007, when average feedback scores increased by about 0.53 stars.\n\nThe increase in average feedback could be the outcome of raters giving less bad feedback, more good feedback, or some combination thereof. Figure 1(c) shows the fractions of contracts having ratings within different ranges, over time. In the early days of the platform, rating assignments were reasonably dispersed, with completed contracts regularly receiving ratings in the (0, 3] range. Near the end of our data, completed contracts essentially never receive a rating in the (0, 3] range. Instead, there has been a dramatic increase in the fraction of contracts getting exactly five stars: 33% of contracts received a five-star rating at the start of sample, compared with 85% at the end of the sample.\n\n# 2.3. Evidence of Increasing Average Scores from Several Online Marketplaces\n\nOur focal marketplace clearly shows an increase in average ratings over time, but a natural question is whether this kind of pattern is common in online markets. To answer this question, we collected average feedback score ratings from a number of online marketplaces. The average feedback scores for the various marketplaces are shown in Figure 2. For some marketplaces that are organized by geography, we also obtained city-specific data.\n\nWe observe an increase in average ratings over time that mirrors the pattern that we found in our focal.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 9.9626
        }
      },
      {
        "type": "text",
        "value": "perform remote tasks, such as computer programming, graphic design, and data entry. Online labor markets differ in their scope and focus, but services provided by the platform are similar to those provided by other peer-to-peer markets, and include maintaining job listings, arbitrating disputes, certifying worker skills, and, importantly, building and maintaining reputation systems (Filippas et al. 2020). Online markets offer a convenient setting for research because of the excellent measurement afforded in the online setting (Horton et al. 2011, Horton and Tambe 2015).",
        "md": "perform remote tasks, such as computer programming, graphic design, and data entry. Online labor markets differ in their scope and focus, but services provided by the platform are similar to those provided by other peer-to-peer markets, and include maintaining job listings, arbitrating disputes, certifying worker skills, and, importantly, building and maintaining reputation systems (Filippas et al. 2020). Online markets offer a convenient setting for research because of the excellent measurement afforded in the online setting (Horton et al. 2011, Horton and Tambe 2015).",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 237.54026431999992,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "2.1. How the Reputation System Functions in Our Focal Marketplace",
        "md": "# 2.1. How the Reputation System Functions in Our Focal Marketplace",
        "bBox": {
          "x": 45,
          "y": 148.87199999999996,
          "w": 273.7558,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "In our focal marketplace, when one party ends a contract, both parties are prompted to give feedback. Employers are asked to give both written feedback (e.g., “Paul did excellent work—I’d work with him again” or “Ada is a great person to work for—her instructions were always very clear”) and numerical feedback. The numerical feedback is given on several weighted dimensions: skills (20%), quality of work (20%), availability (15%), adherence to schedule (15%), communication (15%), and cooperation (15%). On each dimension, the rater gives a score on a one-to-five-star scale.\n\nThe scores are aggregated according to the dimension weights. A worker’s reputation at a moment in time is the average of her scores on completed projects, weighted by the dollar value of each project. On the worker profile, a lifetime score is shown as well as a “last 6 months” score, which is more prominently displayed. Showing recent feedback is presumably the platform’s response to the opportunism that becomes possible once an employer or worker has obtained a high, hard-to-lower reputation (Aperjis and Johari 2010, Liu 2011). Despite the aggregation of individual scores into a reputation, the entire feedback history is available to interested parties for inspection. Workers can view the feedback given to previous workers rated by an employer, and the feedback received by that employer from those same workers.\n\nThe reputation system could be characterized as state-of-the-art, in the sense that direct tit-for-tat conditioning is not possible (Dellarocas 2005, Bolton et al. 2013, Fradkin et al. 2019). Both the employer and the worker have an initial 14-day period in which to leave feedback. The platform does not reveal public feedback immediately, but rather uses a double-blind process. If both parties leave feedback during the 14-day feedback period, then the platform reveals both sets of feedback simultaneously. If only one party leaves feedback, then the platform reveals it at the end of the feedback period. Thus, neither party learns its own rating before leaving a rating for the other party. Leaving feedback is strongly encouraged, but not compulsory. These encouragements seem effective, in that over the history of the platform, 81.8% of employers eligible to leave feedback have chosen to do so.",
        "md": "In our focal marketplace, when one party ends a contract, both parties are prompted to give feedback. Employers are asked to give both written feedback (e.g., “Paul did excellent work—I’d work with him again” or “Ada is a great person to work for—her instructions were always very clear”) and numerical feedback. The numerical feedback is given on several weighted dimensions: skills (20%), quality of work (20%), availability (15%), adherence to schedule (15%), communication (15%), and cooperation (15%). On each dimension, the rater gives a score on a one-to-five-star scale.\n\nThe scores are aggregated according to the dimension weights. A worker’s reputation at a moment in time is the average of her scores on completed projects, weighted by the dollar value of each project. On the worker profile, a lifetime score is shown as well as a “last 6 months” score, which is more prominently displayed. Showing recent feedback is presumably the platform’s response to the opportunism that becomes possible once an employer or worker has obtained a high, hard-to-lower reputation (Aperjis and Johari 2010, Liu 2011). Despite the aggregation of individual scores into a reputation, the entire feedback history is available to interested parties for inspection. Workers can view the feedback given to previous workers rated by an employer, and the feedback received by that employer from those same workers.\n\nThe reputation system could be characterized as state-of-the-art, in the sense that direct tit-for-tat conditioning is not possible (Dellarocas 2005, Bolton et al. 2013, Fradkin et al. 2019). Both the employer and the worker have an initial 14-day period in which to leave feedback. The platform does not reveal public feedback immediately, but rather uses a double-blind process. If both parties leave feedback during the 14-day feedback period, then the platform reveals both sets of feedback simultaneously. If only one party leaves feedback, then the platform reveals it at the end of the feedback period. Thus, neither party learns its own rating before leaving a rating for the other party. Leaving feedback is strongly encouraged, but not compulsory. These encouragements seem effective, in that over the history of the platform, 81.8% of employers eligible to leave feedback have chosen to do so.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.5721446399998,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "2.2. Feedback Ratings Now and in the Past",
        "md": "# 2.2. Feedback Ratings Now and in the Past",
        "bBox": {
          "x": 45,
          "y": 136.87199999999996,
          "w": 461.63105278,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "The distribution of employer-on-worker feedback scores in our focal marketplace is highly right skewed. Figure 1(a) depicts the histogram of public feedback scores from January 1, 2014, to May 11, 2016, for contracts worth more than $10. Public feedback scores are between 1 and 5 stars, inclusive, and with increments of 0.25 stars. Each bar is labeled with the percentage of total observations falling in that bin, and the dashed line shows the cumulative number of assignments with feedback less than or equal to the right limit of the bin it is above. More than 80% of the evaluations fall in the 4.75-to-5.00-star bin (1,339,071 observations). The average feedback pooled for the whole sample shown in Figure 1(a) is 4.77.\n\nScores have not always been highly right skewed. Figure 1(b) shows the average monthly feedback over time, for contracts ending within each month. There is a clear increase in the feedback scores awarded on the platform: the feedback score average has increased from 3.74 in the beginning of 2007 to 4.85 in May 2016. The strongest period of increase was 2007, when average feedback scores increased by about 0.53 stars.\n\nThe increase in average feedback could be the outcome of raters giving less bad feedback, more good feedback, or some combination thereof. Figure 1(c) shows the fractions of contracts having ratings within different ranges, over time. In the early days of the platform, rating assignments were reasonably dispersed, with completed contracts regularly receiving ratings in the (0, 3] range. Near the end of our data, completed contracts essentially never receive a rating in the (0, 3] range. Instead, there has been a dramatic increase in the fraction of contracts getting exactly five stars: 33% of contracts received a five-star rating at the start of sample, compared with 85% at the end of the sample.",
        "md": "The distribution of employer-on-worker feedback scores in our focal marketplace is highly right skewed. Figure 1(a) depicts the histogram of public feedback scores from January 1, 2014, to May 11, 2016, for contracts worth more than $10. Public feedback scores are between 1 and 5 stars, inclusive, and with increments of 0.25 stars. Each bar is labeled with the percentage of total observations falling in that bin, and the dashed line shows the cumulative number of assignments with feedback less than or equal to the right limit of the bin it is above. More than 80% of the evaluations fall in the 4.75-to-5.00-star bin (1,339,071 observations). The average feedback pooled for the whole sample shown in Figure 1(a) is 4.77.\n\nScores have not always been highly right skewed. Figure 1(b) shows the average monthly feedback over time, for contracts ending within each month. There is a clear increase in the feedback scores awarded on the platform: the feedback score average has increased from 3.74 in the beginning of 2007 to 4.85 in May 2016. The strongest period of increase was 2007, when average feedback scores increased by about 0.53 stars.\n\nThe increase in average feedback could be the outcome of raters giving less bad feedback, more good feedback, or some combination thereof. Figure 1(c) shows the fractions of contracts having ratings within different ranges, over time. In the early days of the platform, rating assignments were reasonably dispersed, with completed contracts regularly receiving ratings in the (0, 3] range. Near the end of our data, completed contracts essentially never receive a rating in the (0, 3] range. Instead, there has been a dramatic increase in the fraction of contracts getting exactly five stars: 33% of contracts received a five-star rating at the start of sample, compared with 85% at the end of the sample.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.52532042000007,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "2.3. Evidence of Increasing Average Scores from Several Online Marketplaces",
        "md": "# 2.3. Evidence of Increasing Average Scores from Several Online Marketplaces",
        "bBox": {
          "x": 269,
          "y": 51.87199999999996,
          "w": 268.98130000000003,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "Our focal marketplace clearly shows an increase in average ratings over time, but a natural question is whether this kind of pattern is common in online markets. To answer this question, we collected average feedback score ratings from a number of online marketplaces. The average feedback scores for the various marketplaces are shown in Figure 2. For some marketplaces that are organized by geography, we also obtained city-specific data.\n\nWe observe an increase in average ratings over time that mirrors the pattern that we found in our focal.",
        "md": "Our focal marketplace clearly shows an increase in average ratings over time, but a natural question is whether this kind of pattern is common in online markets. To answer this question, we collected average feedback score ratings from a number of online marketplaces. The average feedback scores for the various marketplaces are shown in Figure 2. For some marketplaces that are organized by geography, we also obtained city-specific data.\n\nWe observe an increase in average ratings over time that mirrors the pattern that we found in our focal.",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 295.10320500000006,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 5,
    "text": "                                                                                                                            Filippas, Horton, and Golden: Reputation Inflation\n               4                                                                                                   Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n               Figure 1.     (Color online) Employer-on-Worker Feedback Characteristics in an Online Marketplace\n\n                                     (a) Distribution of feedback scores for the period January 1, 2014, to May 11, 2016.PercentagePercentage of workersPublic employer-to-\nworker feedback scores100%                                                                                                                                      82.8%\n                                     75%\n                                     50%\n                                     25%                                                                                                                 4.3%\n                                            0.9%   0.1%    0.2% 0.2%     0.3% 0.3% 0.3%        0.4%   1.1%    0.6%   0.7%   0.9%   2.8%   1.8%    2.5%\n                                      0%\n\n                                          (0.75,1](1,1.25](1.25,1.5](1.5,1.75](1.75,2](2,2.25](2.25,2.5](2.5,2.75](2.75,3](3,3.25](3.25,3.5](3.5,3.75](3.75,4](4,4.25](4.25,4.5](4.5,4.75](4.75,5]\n                                                                        Public employer-to-worker feedback scores\n\n                                           (b) Monthly average public feedback scores assigned on completed projects.\n\n                                        4.8\n\n                                        4.5                                                                                          Distribution for this\n                                                                                                                                       period shown in\n                                        4.2                                                                                                 Figure 1a\n                                        3.9\n\n                                                2007        2008       2009         2010        2011        2012       2013        2014       2015         2016\n                                                                                                       Year\n\n                                         (c) Percentage of completed projects receiving different star ratings over time.\n\n                                                        [1,3)                          [3,4)                         [4,4.99)                       [4.99,5]\n                                    100%\n\n                                     75%\n\n                                     50%\n\n                                     25%\n\n                                      0%\n                                           2007 2009  2011  2013 2015     2007  2009  2011 2013  2015     2007 2009  2011  2013 2015     2007 2009  2011  2013  2015\n                                                                                                      Year\n               Notes. The top panel shows the histogram of public numerical ratings assigned by employers to workers, discretized by 0.25 star interval bins.\n               The scale for feedback is one to five stars. The value of each bin is shown above it, and the dashed line depicts the empirical cumulative density\n               function. The sample we use consists of all contracts worth more than $10 from January 1, 2014, to May 11, 2016, for which the employer pro-\n               vided feedback. See Section 2.2 for the description of the sample. The middle panel plots the average public feedback scores assigned by employ-\n               ers to workers on completed contracts by month. The average scores are computed for every month, and a 95% interval is depicted for every\n               point estimate. The shaded area denotes the data that were used in panel (a). The bottom panel plots the fraction of public feedback scores\n               assigned in a given month into four bins, [1, 3), [3, 4), [4, 4:99), and 5 stars, over time.\n\nmarket.       While      the    precise      slope     and     size    of   the\nincrease differs by market (and by city), the general\npattern of increase is clear.\n   The common pattern of increase in average feed-\nback occurs despite the fact that the goods and serv-\nices that are transacted in these marketplaces differ\ndramatically, and even though these platforms greatly\ndiffer in the marketplace mechanisms and matching\ntechnologies they employ. Panel (a) of Figure 2 shows\nlongitudinal data in a competing online labor market,\nand ratings are assigned by employers to workers\n(freelancers). Panel (b) plots longitudinal ratings data\nfrom four major cities in the United States and Europe\nin a large home-sharing platform. Home-sharing plat-\nforms      are    peer-to-peer         marketplaces          that    facilitate\nshort-term rentals for lodging (Filippas and Horton\n2018). The ratings are by guests (those who are renting\nproperties)        to    hosts     (those      who      are    renting       out",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\n# Figure 1. (Color online) Employer-on-Worker Feedback Characteristics in an Online Marketplace\n\n# (a) Distribution of feedback scores for the period January 1, 2014, to May 11, 2016.\n\n|Public employer-to-worker feedback scores|Percentage|\n|---|---|\n|(0.75,1]|0.9%|\n|(1,1.25]|0.1%|\n|(1.25,1.5]|0.2%|\n|(1.5,1.75]|0.2%|\n|(1.75,2]|0.3%|\n|(2,2.25]|0.3%|\n|(2.25,2.5]|0.3%|\n|(2.5,2.75]|0.4%|\n|(2.75,3]|1.1%|\n|(3,3.25]|0.6%|\n|(3.25,3.5]|0.7%|\n|(3.5,3.75]|0.9%|\n|(3.75,4]|2.8%|\n|(4,4.25]|1.8%|\n|(4.25,4.5]|2.5%|\n|(4.5,4.75]| |\n|(4.75,5]|82.8%|\n\n# (b) Monthly average public feedback scores assigned on completed projects.\n\n4.8\n\n4.5\n\n4.2\n\n3.9\n\n2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n\n# (c) Percentage of completed projects receiving different star ratings over time.\n\n|Star Rating|Year|Percentage|\n|---|---|---|\n|[1,3)|2007| |\n|[1,3)|2009| |\n|[1,3)|2011| |\n|[1,3)|2013| |\n|[1,3)|2015| |\n|[3,4)|2007| |\n|[3,4)|2009| |\n|[3,4)|2011| |\n|[3,4)|2013| |\n|[3,4)|2015| |\n|[4,4.99)|2007| |\n|[4,4.99)|2009| |\n|[4,4.99)|2011| |\n|[4,4.99)|2013| |\n|[4,4.99)|2015| |\n|[4.99,5]|2007| |\n|[4.99,5]|2009| |\n|[4.99,5]|2011| |\n|[4.99,5]|2013| |\n|[4.99,5]|2015| |\n\n# Notes\n\nThe top panel shows the histogram of public numerical ratings assigned by employers to workers, discretized by 0.25 star interval bins. The scale for feedback is one to five stars. The value of each bin is shown above it, and the dashed line depicts the empirical cumulative density function. The sample we use consists of all contracts worth more than $10 from January 1, 2014, to May 11, 2016, for which the employer provided feedback. See Section 2.2 for the description of the sample. The middle panel plots the average public feedback scores assigned by employers to workers on completed contracts by month. The average scores are computed for every month, and a 95% interval is depicted for every point estimate. The shaded area denotes the data that were used in panel (a). The bottom panel plots the fraction of public feedback scores assigned in a given month into four bins, [1, 3), [3, 4), [4, 4:99), and 5 stars, over time.\n\nmarket. While the precise slope and size of the increase differs by market (and by city), the general pattern of increase is clear. The common pattern of increase in average feedback occurs despite the fact that the goods and services that are transacted in these marketplaces differ dramatically, and even though these platforms greatly differ in the marketplace mechanisms and matching technologies they employ. Panel (a) of Figure 2 shows longitudinal data in a competing online labor market, and ratings are assigned by employers to workers (freelancers). Panel (b) plots longitudinal ratings data from four major cities in the United States and Europe in a large home-sharing platform. Home-sharing platforms are peer-to-peer marketplaces that facilitate short-term rentals for lodging (Filippas and Horton 2018). The ratings are by guests (those who are renting properties) to hosts (those who are renting out.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 207,
          "y": 42.87199999999996,
          "w": 331.77079266,
          "h": 9.962599999999952
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 338,
          "y": 51.87199999999996,
          "w": 200.56013871999983,
          "h": 6.475700000000003
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 1. (Color online) Employer-on-Worker Feedback Characteristics in an Online Marketplace",
        "md": "# Figure 1. (Color online) Employer-on-Worker Feedback Characteristics in an Online Marketplace",
        "bBox": {
          "x": 45,
          "y": 75.87199999999996,
          "w": 335.09752989999953,
          "h": 8.966300000000004
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "(a) Distribution of feedback scores for the period January 1, 2014, to May 11, 2016.",
        "md": "# (a) Distribution of feedback scores for the period January 1, 2014, to May 11, 2016.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 344.77113595,
          "h": 9.962900000000005
        }
      },
      {
        "type": "table",
        "rows": [
          ["Public employer-to-worker feedback scores", "Percentage"],
          ["(0.75,1]", "0.9%"],
          ["(1,1.25]", "0.1%"],
          ["(1.25,1.5]", "0.2%"],
          ["(1.5,1.75]", "0.2%"],
          ["(1.75,2]", "0.3%"],
          ["(2,2.25]", "0.3%"],
          ["(2.25,2.5]", "0.3%"],
          ["(2.5,2.75]", "0.4%"],
          ["(2.75,3]", "1.1%"],
          ["(3,3.25]", "0.6%"],
          ["(3.25,3.5]", "0.7%"],
          ["(3.5,3.75]", "0.9%"],
          ["(3.75,4]", "2.8%"],
          ["(4,4.25]", "1.8%"],
          ["(4.25,4.5]", "2.5%"],
          ["(4.5,4.75]", ""],
          ["(4.75,5]", "82.8%"]
        ],
        "md": "|Public employer-to-worker feedback scores|Percentage|\n|---|---|\n|(0.75,1]|0.9%|\n|(1,1.25]|0.1%|\n|(1.25,1.5]|0.2%|\n|(1.5,1.75]|0.2%|\n|(1.75,2]|0.3%|\n|(2,2.25]|0.3%|\n|(2.25,2.5]|0.3%|\n|(2.5,2.75]|0.4%|\n|(2.75,3]|1.1%|\n|(3,3.25]|0.6%|\n|(3.25,3.5]|0.7%|\n|(3.5,3.75]|0.9%|\n|(3.75,4]|2.8%|\n|(4,4.25]|1.8%|\n|(4.25,4.5]|2.5%|\n|(4.5,4.75]| |\n|(4.75,5]|82.8%|",
        "isPerfectTable": true,
        "csv": "\"Public employer-to-worker feedback scores\",\"Percentage\"\n\"(0.75,1]\",\"0.9%\"\n\"(1,1.25]\",\"0.1%\"\n\"(1.25,1.5]\",\"0.2%\"\n\"(1.5,1.75]\",\"0.2%\"\n\"(1.75,2]\",\"0.3%\"\n\"(2,2.25]\",\"0.3%\"\n\"(2.25,2.5]\",\"0.3%\"\n\"(2.5,2.75]\",\"0.4%\"\n\"(2.75,3]\",\"1.1%\"\n\"(3,3.25]\",\"0.6%\"\n\"(3.25,3.5]\",\"0.7%\"\n\"(3.5,3.75]\",\"0.9%\"\n\"(3.75,4]\",\"2.8%\"\n\"(4,4.25]\",\"1.8%\"\n\"(4.25,4.5]\",\"2.5%\"\n\"(4.5,4.75]\",\"\"\n\"(4.75,5]\",\"82.8%\""
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "(b) Monthly average public feedback scores assigned on completed projects.",
        "md": "# (b) Monthly average public feedback scores assigned on completed projects.",
        "bBox": {
          "x": 127,
          "y": 249.87199999999996,
          "w": 311.8128664600002,
          "h": 9.9629
        }
      },
      {
        "type": "text",
        "value": "4.8\n\n4.5\n\n4.2\n\n3.9\n\n2007 2008 2009 2010 2011 2012 2013 2014 2015 2016",
        "md": "4.8\n\n4.5\n\n4.2\n\n3.9\n\n2007 2008 2009 2010 2011 2012 2013 2014 2015 2016",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 19.334300000000013,
          "h": 9.962600000000002
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "(c) Percentage of completed projects receiving different star ratings over time.",
        "md": "# (c) Percentage of completed projects receiving different star ratings over time.",
        "bBox": {
          "x": null,
          "y": 101,
          "w": 318.6434307,
          "h": 10.1015
        }
      },
      {
        "type": "table",
        "rows": [
          ["Star Rating", "Year", "Percentage"],
          ["[1,3)", "2007", ""],
          ["[1,3)", "2009", ""],
          ["[1,3)", "2011", ""],
          ["[1,3)", "2013", ""],
          ["[1,3)", "2015", ""],
          ["[3,4)", "2007", ""],
          ["[3,4)", "2009", ""],
          ["[3,4)", "2011", ""],
          ["[3,4)", "2013", ""],
          ["[3,4)", "2015", ""],
          ["[4,4.99)", "2007", ""],
          ["[4,4.99)", "2009", ""],
          ["[4,4.99)", "2011", ""],
          ["[4,4.99)", "2013", ""],
          ["[4,4.99)", "2015", ""],
          ["[4.99,5]", "2007", ""],
          ["[4.99,5]", "2009", ""],
          ["[4.99,5]", "2011", ""],
          ["[4.99,5]", "2013", ""],
          ["[4.99,5]", "2015", ""]
        ],
        "md": "|Star Rating|Year|Percentage|\n|---|---|---|\n|[1,3)|2007| |\n|[1,3)|2009| |\n|[1,3)|2011| |\n|[1,3)|2013| |\n|[1,3)|2015| |\n|[3,4)|2007| |\n|[3,4)|2009| |\n|[3,4)|2011| |\n|[3,4)|2013| |\n|[3,4)|2015| |\n|[4,4.99)|2007| |\n|[4,4.99)|2009| |\n|[4,4.99)|2011| |\n|[4,4.99)|2013| |\n|[4,4.99)|2015| |\n|[4.99,5]|2007| |\n|[4.99,5]|2009| |\n|[4.99,5]|2011| |\n|[4.99,5]|2013| |\n|[4.99,5]|2015| |",
        "isPerfectTable": true,
        "csv": "\"Star Rating\",\"Year\",\"Percentage\"\n\"[1,3)\",\"2007\",\"\"\n\"[1,3)\",\"2009\",\"\"\n\"[1,3)\",\"2011\",\"\"\n\"[1,3)\",\"2013\",\"\"\n\"[1,3)\",\"2015\",\"\"\n\"[3,4)\",\"2007\",\"\"\n\"[3,4)\",\"2009\",\"\"\n\"[3,4)\",\"2011\",\"\"\n\"[3,4)\",\"2013\",\"\"\n\"[3,4)\",\"2015\",\"\"\n\"[4,4.99)\",\"2007\",\"\"\n\"[4,4.99)\",\"2009\",\"\"\n\"[4,4.99)\",\"2011\",\"\"\n\"[4,4.99)\",\"2013\",\"\"\n\"[4,4.99)\",\"2015\",\"\"\n\"[4.99,5]\",\"2007\",\"\"\n\"[4.99,5]\",\"2009\",\"\"\n\"[4.99,5]\",\"2011\",\"\"\n\"[4.99,5]\",\"2013\",\"\"\n\"[4.99,5]\",\"2015\",\"\""
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Notes",
        "md": "# Notes",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 584.674
        }
      },
      {
        "type": "text",
        "value": "The top panel shows the histogram of public numerical ratings assigned by employers to workers, discretized by 0.25 star interval bins. The scale for feedback is one to five stars. The value of each bin is shown above it, and the dashed line depicts the empirical cumulative density function. The sample we use consists of all contracts worth more than $10 from January 1, 2014, to May 11, 2016, for which the employer provided feedback. See Section 2.2 for the description of the sample. The middle panel plots the average public feedback scores assigned by employers to workers on completed contracts by month. The average scores are computed for every month, and a 95% interval is depicted for every point estimate. The shaded area denotes the data that were used in panel (a). The bottom panel plots the fraction of public feedback scores assigned in a given month into four bins, [1, 3), [3, 4), [4, 4:99), and 5 stars, over time.\n\nmarket. While the precise slope and size of the increase differs by market (and by city), the general pattern of increase is clear. The common pattern of increase in average feedback occurs despite the fact that the goods and services that are transacted in these marketplaces differ dramatically, and even though these platforms greatly differ in the marketplace mechanisms and matching technologies they employ. Panel (a) of Figure 2 shows longitudinal data in a competing online labor market, and ratings are assigned by employers to workers (freelancers). Panel (b) plots longitudinal ratings data from four major cities in the United States and Europe in a large home-sharing platform. Home-sharing platforms are peer-to-peer marketplaces that facilitate short-term rentals for lodging (Filippas and Horton 2018). The ratings are by guests (those who are renting properties) to hosts (those who are renting out.",
        "md": "The top panel shows the histogram of public numerical ratings assigned by employers to workers, discretized by 0.25 star interval bins. The scale for feedback is one to five stars. The value of each bin is shown above it, and the dashed line depicts the empirical cumulative density function. The sample we use consists of all contracts worth more than $10 from January 1, 2014, to May 11, 2016, for which the employer provided feedback. See Section 2.2 for the description of the sample. The middle panel plots the average public feedback scores assigned by employers to workers on completed contracts by month. The average scores are computed for every month, and a 95% interval is depicted for every point estimate. The shaded area denotes the data that were used in panel (a). The bottom panel plots the fraction of public feedback scores assigned in a given month into four bins, [1, 3), [3, 4), [4, 4:99), and 5 stars, over time.\n\nmarket. While the precise slope and size of the increase differs by market (and by city), the general pattern of increase is clear. The common pattern of increase in average feedback occurs despite the fact that the goods and services that are transacted in these marketplaces differ dramatically, and even though these platforms greatly differ in the marketplace mechanisms and matching technologies they employ. Panel (a) of Figure 2 shows longitudinal data in a competing online labor market, and ratings are assigned by employers to workers (freelancers). Panel (b) plots longitudinal ratings data from four major cities in the United States and Europe in a large home-sharing platform. Home-sharing platforms are peer-to-peer marketplaces that facilitate short-term rentals for lodging (Filippas and Horton 2018). The ratings are by guests (those who are renting properties) to hosts (those who are renting out.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.1129242599998,
          "h": 9.962600000000002
        }
      }
    ]
  },
  {
    "page": 6,
    "text": "              Filippas, Horton, and Golden: Reputation Inflation\n              Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS                                                                                              5\n\n              Figure 2.    (Color online) Longitudinal Buyer-on-Seller Feedback Scores for a Collection of Online MarketplacesPublic numerical user-to-user feedback score\n                                        (a) Competing online labor market                                           (b) Home-sharing market\n                                                                                                4.75\n                            4.8                                                                 4.70\n\n                                                                                                4.65\n                            4.7\n                                                                                                4.60\n                            4.6                                                                 4.55\n\n                                 2007    2008   2009   2010   2011    2012   2013    2014                            2016                  2017\n\n                                     (c) Durable asset short-term rentals market                            (d) Geosegmented online service market\n                            4.9\n                                                                                                 0.2\n                            4.8\n                                                                                                 0.1\n                            4.7\n                                                                                                 0.0\n\n                                 2013                                      2014                       2014                2015                2016\n                                                                                               Year\n              Notes. This figure plots the average public feedback scores assigned in four online peer-to-peer marketplaces. In all markets, scores are assigned\n              upon the completion of each transaction, and the scale for feedback is one to five stars. Scores are assigned by employers to workers in panel (a),\n              by guests (users renting properties) to hosts (users renting out properties) in panel (b), by renters (those renting a durable asset) to providers\n              (those renting out the durable asset) in panel (c), and by customers to providers of a service in panel (d). The lines in panels (b) and (d) corre-\n              spond to different cities. In panel (d), average feedback scores for the time series of each city are normalized so that the mean score is equal to\n              zero during the first period of data collection. For each observation, average scores are computed for every time period, and a 95% interval is\n              depicted for every point estimate.\n\nproperties). Panel (c) plots numerical feedback data\nfrom an online marketplace that facilitates the short-\nterm rental of durable assets (Sundararajan 2013, Fili-\nppas et al. 2021). The ratings are by users (renters of\nassets) to users (providers of the assets) after the trans-\nactions have taken place. Panel (d) plots longitudinal\nratings data from six major cities in the United States\nin a large online marketplace for services (Hall et al.\n2021). The ratings are by consumers of the service to\nproviders of the service.\n   Despite the differences in the goods being transacted\nand the market mechanisms used, these marketplaces\ndo share similarities. Transactions in these marketplaces\nare personal (peer-to-peer rather than person-to-firm).\nFurthermore, the same basic reputation system design\nis used across markets—ratings are given after the\ntransaction has taken place and are consequential for\nthe rated party, and all platforms use simultaneous\nreveal to prevent tit-for-tat rating behavior.\n\n\n3. Descriptive Evidence for\n     Reputation Inflation\nThe increase in average feedback scores in a market\ncould be explained by two broad—but not mutually\nexclusive—sets of reasons: (1) rater satisfaction has\nincreased and (2) reputation inflation, that is, raters\nare not any more satisfied but simply give higher\nfeedback scores. If reason (2) is important, then it\nshould leave some clues in the data. In this section,\nwe examine some of these clues.\n\n3.1. Some Employers Are Not Very Satisfied and\n       Report Strategically\nIf improvements in platform fundamentals have left\nraters very pleased, we might expect alternative meas-\nures of rater satisfaction to show similar increases, at\nleast in direction. Of course, there is no immediate\nmapping from one measure of satisfaction to an other,\nbut if a person gives five stars in public but reports “it\nwas not very good” in private, then one might suspect\nthat the private measure is perhaps closer the rater’s\ntrue    feelings.      The     expressions        “don’t      shoot     the\nmessenger” or “I dare you to say that to my face” are\nsuggestive of why we might get more candor in pri-\nvate than in public.\n   Toward that end of receiving more candid evalua-\ntions, the platform running our focal marketplace con-\nducted an intervention that elicited an additional private\nfeedback measure of satisfaction. This feedback measure\nwas private in the sense that the platform let the employ-\ners know that private feedback would not be shared\nwith the workers or with other employers, and that it\nwould be collected by the platform only for internal",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\n# Figure 2. (Color online) Longitudinal Buyer-on-Seller Feedback Scores for a Collection of Online Marketplaces\n\n|Public numerical user-to-user feedback score|Public numerical user-to-user feedback score|\n|---|\n|(a) Competing online labor market|(b) Home-sharing market|\n|||\n|(c) Durable asset short-term rentals market|(d) Geosegmented online service market|\n|||\n\nNotes. This figure plots the average public feedback scores assigned in four online peer-to-peer marketplaces. In all markets, scores are assigned upon the completion of each transaction, and the scale for feedback is one to five stars. Scores are assigned by employers to workers in panel (a), by guests (users renting properties) to hosts (users renting out properties) in panel (b), by renters (those renting a durable asset) to providers (those renting out the durable asset) in panel (c), and by customers to providers of a service in panel (d). The lines in panels (b) and (d) correspond to different cities. In panel (d), average feedback scores for the time series of each city are normalized so that the mean score is equal to zero during the first period of data collection. For each observation, average scores are computed for every time period, and a 95% interval is depicted for every point estimate.\n\nPanel (c) plots numerical feedback data from an online marketplace that facilitates the short-term rental of durable assets (Sundararajan 2013, Filippas et al. 2021). The ratings are by users (renters of assets) to users (providers of the assets) after the transactions have taken place. Panel (d) plots longitudinal ratings data from six major cities in the United States in a large online marketplace for services (Hall et al. 2021). The ratings are by consumers of the service to providers of the service.\n\nDespite the differences in the goods being transacted and the market mechanisms used, these marketplaces do share similarities. Transactions in these marketplaces are personal (peer-to-peer rather than person-to-firm). Furthermore, the same basic reputation system design is used across markets—ratings are given after the transaction has taken place and are consequential for the rated party, and all platforms use simultaneous reveal to prevent tit-for-tat rating behavior.\n\n# 3. Descriptive Evidence for Reputation Inflation\n\nThe increase in average feedback scores in a market could be explained by two broad—but not mutually exclusive—sets of reasons: (1) rater satisfaction has increased and (2) reputation inflation, that is, raters are not any more satisfied but simply give higher feedback scores. If reason (2) is important, then it should leave some clues in the data. In this section, we examine some of these clues.\n\n# 3.1. Some Employers Are Not Very Satisfied and Report Strategically\n\nIf improvements in platform fundamentals have left raters very pleased, we might expect alternative measures of rater satisfaction to show similar increases, at least in direction. Of course, there is no immediate mapping from one measure of satisfaction to another, but if a person gives five stars in public but reports “it was not very good” in private, then one might suspect that the private measure is perhaps closer to the rater’s true feelings. The expressions “don’t shoot the messenger” or “I dare you to say that to my face” are suggestive of why we might get more candor in private than in public.\n\nToward that end of receiving more candid evaluations, the platform running our focal marketplace conducted an intervention that elicited an additional private feedback measure of satisfaction. This feedback measure was private in the sense that the platform let the employers know that private feedback would not be shared with the workers or with other employers, and that it would be collected by the platform only for internal purposes.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 11.9551
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 6.475700000000003
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 2. (Color online) Longitudinal Buyer-on-Seller Feedback Scores for a Collection of Online Marketplaces",
        "md": "# Figure 2. (Color online) Longitudinal Buyer-on-Seller Feedback Scores for a Collection of Online Marketplaces",
        "bBox": {
          "x": 45,
          "y": 75.87199999999996,
          "w": 385.0030590699992,
          "h": 8.966300000000004
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Public numerical user-to-user feedback score",
            "Public numerical user-to-user feedback score"
          ],
          ["(a) Competing online labor market", "(b) Home-sharing market"],
          ["", ""],
          [
            "(c) Durable asset short-term rentals market",
            "(d) Geosegmented online service market"
          ],
          ["", ""]
        ],
        "md": "|Public numerical user-to-user feedback score|Public numerical user-to-user feedback score|\n|---|\n|(a) Competing online labor market|(b) Home-sharing market|\n|||\n|(c) Durable asset short-term rentals market|(d) Geosegmented online service market|\n|||",
        "isPerfectTable": true,
        "csv": "\"Public numerical user-to-user feedback score\",\"Public numerical user-to-user feedback score\"\n\"(a) Competing online labor market\",\"(b) Home-sharing market\"\n\"\",\"\"\n\"(c) Durable asset short-term rentals market\",\"(d) Geosegmented online service market\"\n\"\",\"\""
      },
      {
        "type": "text",
        "value": "Notes. This figure plots the average public feedback scores assigned in four online peer-to-peer marketplaces. In all markets, scores are assigned upon the completion of each transaction, and the scale for feedback is one to five stars. Scores are assigned by employers to workers in panel (a), by guests (users renting properties) to hosts (users renting out properties) in panel (b), by renters (those renting a durable asset) to providers (those renting out the durable asset) in panel (c), and by customers to providers of a service in panel (d). The lines in panels (b) and (d) correspond to different cities. In panel (d), average feedback scores for the time series of each city are normalized so that the mean score is equal to zero during the first period of data collection. For each observation, average scores are computed for every time period, and a 95% interval is depicted for every point estimate.\n\nPanel (c) plots numerical feedback data from an online marketplace that facilitates the short-term rental of durable assets (Sundararajan 2013, Filippas et al. 2021). The ratings are by users (renters of assets) to users (providers of the assets) after the transactions have taken place. Panel (d) plots longitudinal ratings data from six major cities in the United States in a large online marketplace for services (Hall et al. 2021). The ratings are by consumers of the service to providers of the service.\n\nDespite the differences in the goods being transacted and the market mechanisms used, these marketplaces do share similarities. Transactions in these marketplaces are personal (peer-to-peer rather than person-to-firm). Furthermore, the same basic reputation system design is used across markets—ratings are given after the transaction has taken place and are consequential for the rated party, and all platforms use simultaneous reveal to prevent tit-for-tat rating behavior.",
        "md": "Notes. This figure plots the average public feedback scores assigned in four online peer-to-peer marketplaces. In all markets, scores are assigned upon the completion of each transaction, and the scale for feedback is one to five stars. Scores are assigned by employers to workers in panel (a), by guests (users renting properties) to hosts (users renting out properties) in panel (b), by renters (those renting a durable asset) to providers (those renting out the durable asset) in panel (c), and by customers to providers of a service in panel (d). The lines in panels (b) and (d) correspond to different cities. In panel (d), average feedback scores for the time series of each city are normalized so that the mean score is equal to zero during the first period of data collection. For each observation, average scores are computed for every time period, and a 95% interval is depicted for every point estimate.\n\nPanel (c) plots numerical feedback data from an online marketplace that facilitates the short-term rental of durable assets (Sundararajan 2013, Filippas et al. 2021). The ratings are by users (renters of assets) to users (providers of the assets) after the transactions have taken place. Panel (d) plots longitudinal ratings data from six major cities in the United States in a large online marketplace for services (Hall et al. 2021). The ratings are by consumers of the service to providers of the service.\n\nDespite the differences in the goods being transacted and the market mechanisms used, these marketplaces do share similarities. Transactions in these marketplaces are personal (peer-to-peer rather than person-to-firm). Furthermore, the same basic reputation system design is used across markets—ratings are given after the transaction has taken place and are consequential for the rated party, and all platforms use simultaneous reveal to prevent tit-for-tat rating behavior.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.9742984799998,
          "h": 9.9626
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "3. Descriptive Evidence for Reputation Inflation",
        "md": "# 3. Descriptive Evidence for Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 671.872,
          "w": 153.63977213999996,
          "h": 11.955100000000016
        }
      },
      {
        "type": "text",
        "value": "The increase in average feedback scores in a market could be explained by two broad—but not mutually exclusive—sets of reasons: (1) rater satisfaction has increased and (2) reputation inflation, that is, raters are not any more satisfied but simply give higher feedback scores. If reason (2) is important, then it should leave some clues in the data. In this section, we examine some of these clues.",
        "md": "The increase in average feedback scores in a market could be explained by two broad—but not mutually exclusive—sets of reasons: (1) rater satisfaction has increased and (2) reputation inflation, that is, raters are not any more satisfied but simply give higher feedback scores. If reason (2) is important, then it should leave some clues in the data. In this section, we examine some of these clues.",
        "bBox": {
          "x": 45,
          "y": 421.87199999999996,
          "w": 493.56417455999986,
          "h": 11.9551
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "3.1. Some Employers Are Not Very Satisfied and Report Strategically",
        "md": "# 3.1. Some Employers Are Not Very Satisfied and Report Strategically",
        "bBox": {
          "x": 301,
          "y": 481.87199999999996,
          "w": 231.66830787999993,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "If improvements in platform fundamentals have left raters very pleased, we might expect alternative measures of rater satisfaction to show similar increases, at least in direction. Of course, there is no immediate mapping from one measure of satisfaction to another, but if a person gives five stars in public but reports “it was not very good” in private, then one might suspect that the private measure is perhaps closer to the rater’s true feelings. The expressions “don’t shoot the messenger” or “I dare you to say that to my face” are suggestive of why we might get more candor in private than in public.\n\nToward that end of receiving more candid evaluations, the platform running our focal marketplace conducted an intervention that elicited an additional private feedback measure of satisfaction. This feedback measure was private in the sense that the platform let the employers know that private feedback would not be shared with the workers or with other employers, and that it would be collected by the platform only for internal purposes.",
        "md": "If improvements in platform fundamentals have left raters very pleased, we might expect alternative measures of rater satisfaction to show similar increases, at least in direction. Of course, there is no immediate mapping from one measure of satisfaction to another, but if a person gives five stars in public but reports “it was not very good” in private, then one might suspect that the private measure is perhaps closer to the rater’s true feelings. The expressions “don’t shoot the messenger” or “I dare you to say that to my face” are suggestive of why we might get more candor in private than in public.\n\nToward that end of receiving more candid evaluations, the platform running our focal marketplace conducted an intervention that elicited an additional private feedback measure of satisfaction. This feedback measure was private in the sense that the platform let the employers know that private feedback would not be shared with the workers or with other employers, and that it would be collected by the platform only for internal purposes.",
        "bBox": {
          "x": 301,
          "y": 505.87199999999996,
          "w": 237.7884422200001,
          "h": 9.9626
        }
      }
    ]
  },
  {
    "page": 7,
    "text": "6\nevaluation purposes, such as to determine whether their\nrecommendation systems needed to be improved. As\nwith public feedback, this private feedback was elicited\nat the completion of a contract and was asked in addi-\ntion to public feedback.\n    Employers were initially asked the private feedback\nquestion,        “Would you hire this freelancer [worker]\nagain, if you had a similar project?”                        Starting on June\n2014, employers were instead asked to rate workers\non a numerical scale of 0 to 10, answering the question\n“How likely are you to recommend this freelancer to\na friend or colleague?” The private feedback question\nwas simply appended to the end of the public feed-\nback form. Employers assigned both public and pri-\nvate feedback for the same contract.\n    Figure 3 shows the distribution of public feedback,\nconditioned on the private feedback. The percentage\nof employers giving that feedback score is shown in\nparentheses in each panel’s label. Although the most\ncommon response to the question of “would you hire\nthis freelancer again” was “Definitely Yes,” about 15%\nof the employers gave unambiguously bad private\nfeedback (“Definitely Not”                     and    “Probably Not”). In\ncontrast, during the same period, less than 4% of the\nemployers gave a numerical score of three stars or\nless. Given this gap, we might suspect that some\nemployers expressing a negative private sentiment\nare less candid in public.\n    Employers who leave more negative private feed-\nback assign lower public feedback scores, but many\nstill    give     perfect       public       feedback         scores.      Among\nemployers who selected the                     “Definitely Not” answer\nto the private feedback question, 29.1% assigned a\none-star rating publicly. However, the second most\ncommon choice for these employers at 15.7% was in\nthe 4.75 to 5.00 bin, and 28.4% publicly assigned more\nthan four stars. In short, many privately dissatisfied\nemployers publicly claimed to be satisfied. We can see\nthat the reverse—privately satisfied employers giving\nbad       public       feedback—essentially                  never       happens.\nEmployers who selected                      “Definitely Yes”             left very\npositive public feedback: none of these employers\nassigned less than 3.75 stars, and more than 95% of\nthe observations fell into the highest bin.\n\n3.2. Average Private Feedback Sentiment\n        Decreased Whereas Public Feedback\n        Sentiment Increased for the Same\n        Transactions\nIf private feedback scores were measuring rater satis-\nfaction, then we would expect these two measures to\ncovary over time: fundamental improvements that\nmade raters happier should show up both in public\nand private ratings. In contrast, if one measure of sat-\nisfaction was inflating but the other was not, we could\nsee a divergence.\n                       Filippas, Horton, and Golden: Reputation Inflation\n              Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n    In Figure 4, we see a divergence between public\nand private scores, with public scores rising while pri-\nvate scores were falling. The                    figure reports the aver-\nage monthly feedback over time, for the numerical\npublic and private feedback (when the private feed-\nback scale was numeric and 1 to 10). The x-axis covers\nthe period when both were collected. To make the\ntwo scores comparable, we normalize them by their\nrespective means in the first period when they were\nboth collected, that is, (s¯ t −¯s0)=¯s0 , where¯s t is the aver-\nage     feedback         in   month        t.   Public      feedback         scores\nexhibit a small increase during the period of interest\n(as we saw in Figure 1(b)), whereas private feedback\nscores exhibit a strong decreasing trend. Overall, the\ndivergence in the two scores at the end of the nine-\nmonth period is 3.5 percentage points.\n    It is critical to note that the average feedback scores\nshown in Figure 4 are being assigned by                                  the same\nemployers on the same contracts. The decreasing private\nfeedback scores would seemingly suggest a decline in\nrater     satisfaction,        and      yet     public      feedback         scores\nincreased. This divergence in trends suggests that the\npublic feedback scores were increasing at least in part\nbecause of reputation inflation, assuming the private\nfeedback score was not deflating.\n    An alternative explanation for the divergence is that\nthe elicitation of private feedback somehow affected\nhow employers assigned other types of feedback. For\nexample, suppose employers who had negative experi-\nences assigned workers bad private feedback scores\ninstead of bad public feedback scores, perhaps to blow\noff steam. However, we view this as unlikely, as the pri-\nvate feedback was elicited simply by appending one\nadditional question at the end of the feedback screen\n(see the online appendix, Section A.1). Even if employ-\ners read further down the page and considered both\nfeedback decisions jointly, we would expect to see\neither a discontinuity in public feedback score averages\nor a change in the rate of their increase, when private\nfeedback was           first elicited. We see no such pattern in\nthe public feedback scores (see Figure 1(b)) or in the\nsentiment         expressed         in    written       employer         feedback\n(which we will show in Section 4). We provide addi-\ntional robustness checks in the online appendix, Section\nA.1, that rule out other conjectures that could rational-\nize the divergent trends, such as the possibility that\nworkers misunderstood and misused private ratings.\n\n3.3. The Same Written Sentences Are Associated\n        with Much Higher Ratings Now Than in\n        the Past\nThe telltale sign of reputation inflation is raters rating\nmore       positively        without        being      more       satisfied.      An\nalternative        measure         of    rater     satisfaction        can    come\nfrom the written feedback employers leave after each\ntransaction. If we think a distinct piece of written",
    "md": "# 3.2. Average Private Feedback Sentiment Decreased Whereas Public Feedback Sentiment Increased for the Same Transactions\n\nIf private feedback scores were measuring rater satisfaction, then we would expect these two measures to covary over time: fundamental improvements that made raters happier should show up both in public and private ratings. In contrast, if one measure of satisfaction was inflating but the other was not, we could see a divergence.\n\nIn Figure 4, we see a divergence between public and private scores, with public scores rising while private scores were falling. The figure reports the average monthly feedback over time, for the numerical public and private feedback (when the private feedback scale was numeric and 1 to 10). The x-axis covers the period when both were collected. To make the two scores comparable, we normalize them by their respective means in the first period when they were both collected, that is, (s¯ t −¯s0)=¯s0, where¯s t is the average feedback in month t. Public feedback scores exhibit a small increase during the period of interest (as we saw in Figure 1(b)), whereas private feedback scores exhibit a strong decreasing trend. Overall, the divergence in the two scores at the end of the nine-month period is 3.5 percentage points.\n\nIt is critical to note that the average feedback scores shown in Figure 4 are being assigned by the same employers on the same contracts. The decreasing private feedback scores would seemingly suggest a decline in rater satisfaction, and yet public feedback scores increased. This divergence in trends suggests that the public feedback scores were increasing at least in part because of reputation inflation, assuming the private feedback score was not deflating.\n\nAn alternative explanation for the divergence is that the elicitation of private feedback somehow affected how employers assigned other types of feedback. For example, suppose employers who had negative experiences assigned workers bad private feedback scores instead of bad public feedback scores, perhaps to blow off steam. However, we view this as unlikely, as the private feedback was elicited simply by appending one additional question at the end of the feedback screen (see the online appendix, Section A.1). Even if employers read further down the page and considered both feedback decisions jointly, we would expect to see either a discontinuity in public feedback score averages or a change in the rate of their increase, when private feedback was first elicited. We see no such pattern in the public feedback scores (see Figure 1(b)) or in the sentiment expressed in written employer feedback (which we will show in Section 4). We provide additional robustness checks in the online appendix, Section A.1, that rule out other conjectures that could rationalize the divergent trends, such as the possibility that workers misunderstood and misused private ratings.\n\n# 3.3. The Same Written Sentences Are Associated with Much Higher Ratings Now Than in the Past\n\nThe telltale sign of reputation inflation is raters rating more positively without being more satisfied. An alternative measure of rater satisfaction can come from the written feedback employers leave after each transaction. If we think a distinct piece of written",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "3.2. Average Private Feedback Sentiment Decreased Whereas Public Feedback Sentiment Increased for the Same Transactions",
        "md": "# 3.2. Average Private Feedback Sentiment Decreased Whereas Public Feedback Sentiment Increased for the Same Transactions",
        "bBox": {
          "x": 45,
          "y": 196.87199999999996,
          "w": 295.6958,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "If private feedback scores were measuring rater satisfaction, then we would expect these two measures to covary over time: fundamental improvements that made raters happier should show up both in public and private ratings. In contrast, if one measure of satisfaction was inflating but the other was not, we could see a divergence.\n\nIn Figure 4, we see a divergence between public and private scores, with public scores rising while private scores were falling. The figure reports the average monthly feedback over time, for the numerical public and private feedback (when the private feedback scale was numeric and 1 to 10). The x-axis covers the period when both were collected. To make the two scores comparable, we normalize them by their respective means in the first period when they were both collected, that is, (s¯ t −¯s0)=¯s0, where¯s t is the average feedback in month t. Public feedback scores exhibit a small increase during the period of interest (as we saw in Figure 1(b)), whereas private feedback scores exhibit a strong decreasing trend. Overall, the divergence in the two scores at the end of the nine-month period is 3.5 percentage points.\n\nIt is critical to note that the average feedback scores shown in Figure 4 are being assigned by the same employers on the same contracts. The decreasing private feedback scores would seemingly suggest a decline in rater satisfaction, and yet public feedback scores increased. This divergence in trends suggests that the public feedback scores were increasing at least in part because of reputation inflation, assuming the private feedback score was not deflating.\n\nAn alternative explanation for the divergence is that the elicitation of private feedback somehow affected how employers assigned other types of feedback. For example, suppose employers who had negative experiences assigned workers bad private feedback scores instead of bad public feedback scores, perhaps to blow off steam. However, we view this as unlikely, as the private feedback was elicited simply by appending one additional question at the end of the feedback screen (see the online appendix, Section A.1). Even if employers read further down the page and considered both feedback decisions jointly, we would expect to see either a discontinuity in public feedback score averages or a change in the rate of their increase, when private feedback was first elicited. We see no such pattern in the public feedback scores (see Figure 1(b)) or in the sentiment expressed in written employer feedback (which we will show in Section 4). We provide additional robustness checks in the online appendix, Section A.1, that rule out other conjectures that could rationalize the divergent trends, such as the possibility that workers misunderstood and misused private ratings.",
        "md": "If private feedback scores were measuring rater satisfaction, then we would expect these two measures to covary over time: fundamental improvements that made raters happier should show up both in public and private ratings. In contrast, if one measure of satisfaction was inflating but the other was not, we could see a divergence.\n\nIn Figure 4, we see a divergence between public and private scores, with public scores rising while private scores were falling. The figure reports the average monthly feedback over time, for the numerical public and private feedback (when the private feedback scale was numeric and 1 to 10). The x-axis covers the period when both were collected. To make the two scores comparable, we normalize them by their respective means in the first period when they were both collected, that is, (s¯ t −¯s0)=¯s0, where¯s t is the average feedback in month t. Public feedback scores exhibit a small increase during the period of interest (as we saw in Figure 1(b)), whereas private feedback scores exhibit a strong decreasing trend. Overall, the divergence in the two scores at the end of the nine-month period is 3.5 percentage points.\n\nIt is critical to note that the average feedback scores shown in Figure 4 are being assigned by the same employers on the same contracts. The decreasing private feedback scores would seemingly suggest a decline in rater satisfaction, and yet public feedback scores increased. This divergence in trends suggests that the public feedback scores were increasing at least in part because of reputation inflation, assuming the private feedback score was not deflating.\n\nAn alternative explanation for the divergence is that the elicitation of private feedback somehow affected how employers assigned other types of feedback. For example, suppose employers who had negative experiences assigned workers bad private feedback scores instead of bad public feedback scores, perhaps to blow off steam. However, we view this as unlikely, as the private feedback was elicited simply by appending one additional question at the end of the feedback screen (see the online appendix, Section A.1). Even if employers read further down the page and considered both feedback decisions jointly, we would expect to see either a discontinuity in public feedback score averages or a change in the rate of their increase, when private feedback was first elicited. We see no such pattern in the public feedback scores (see Figure 1(b)) or in the sentiment expressed in written employer feedback (which we will show in Section 4). We provide additional robustness checks in the online appendix, Section A.1, that rule out other conjectures that could rationalize the divergent trends, such as the possibility that workers misunderstood and misused private ratings.",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 493.72235997999996,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "3.3. The Same Written Sentences Are Associated with Much Higher Ratings Now Than in the Past",
        "md": "# 3.3. The Same Written Sentences Are Associated with Much Higher Ratings Now Than in the Past",
        "bBox": {
          "x": 301,
          "y": 196.87199999999996,
          "w": 234.73379990000012,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "The telltale sign of reputation inflation is raters rating more positively without being more satisfied. An alternative measure of rater satisfaction can come from the written feedback employers leave after each transaction. If we think a distinct piece of written",
        "md": "The telltale sign of reputation inflation is raters rating more positively without being more satisfied. An alternative measure of rater satisfaction can come from the written feedback employers leave after each transaction. If we think a distinct piece of written",
        "bBox": {
          "x": 167,
          "y": 196.87199999999996,
          "w": 237.50041391999983,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 8,
    "text": "Filippas, Horton, and Golden: Reputation Inflation\nMarketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS                                                                                             7\n\nFigure 3.    (Color online) Distribution of Public Employer-on-Worker Feedback, by Employers’ Response to the Private Feedback\nQuestion, “Would You Hire This Freelancer [Worker] Again, if You Had a Similar Project?”Percentage of observations\n                                           Response: Definitely Not (6.6% reporting this tier)Feedback score differences100%\n                75%\n                50%    23.5%                                                                                                          3.0% 18.2%\n                25%             1.2%   3.2%   3.7%   5.0%    3.9% 3.9%     4.0%    7.1%   3.5%   3.5%    3.2%   6.2%   4.1%    2.6%\n                  0%\n                                           Response: Probably Not (8.3% reporting this tier)\n               100%\n                75%\n                50%                                                                                                                         30.6%\n                25%     1.5%    0.1%   0.4%   0.6%   1.9%    1.4%   1.8%   2.7%    7.8%   4.0%   4.9%    5.6% 11.9% 9.1%       6.8%   8.9%\n                  0%\n                                          Response: Probably Yes (19.4% reporting this tier)\n               100%                                                                                                                         66.7%\n                75%\n                50%\n                25%     0.1%    0.0% 0.0%     0.0%   0.1%    0.0%   0.1%   0.1%    1.2%   0.5%   1.0%    1.6%   7.6%   4.3%    6.2% 10.5%\n                  0%\n                                          Response: Definitely Yes (65.7% reporting this tier)\n               100%                                                                                                                         95.1%\n                75%\n                50%\n                25%     0.0%    0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%                 0.0%   0.0%   0.0%    0.1%   0.6%   0.4%    1.1%   2.6%\n                  0%\n\n                    (0.75,1](1.25,1.5](1,1.25](1.5,1.75](1.75,2](2.25,2.5](2,2.25](2.5,2.75](2.75,3](3.25,3.5](3,3.25](3.5,3.75](3.75,4](4.25,4.5](4,4.25](4.5,4.75](4.75,5]\n                                                              Public feedback scores\nNotes. This figure plots the distribution of public feedback scores, computed separately for every set of users that gave the same answer to the\nprivate feedback question. The dashed line in each panel plots the cumulative distribution function.\n\nfeedback—say,          “good job”—reflects an unchanging                          2008 and 2015 and find all lexically identical sentences\nlevel of rater utility, then we can see whether the                               generated during these periods. We then compare\nnumerical       rating     has    changed       over     time    for   this       average feedback by sentence across the two periods.\nphrase. To this end, we select written feedback from                              Figure 5 shows the average numerical feedback scores\n\nFigure 4.    (Color online) Numerical Public Feedback Score and Private Feedback Score\n\n                          1%                                                                                           avg. public\n                                                                                                                        feedback\n\n                          0%\n\n                         −1%\n\n                         −2%                                                                                          avg. private\n                                                                                                                        feedback\n                         −3%     Jun 14     Jul 14   Aug 14    Sep 14     Oct 14   Nov 14    Dec 14     Jan 15    Feb 15\n                                                                                Month\n\nNotes. This figure shows the evolution of the average public feedback scores (solid line) versus the average private feedback scores (dashed line)\nassigned by employers to workers, for the same contracts. The average scores are computed for every month and are normalized by the value of\ntheir respective first observation. A 95% confidence interval is shown for each mean.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\n# Figure 3. (Color online) Distribution of Public Employer-on-Worker Feedback, by Employers’ Response to the Private Feedback Question, “Would You Hire This Freelancer [Worker] Again, if You Had a Similar Project?”\n\n|Response|Percentage of observations|Feedback score differences|\n|---|---|---|\n|Definitely Not (6.6% reporting this tier)|100%|- 75%\n- 50% 23.5%\n- 3.0%\n- 18.2%\n- 25% 1.2%\n- 3.2%\n- 3.7%\n- 5.0%\n- 3.9%\n- 3.9%\n- 4.0%\n- 7.1%\n- 3.5%\n- 3.5%\n- 3.2%\n- 6.2%\n- 4.1%\n- 2.6%\n|\n|Probably Not (8.3% reporting this tier)|100%|- 75%\n- 50% 30.6%\n- 25% 1.5%\n- 0.1%\n- 0.4%\n- 0.6%\n- 1.9%\n- 1.4%\n- 1.8%\n- 2.7%\n- 7.8%\n- 4.0%\n- 4.9%\n- 5.6%\n- 11.9%\n- 9.1%\n- 6.8%\n- 8.9%\n|\n|Probably Yes (19.4% reporting this tier)|100%|- 75%\n- 50% 66.7%\n- 25% 0.1%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.0%\n- 0.1%\n- 0.1%\n- 1.2%\n- 0.5%\n- 1.0%\n- 1.6%\n- 7.6%\n- 4.3%\n- 6.2%\n- 10.5%\n|\n|Definitely Yes (65.7% reporting this tier)|100%|- 75%\n- 50% 95.1%\n- 25% 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.6%\n- 0.4%\n- 1.1%\n- 2.6%\n|\n\n(0.75,1](1.25,1.5](1,1.25](1.5,1.75](1.75,2](2.25,2.5](2,2.25](2.5,2.75](2.75,3](3.25,3.5](3,3.25](3.5,3.75](3.75,4](4.25,4.5](4,4.25](4.5,4.75](4.75,5]\n\n# Notes\n\nThis figure plots the distribution of public feedback scores, computed separately for every set of users that gave the same answer to the private feedback question. The dashed line in each panel plots the cumulative distribution function.\n\nfeedback—say, “good job”—reflects an unchanging level of rater utility, then we can see whether the numerical rating has changed over time for this average feedback by sentence across the two periods. We then compare numerical feedback scores.\n\n# Figure 4. (Color online) Numerical Public Feedback Score and Private Feedback Score\n\n|Month|avg. public feedback|avg. private feedback|\n|---|---|---|\n|Jun 14|1%|0%|\n|Jul 14|0%|-1%|\n|Aug 14|-2%|-3%|\n|Sep 14|0%|0%|\n|Oct 14|0%|0%|\n|Nov 14|0%|0%|\n|Dec 14|0%|0%|\n|Jan 15|0%|0%|\n|Feb 15|0%|0%|\n\nNotes. This figure shows the evolution of the average public feedback scores (solid line) versus the average private feedback scores (dashed line) assigned by employers to workers, for the same contracts. The average scores are computed for every month and are normalized by the value of their respective first observation. A 95% confidence interval is shown for each mean.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 7.471800000000002
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 6.475700000000003
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 3. (Color online) Distribution of Public Employer-on-Worker Feedback, by Employers’ Response to the Private Feedback Question, “Would You Hire This Freelancer [Worker] Again, if You Had a Similar Project?”",
        "md": "# Figure 3. (Color online) Distribution of Public Employer-on-Worker Feedback, by Employers’ Response to the Private Feedback Question, “Would You Hire This Freelancer [Worker] Again, if You Had a Similar Project?”",
        "bBox": {
          "x": 45,
          "y": 75.87199999999996,
          "w": 452.34989460999986,
          "h": 9.9626
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Response",
            "Percentage of observations",
            "Feedback score differences"
          ],
          ["Definitely Not (6.6% reporting this tier)", "100%"]
        ],
        "md": "|Response|Percentage of observations|Feedback score differences|\n|---|---|---|\n|Definitely Not (6.6% reporting this tier)|100%|- 75%",
        "isPerfectTable": false,
        "csv": "\"Response\",\"Percentage of observations\",\"Feedback score differences\"\n\"Definitely Not (6.6% reporting this tier)\",\"100%\""
      },
      {
        "type": "text",
        "value": "- 50% 23.5%\n- 3.0%\n- 18.2%\n- 25% 1.2%\n- 3.2%\n- 3.7%\n- 5.0%\n- 3.9%\n- 3.9%\n- 4.0%\n- 7.1%\n- 3.5%\n- 3.5%\n- 3.2%\n- 6.2%\n- 4.1%\n- 2.6%",
        "md": "- 50% 23.5%\n- 3.0%\n- 18.2%\n- 25% 1.2%\n- 3.2%\n- 3.7%\n- 5.0%\n- 3.9%\n- 3.9%\n- 4.0%\n- 7.1%\n- 3.5%\n- 3.5%\n- 3.2%\n- 6.2%\n- 4.1%\n- 2.6%",
        "bBox": {
          "x": 96,
          "y": 51.87199999999996,
          "w": 441.9813,
          "h": 9.9626
        }
      },
      {
        "type": "table",
        "rows": [[], ["Probably Not (8.3% reporting this tier)", "100%"]],
        "md": "|\n|Probably Not (8.3% reporting this tier)|100%|- 75%",
        "isPerfectTable": false,
        "csv": "\n\"Probably Not (8.3% reporting this tier)\",\"100%\""
      },
      {
        "type": "text",
        "value": "- 50% 30.6%\n- 25% 1.5%\n- 0.1%\n- 0.4%\n- 0.6%\n- 1.9%\n- 1.4%\n- 1.8%\n- 2.7%\n- 7.8%\n- 4.0%\n- 4.9%\n- 5.6%\n- 11.9%\n- 9.1%\n- 6.8%\n- 8.9%",
        "md": "- 50% 30.6%\n- 25% 1.5%\n- 0.1%\n- 0.4%\n- 0.6%\n- 1.9%\n- 1.4%\n- 1.8%\n- 2.7%\n- 7.8%\n- 4.0%\n- 4.9%\n- 5.6%\n- 11.9%\n- 9.1%\n- 6.8%\n- 8.9%",
        "bBox": {
          "x": 96,
          "y": 51.87199999999996,
          "w": 441.9813,
          "h": 9.9626
        }
      },
      {
        "type": "table",
        "rows": [[], ["Probably Yes (19.4% reporting this tier)", "100%"]],
        "md": "|\n|Probably Yes (19.4% reporting this tier)|100%|- 75%",
        "isPerfectTable": false,
        "csv": "\n\"Probably Yes (19.4% reporting this tier)\",\"100%\""
      },
      {
        "type": "text",
        "value": "- 50% 66.7%\n- 25% 0.1%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.0%\n- 0.1%\n- 0.1%\n- 1.2%\n- 0.5%\n- 1.0%\n- 1.6%\n- 7.6%\n- 4.3%\n- 6.2%\n- 10.5%",
        "md": "- 50% 66.7%\n- 25% 0.1%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.0%\n- 0.1%\n- 0.1%\n- 1.2%\n- 0.5%\n- 1.0%\n- 1.6%\n- 7.6%\n- 4.3%\n- 6.2%\n- 10.5%",
        "bBox": {
          "x": 96,
          "y": 51.87199999999996,
          "w": 441.9813,
          "h": 9.9626
        }
      },
      {
        "type": "table",
        "rows": [[], ["Definitely Yes (65.7% reporting this tier)", "100%"]],
        "md": "|\n|Definitely Yes (65.7% reporting this tier)|100%|- 75%",
        "isPerfectTable": false,
        "csv": "\n\"Definitely Yes (65.7% reporting this tier)\",\"100%\""
      },
      {
        "type": "text",
        "value": "- 50% 95.1%\n- 25% 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.6%\n- 0.4%\n- 1.1%\n- 2.6%",
        "md": "- 50% 95.1%\n- 25% 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.0%\n- 0.1%\n- 0.6%\n- 0.4%\n- 1.1%\n- 2.6%",
        "bBox": {
          "x": 96,
          "y": 146.87199999999996,
          "w": 21.838490100000005,
          "h": 8.4547
        }
      },
      {
        "type": "table",
        "rows": [[]],
        "md": "|",
        "isPerfectTable": true,
        "csv": ""
      },
      {
        "type": "text",
        "value": "(0.75,1](1.25,1.5](1,1.25](1.5,1.75](1.75,2](2.25,2.5](2,2.25](2.5,2.75](2.75,3](3.25,3.5](3,3.25](3.5,3.75](3.75,4](4.25,4.5](4,4.25](4.5,4.75](4.75,5]",
        "md": "(0.75,1](1.25,1.5](1,1.25](1.5,1.75](1.75,2](2.25,2.5](2,2.25](2.5,2.75](2.75,3](3.25,3.5](3,3.25](3.5,3.75](3.75,4](4.25,4.5](4,4.25](4.5,4.75](4.75,5]",
        "bBox": {
          "x": 107,
          "y": 51.87199999999996,
          "w": 430.9813,
          "h": 19.076252451816345
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Notes",
        "md": "# Notes",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 584.674
        }
      },
      {
        "type": "text",
        "value": "This figure plots the distribution of public feedback scores, computed separately for every set of users that gave the same answer to the private feedback question. The dashed line in each panel plots the cumulative distribution function.\n\nfeedback—say, “good job”—reflects an unchanging level of rater utility, then we can see whether the numerical rating has changed over time for this average feedback by sentence across the two periods. We then compare numerical feedback scores.",
        "md": "This figure plots the distribution of public feedback scores, computed separately for every set of users that gave the same answer to the private feedback question. The dashed line in each panel plots the cumulative distribution function.\n\nfeedback—say, “good job”—reflects an unchanging level of rater utility, then we can see whether the numerical rating has changed over time for this average feedback by sentence across the two periods. We then compare numerical feedback scores.",
        "bBox": {
          "x": 45,
          "y": 436.87199999999996,
          "w": 338.7382821199998,
          "h": 14.4937
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 4. (Color online) Numerical Public Feedback Score and Private Feedback Score",
        "md": "# Figure 4. (Color online) Numerical Public Feedback Score and Private Feedback Score",
        "bBox": {
          "x": 45,
          "y": 515.872,
          "w": 291.78492111999975,
          "h": 9.962599999999952
        }
      },
      {
        "type": "table",
        "rows": [
          ["Month", "avg. public feedback", "avg. private feedback"],
          ["Jun 14", "1%", "0%"],
          ["Jul 14", "0%", "-1%"],
          ["Aug 14", "-2%", "-3%"],
          ["Sep 14", "0%", "0%"],
          ["Oct 14", "0%", "0%"],
          ["Nov 14", "0%", "0%"],
          ["Dec 14", "0%", "0%"],
          ["Jan 15", "0%", "0%"],
          ["Feb 15", "0%", "0%"]
        ],
        "md": "|Month|avg. public feedback|avg. private feedback|\n|---|---|---|\n|Jun 14|1%|0%|\n|Jul 14|0%|-1%|\n|Aug 14|-2%|-3%|\n|Sep 14|0%|0%|\n|Oct 14|0%|0%|\n|Nov 14|0%|0%|\n|Dec 14|0%|0%|\n|Jan 15|0%|0%|\n|Feb 15|0%|0%|",
        "isPerfectTable": true,
        "csv": "\"Month\",\"avg. public feedback\",\"avg. private feedback\"\n\"Jun 14\",\"1%\",\"0%\"\n\"Jul 14\",\"0%\",\"-1%\"\n\"Aug 14\",\"-2%\",\"-3%\"\n\"Sep 14\",\"0%\",\"0%\"\n\"Oct 14\",\"0%\",\"0%\"\n\"Nov 14\",\"0%\",\"0%\"\n\"Dec 14\",\"0%\",\"0%\"\n\"Jan 15\",\"0%\",\"0%\"\n\"Feb 15\",\"0%\",\"0%\""
      },
      {
        "type": "text",
        "value": "Notes. This figure shows the evolution of the average public feedback scores (solid line) versus the average private feedback scores (dashed line) assigned by employers to workers, for the same contracts. The average scores are computed for every month and are normalized by the value of their respective first observation. A 95% confidence interval is shown for each mean.",
        "md": "Notes. This figure shows the evolution of the average public feedback scores (solid line) versus the average private feedback scores (dashed line) assigned by employers to workers, for the same contracts. The average scores are computed for every month and are normalized by the value of their respective first observation. A 95% confidence interval is shown for each mean.",
        "bBox": {
          "x": 45,
          "y": 436.87199999999996,
          "w": 493.6901190999997,
          "h": 14.4937
        }
      }
    ]
  },
  {
    "page": 9,
    "text": "                                                                                                                                                                                                                                                                                                        Filippas, Horton, and Golden: Reputation Inflation\n8                                                                                                                                                                                                                                                                       Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\nfor a set of commonly used short sentences across                                                                                                                                                                          4.1. Method for Decomposing Changes in\nthese two periods. We select sentences spanning both                                                                                                                                                                                                  Average Feedback ScoresMean feedback scoreMean feedback scoregood and bad feedback, and which most frequentlyLet u denote the utility a rater obtains after some\noccurred in the corresponding written feedback in our                                                                                                                                                                      transaction.\ndata.                                                                                                                                                                                                                      Assumption 1.                                                                         In response obtaining utility u, the rater\n            Figure 5 shows that the numerical feedback scores                                                                                                                                                              leaves primary feedback.\nassociated                                                    with                           identical                                       sentences      have                            increased\nconsiderably over time, and that this increase has                                                                                                                                                                                                                                                                                       s  σ(u) + ,                                                                                      (1)\naffected both positive and negative sentences. This                                                                                                                                                                        where                              σ(·)                    is common among raters and monotonically\npattern is consistent with greater reputation inflation                                                                                                                                                                    increasing in the latent utility, that is, σ′ (u) > 0                                                                                                                                              for all u.\nin the numerical feedback score.                                                                                                                                                                                           Furthermore, E[ | u]  0 for all u, and so the expected score\n4. Quantifying the Contribution of                                                                                                                                                                                         conditional upon the latent utility is\n                  Reputation Inflation                                                                                                                                                                                                                                                                                              E[s | u]  σ(u):                                                                                       (2)\nThe private feedback and written text comparisons in                                                                                                                                                                       In addition to this primary feedback, raters also leave\nSection 3 suggest an approach to quantifying rep-                                                                                                                                                                          alternative feedback on the same transaction.\nutation inflation:                                                                                find some alternative measure of                                                                                         Assumption 2.                                                                       The alternative feedback is such that after\nsatisfaction and compare that to the primary measure                                                                                                                                                                       each transaction, a  α(u), where α′ (u) > 0 for all u.\nsuspected of inflating. We formalize this approach\nand show how using alternative feedback measures                                                                                                                                                                                       Assumption 1 describes a data-generating process\ncircumvents the problem of estimating latent utilities                                                                                                                                                                     that is arguably a precondition for a useful reputation\nfrom observational data, and hence allows us to net                                                                                                                                                                        system. If different subjective ratings could not be use-\nout                       increases                                               in                average                                      feedback      scores                                  due       to        fully aggregated to some common scale, it is unclear\nimprovements in marketplace fundamentals. We then                                                                                                                                                                          how the system could convey information. This data-\nuse our method to quantify the importance of reputa-                                                                                                                                                                       generating process seems particularly appropriate for\ntion inflation in our focal marketplace using written                                                                                                                                                                      market settings where ratings do not simply reflect var-\nfeedback.                                                                                                                                                                                                                  iation in taste, but rather depend on some notion of\n\nFigure 5.                                        Difference over Time in the Feedback Scores Associated with Identical Sentences\n\n                                                                                                                           good job                                   highly recommended                                                                             thank you                                                                                           would hire again\n                                                                     5.0\n\n                                                                     4.9\n\n                                                                     4.8\n\n                                                                     4.7\n\n                                                                     4.6\n\n                                                                                                                               terrible                                               unresponsive                                               would not hire again                                                                                          would not recommend\n\n                                                                     2.5\n\n                                                                     2.0\n\n                                                                     1.5\n\n                                                                     1.0\n                                                                                                          2008                                        2015                    2008                          2015                                      200 8                                             2015                                                           2008                                             2015\n                                                                                                                                                                                                                     Year\nNotes. This figure shows the average numerical feedback associated with identical sentences found in the text of employer-on-worker written\nfeedback in 2008 and 2015. The sentences plotted are the four most common sentences associated with high feedback scores and the four most\ncommon sentences associated with low feedback scores. A 95% confidence interval is shown for each mean.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# 4. Quantifying the Contribution of Reputation Inflation\n\nfor a set of commonly used short sentences across these two periods. We select sentences spanning both good and bad feedback, and which most frequently occurred in the corresponding written feedback in our data.\n\nLet u denote the utility a rater obtains after some transaction.\n\nAssumption 1. In response obtaining utility u, the rater leaves primary feedback.\n\nFigure 5 shows that the numerical feedback scores associated with identical sentences have increased considerably over time, and that this increase has affected both positive and negative sentences. This pattern is consistent with greater reputation inflation in the numerical feedback score.\n\nFurthermore, E[ | u]  0 for all u, and so the expected score conditional upon the latent utility is E[s | u]  σ(u):\n\nIn addition to this primary feedback, raters also leave alternative feedback on the same transaction.\n\nAssumption 2. The alternative feedback is such that after each transaction, a  α(u), where α′ (u) > 0 for all u.\n\nAssumption 1 describes a data-generating process that is arguably a precondition for a useful reputation system. If different subjective ratings could not be fully aggregated to some common scale, it is unclear how the system could convey information. This data-generating process seems particularly appropriate for market settings where ratings do not simply reflect variation in taste, but rather depend on some notion of.\n\n# Figure 5. Difference over Time in the Feedback Scores Associated with Identical Sentences\n\n|Year|Good Feedback|Bad Feedback|\n|---|---|---|\n|2008|5.0|2.5|\n|2015|4.9|2.0|\n|2008|4.8|1.5|\n|2015|4.7|1.0|\n|2008|4.6| |\n\nNotes. This figure shows the average numerical feedback associated with identical sentences found in the text of employer-on-worker written feedback in 2008 and 2015. The sentences plotted are the four most common sentences associated with high feedback scores and the four most common sentences associated with low feedback scores. A 95% confidence interval is shown for each mean.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 60,
          "y": 42.87199999999996,
          "w": 478.77079266,
          "h": 11.9551
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "4. Quantifying the Contribution of Reputation Inflation",
        "md": "# 4. Quantifying the Contribution of Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 221.87199999999996,
          "w": 191.40115100000006,
          "h": 11.9551
        }
      },
      {
        "type": "text",
        "value": "for a set of commonly used short sentences across these two periods. We select sentences spanning both good and bad feedback, and which most frequently occurred in the corresponding written feedback in our data.\n\nLet u denote the utility a rater obtains after some transaction.\n\nAssumption 1. In response obtaining utility u, the rater leaves primary feedback.\n\nFigure 5 shows that the numerical feedback scores associated with identical sentences have increased considerably over time, and that this increase has affected both positive and negative sentences. This pattern is consistent with greater reputation inflation in the numerical feedback score.\n\nFurthermore, E[ | u]  0 for all u, and so the expected score conditional upon the latent utility is E[s | u]  σ(u):\n\nIn addition to this primary feedback, raters also leave alternative feedback on the same transaction.\n\nAssumption 2. The alternative feedback is such that after each transaction, a  α(u), where α′ (u) > 0 for all u.\n\nAssumption 1 describes a data-generating process that is arguably a precondition for a useful reputation system. If different subjective ratings could not be fully aggregated to some common scale, it is unclear how the system could convey information. This data-generating process seems particularly appropriate for market settings where ratings do not simply reflect variation in taste, but rather depend on some notion of.",
        "md": "for a set of commonly used short sentences across these two periods. We select sentences spanning both good and bad feedback, and which most frequently occurred in the corresponding written feedback in our data.\n\nLet u denote the utility a rater obtains after some transaction.\n\nAssumption 1. In response obtaining utility u, the rater leaves primary feedback.\n\nFigure 5 shows that the numerical feedback scores associated with identical sentences have increased considerably over time, and that this increase has affected both positive and negative sentences. This pattern is consistent with greater reputation inflation in the numerical feedback score.\n\nFurthermore, E[ | u]  0 for all u, and so the expected score conditional upon the latent utility is E[s | u]  σ(u):\n\nIn addition to this primary feedback, raters also leave alternative feedback on the same transaction.\n\nAssumption 2. The alternative feedback is such that after each transaction, a  α(u), where α′ (u) > 0 for all u.\n\nAssumption 1 describes a data-generating process that is arguably a precondition for a useful reputation system. If different subjective ratings could not be fully aggregated to some common scale, it is unclear how the system could convey information. This data-generating process seems particularly appropriate for market settings where ratings do not simply reflect variation in taste, but rather depend on some notion of.",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 238.23652320000002,
          "h": 13.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 5. Difference over Time in the Feedback Scores Associated with Identical Sentences",
        "md": "# Figure 5. Difference over Time in the Feedback Scores Associated with Identical Sentences",
        "bBox": {
          "x": 45,
          "y": 148.87199999999996,
          "w": 308.5276931099997,
          "h": 9.962600000000009
        }
      },
      {
        "type": "table",
        "rows": [
          ["Year", "Good Feedback", "Bad Feedback"],
          ["2008", "5.0", "2.5"],
          ["2015", "4.9", "2.0"],
          ["2008", "4.8", "1.5"],
          ["2015", "4.7", "1.0"],
          ["2008", "4.6", ""]
        ],
        "md": "|Year|Good Feedback|Bad Feedback|\n|---|---|---|\n|2008|5.0|2.5|\n|2015|4.9|2.0|\n|2008|4.8|1.5|\n|2015|4.7|1.0|\n|2008|4.6| |",
        "isPerfectTable": true,
        "csv": "\"Year\",\"Good Feedback\",\"Bad Feedback\"\n\"2008\",\"5.0\",\"2.5\"\n\"2015\",\"4.9\",\"2.0\"\n\"2008\",\"4.8\",\"1.5\"\n\"2015\",\"4.7\",\"1.0\"\n\"2008\",\"4.6\",\"\""
      },
      {
        "type": "text",
        "value": "Notes. This figure shows the average numerical feedback associated with identical sentences found in the text of employer-on-worker written feedback in 2008 and 2015. The sentences plotted are the four most common sentences associated with high feedback scores and the four most common sentences associated with low feedback scores. A 95% confidence interval is shown for each mean.",
        "md": "Notes. This figure shows the average numerical feedback associated with identical sentences found in the text of employer-on-worker written feedback in 2008 and 2015. The sentences plotted are the four most common sentences associated with high feedback scores and the four most common sentences associated with low feedback scores. A 95% confidence interval is shown for each mean.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.12582894,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 10,
    "text": "Filippas, Horton, and Golden: Reputation Inflation\nMarketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS                                                                                                                                                                                                                                                                                                                                                                                                    9\n\nsatisfaction, given what was paid for the good or                                                                                                                                                                            It is straightforward to show that if the alternative\nservice. Assumption 2 simply states that the alter-                                                                                                                                                                          feedback                                            measure                                         also              inflates,                                     then                  our                  method\nnative measure is also a good measure—that when                                                                                                                                                                              yields a lower bound estimate. Our decomposition is\nthe latent utility is higher, it does show up as a                                                                                                                                                                           conceptually similar to estimating monetary inflation\nhigher score for that measure. Importantly, note                                                                                                                                                                             (Sidrauski 1967, Friedman 1977, Mishkin 2000, Berent-\nthat                          the                      monotonicity                                                          assumption                                      ensures                              the        sen et al. 2011), with the assumption that a basket-of-\ninverse function, α−1(·), exists.                                                                                                                                                                                            goods offers the same utility regardless of when it is\n           Suppose that for a set of transactions we observe a                                                                                                                                                               consumed (Diewert 1998). The difference in our setting\nprimary feedback set, S, and an alternative feedback                                                                                                                                                                         is that we can account for changes in the quality of the\nset,                 A, but we do not observe the latent utilities,                                                                                                                                                   U.     goods by using the alternative measure—something\nThese are all data about the same transactions, with                                                                                                                                                                         that is typically not possible in the monetary inflation\neach transaction characterized by a tuple (s, a, u).                                                                                                                                                                         case. As an aside, quality differences are a large concep-\nAssumption                                                             3.               We                     can           approximate                               the            conditional                            tual issue for measuring monetary inflation.5\nexpectation function with a learned function ˆs(a) such that                                                                                                                                                                 4.1.1. Example Decomposition.                                                                                                                           Consider a platform\n                                                                                             sˆ(a)  E[s | a] + η,                                                                                                   (3)      where                                   workers                                        with                   types                              θ ∈ {H, M, L}                                              produce\nwhere E[η | a]  0 for all a.                                                                                                                                                                                                 goods with utilities                                                                              uH,        uM, and uL, respectively, with\n                                                                                                                                                                                                                             u H > u M > u L.                                                          Employers                                         match                                with                   workers                              and\n           Assumption 3 implies that for all u,                                                                                                                                                                              receive goods and their corresponding utility. Employ-\n                                                                                                          E[η | u]  0,                                                                                              (4)      ers                    then                           leave                          primary                                feedback                                         σ(u),                 such                      that\nor that there is not systematic error at any given util-                                                                                                                                                                     σ(u H)  1, σ(u M)  0:5, and                                                                                                     σ(u L)  0, and alternative\nity.4                    Assumption 3 is certainly untestable. However,                                                                                                                                                      feedback that is written text. Suppose the text is such\napproximating the CEF without systematic error is                                                                                                                                                                            that the employer always says                                                                                                                      “good”                           when                       u                uH,\nprecisely                                           what                             flexible,                            modern                            machine                              learning                    “OK”                             when                            u                   uM, and                             “bad”                             when                     u                 uL. Using\nmethods are trying to accomplish. And given that the                                                                                                                                                                         transaction data, we can approximate the CEF via the\nsupports for both s an a are typically unchanged from                                                                                                                                                                        learned function,\none period to the next and the amount of ratings data                                                                                                                                                                                                                                                                      { 1                     text  “good”,\ncan be vast in online marketplaces, this kind of predic-                                                                                                                                                                                                                                        ˆs(a)                              0:5             text  “OK”,                                                                                                 (6)\ntion exercise is likely to go well in practice.                                                                                                                                                                                                                                                                                    0:0             text  “bad”:\n           Now suppose that at some later time, we observe a                                                                                                                                                                 We presented α(·) as being a single measure in Section\nnew set of primary and alternative feedback,                                                                                                                                                             S′ and              4.1, but it could be constructed as index from numer-\nA′, with unobserved utilities, U.′                                                                                                                                                                                           ous inputs, such as whether some text contained a\nProposition 1.                                                                 The expected value of the CEF applied to                                                                                                      specific term. Notice that we do not have to observe\nthe new data is unbiased estimate of the expected score,                                                                                                                                                                     the underlying utilities to learn the functionˆ(a). s\nregardless of the change in underlying utilities.                                                                                                                                                                                       At                  some                             later                     point                   in               time                          t′ > t,                  assume                           that\nProof.                              The expected primary feedback score with the                                                                                                                                             improvements                                                                     in             marketplace                                                   fundamentals                                             have\nnew data is E[s | U′ ]  E[σ(u) | U], by Equation (1). If′                                                                                     ′                                                                              resulted in employers obtaining only utilities                                                                                                                                                                  uM           and\nwe apply the learned ˆs(·) on the new alternative feed-                                                                                                                                                                      uH                 with equal probabilities,                                                                                                    say because                                             of better\nback, the expected value is                                                                                                                                                                                                  matching or a compositional change in sellers. This\n E[ˆs(a′ )|A]′                                                                                                                                                                                                               could be a result of any of the factors we discussed\n                                              ′ ))|U] (by Assumption 2)′                                                                                                                                                     above.                                     For                         example,                                   the                         platform                                     could                       have\n E[\n E[σ(α′ )|U] (byEquation(4))ˆs(α(u1(α(u)))+η|U] (by Assumption 3)−  ′                                           ′                                                                                                            improved its matching systems, enabling employers\n                                                                                                                                                                                                                             to never match with workers of type                                                                                                                                                 L; workers of\n E[σ(u′ ] (by Equation(2) and iterated expectations): ′                                                                                                                                                                   w  type L could now be more experienced or exert higher\n E[s|U                                                                                                                                                                                                                       effort, and hence produce goods with utilities uM and\nProposition 2.                                                               The expected inflation in the primary feed-                                                                                                     uH               with equal probabilities; or all workers of type                                                                                                                                                                      L\nback score is the difference in the average rating and the                                                                                                                                                                   could have exited the platform. We can observe nei-\nexpected rating using the learned CEF.                                                                                                                                                                                       ther the reasons behind the shift in platform funda-\n                                                                                                                                                                                                                             mentals nor the new distribution of employer utilities at\nProof.                                  If there is inflation                                                                             in   the primary                                             metric,               time t′. However, we observe the primary and alternative\nwhere                                    s  σ(u) + τ(u) + ,                                                                      where                      τ(u) > 0,                            we              can         feedback scores left by employers. Insofar as no employ-\nobtain the average inflation by                                                                                                                                                                                              ers have shifted their rating standards, employers leave\n                                                         E[s | U′ ] − E[s(a′ )]  E[τ(u) | U]: ˆ                                                                            ′                                        (5)      average primary feedback equal to 0.75, and equal frac-\n                                                                                                                                                                                                                          w  tions of “good” and “OK” alternative feedback.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\nsatisfaction, given what was paid for the good or service. Assumption 2 simply states that the alternative measure is also a good measure—that when the latent utility is higher, it does show up as a higher score for that measure. Importantly, note that the monotonicity assumption ensures the inverse function, α−1(·), exists.\n\nSuppose that for a set of transactions we observe a primary feedback set, S, and an alternative feedback set, A, but we do not observe the latent utilities, U. These are all data about the same transactions, with each transaction characterized by a tuple (s, a, u).\n\n# Assumption 3.\n\nWe can approximate the conditional expectation function with a learned function ˆs(a) such that\n\nsˆ(a) = E[s | a] + η,\n\nwhere E[η | a] = 0 for all a.\n\nAssumption 3 implies that for all u, E[η | u] = 0, or that there is not systematic error at any given utility. Assumption 3 is certainly untestable. However, approximating the CEF without systematic error is precisely what flexible, modern machine learning methods are trying to accomplish. And given that the supports for both s and a are typically unchanged from one period to the next and the amount of ratings data can be vast in online marketplaces, this kind of prediction exercise is likely to go well in practice.\n\nNow suppose that at some later time, we observe a new set of primary and alternative feedback, S′ and A′, with unobserved utilities, U′.\n\n# Proposition 1.\n\nThe expected value of the CEF applied to the new data is unbiased estimate of the expected score, regardless of the change in underlying utilities.\n\nProof. The expected primary feedback score with the new data is E[s | U′] = E[σ(u) | U], by Equation (1). If we apply the learned ˆs(·) on the new alternative feedback, the expected value is\n\nE[ˆs(a′)|A] = E[σ(α′)|U] (by Assumption 2)\n\nE[σ(α(u1(α(u))) + η|U] (by Assumption 3)\n\nE[σ(u′] (by Equation (2) and iterated expectations):\n\nE[s|U\n\n# Proposition 2.\n\nThe expected inflation in the primary feedback score is the difference in the average rating and the expected rating using the learned CEF.\n\nProof. If there is inflation in the primary metric, where s = σ(u) + τ(u) + ε, where τ(u) > 0, we can obtain the average inflation by\n\nE[s | U′] − E[s(a′)] = E[τ(u) | U]:\n\naverage primary feedback equal to 0.75, and equal fractions of “good” and “OK” alternative feedback.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 9.9626
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "satisfaction, given what was paid for the good or service. Assumption 2 simply states that the alternative measure is also a good measure—that when the latent utility is higher, it does show up as a higher score for that measure. Importantly, note that the monotonicity assumption ensures the inverse function, α−1(·), exists.\n\nSuppose that for a set of transactions we observe a primary feedback set, S, and an alternative feedback set, A, but we do not observe the latent utilities, U. These are all data about the same transactions, with each transaction characterized by a tuple (s, a, u).",
        "md": "satisfaction, given what was paid for the good or service. Assumption 2 simply states that the alternative measure is also a good measure—that when the latent utility is higher, it does show up as a higher score for that measure. Importantly, note that the monotonicity assumption ensures the inverse function, α−1(·), exists.\n\nSuppose that for a set of transactions we observe a primary feedback set, S, and an alternative feedback set, A, but we do not observe the latent utilities, U. These are all data about the same transactions, with each transaction characterized by a tuple (s, a, u).",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 238.11379355999998,
          "h": 13.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Assumption 3.",
        "md": "# Assumption 3.",
        "bBox": {
          "x": 45,
          "y": 136.87199999999996,
          "w": 172.8349,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "We can approximate the conditional expectation function with a learned function ˆs(a) such that\n\nsˆ(a) = E[s | a] + η,\n\nwhere E[η | a] = 0 for all a.\n\nAssumption 3 implies that for all u, E[η | u] = 0, or that there is not systematic error at any given utility. Assumption 3 is certainly untestable. However, approximating the CEF without systematic error is precisely what flexible, modern machine learning methods are trying to accomplish. And given that the supports for both s and a are typically unchanged from one period to the next and the amount of ratings data can be vast in online marketplaces, this kind of prediction exercise is likely to go well in practice.\n\nNow suppose that at some later time, we observe a new set of primary and alternative feedback, S′ and A′, with unobserved utilities, U′.",
        "md": "We can approximate the conditional expectation function with a learned function ˆs(a) such that\n\nsˆ(a) = E[s | a] + η,\n\nwhere E[η | a] = 0 for all a.\n\nAssumption 3 implies that for all u, E[η | u] = 0, or that there is not systematic error at any given utility. Assumption 3 is certainly untestable. However, approximating the CEF without systematic error is precisely what flexible, modern machine learning methods are trying to accomplish. And given that the supports for both s and a are typically unchanged from one period to the next and the amount of ratings data can be vast in online marketplaces, this kind of prediction exercise is likely to go well in practice.\n\nNow suppose that at some later time, we observe a new set of primary and alternative feedback, S′ and A′, with unobserved utilities, U′.",
        "bBox": {
          "x": 45,
          "y": 88.87199999999996,
          "w": 493.97645104,
          "h": 13.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Proposition 1.",
        "md": "# Proposition 1.",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 584.674
        }
      },
      {
        "type": "text",
        "value": "The expected value of the CEF applied to the new data is unbiased estimate of the expected score, regardless of the change in underlying utilities.\n\nProof. The expected primary feedback score with the new data is E[s | U′] = E[σ(u) | U], by Equation (1). If we apply the learned ˆs(·) on the new alternative feedback, the expected value is\n\nE[ˆs(a′)|A] = E[σ(α′)|U] (by Assumption 2)\n\nE[σ(α(u1(α(u))) + η|U] (by Assumption 3)\n\nE[σ(u′] (by Equation (2) and iterated expectations):\n\nE[s|U",
        "md": "The expected value of the CEF applied to the new data is unbiased estimate of the expected score, regardless of the change in underlying utilities.\n\nProof. The expected primary feedback score with the new data is E[s | U′] = E[σ(u) | U], by Equation (1). If we apply the learned ˆs(·) on the new alternative feedback, the expected value is\n\nE[ˆs(a′)|A] = E[σ(α′)|U] (by Assumption 2)\n\nE[σ(α(u1(α(u))) + η|U] (by Assumption 3)\n\nE[σ(u′] (by Equation (2) and iterated expectations):\n\nE[s|U",
        "bBox": {
          "x": 45,
          "y": 88.87199999999996,
          "w": 295.6844,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Proposition 2.",
        "md": "# Proposition 2.",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 584.674
        }
      },
      {
        "type": "text",
        "value": "The expected inflation in the primary feedback score is the difference in the average rating and the expected rating using the learned CEF.\n\nProof. If there is inflation in the primary metric, where s = σ(u) + τ(u) + ε, where τ(u) > 0, we can obtain the average inflation by\n\nE[s | U′] − E[s(a′)] = E[τ(u) | U]:\n\naverage primary feedback equal to 0.75, and equal fractions of “good” and “OK” alternative feedback.",
        "md": "The expected inflation in the primary feedback score is the difference in the average rating and the expected rating using the learned CEF.\n\nProof. If there is inflation in the primary metric, where s = σ(u) + τ(u) + ε, where τ(u) > 0, we can obtain the average inflation by\n\nE[s | U′] − E[s(a′)] = E[τ(u) | U]:\n\naverage primary feedback equal to 0.75, and equal fractions of “good” and “OK” alternative feedback.",
        "bBox": {
          "x": 45,
          "y": 88.87199999999996,
          "w": 295.6844,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 11,
    "text": "10\n   The crucial observation for our approach is that we\ncan use the alternative-to-primary mapping learned\nfrom the period       t data to estimate what the primary\nfeedback average should have been in period                    t′. In\nparticular, because employers leave alternative feed-\nback scores “good” and “OK” with equal frequency at\ntime   t′, we can estimate that the primary feedback\nscore should have been 0.75. If, instead, the observed pri-\nmary feedback average at time t′          is 0.9, then the remain-\ning 0.15 increase cannot be explained by improvements in\nfundamentals. We then say that 0:15=0:4  37:5% of the\nobserved increase in the primary feedback between time t\nand t′ is attributable to reputation inflation. This allows us\nto disentangle changes in fundamentals from reputation\ninflation in our data.\n   It is important to note that if the alternative feed-\nback measure also inflates, this approach will yield a\nlower bound on the magnitude of reputation inflation.\nFor example, assume that when employers experience\nutility uM at time t′, they inflate their alternative feed-\nback, generating “good” and “OK” with equal proba-\nbilities. We would then estimate that the primary\nfeedback average should have been equal to 0.875,\nand hence conservatively estimate that 0:025=0:4\n6:25% of the observed increase in the primary feed-\nback is due to inflation.\n4.2. Using Written Feedback to Estimate the\n      Degree of Reputation Inflation\nUsing written feedback, we fit a predictive model, ˆs(·),\nthat predicts numerical feedback scores from the feed-\nback text. The predictive model is fitted on a narrow\ntime window, using employer written feedback as the\ntraining set and the associated numerical scores as\nthe set of labels. One advantage of the written feed-\nback is that, unlike private feedback, we have access\nto written feedback over the entire platform history.\nEach written feedback left by an employer after a\ntransaction is one instance in our data.\n   To learn the predictive model, we use a standard\nnatural language processing pipeline. For the prepro-\ncessing step, the text of each employer-on-worker\nreview is stripped of accents and special characters\nand is lowercased, and stop words are removed. A\nmatrix of token counts (up to three-grams) is created\nand    is  weighed      using    the   term    frequency-inverse\ndocument frequency (TFIDF) method. To                      find the\nbest-performing algorithm, we conduct an extensive\ngrid search, evaluating each configuration of hyper-\nparameters using a        fivefold cross validation in terms\nof average squared error. We then use the fitted model\nto estimate out-of-sample feedback scores of the writ-\nten feedback for the entire sample.\n   The average quarterly feedback scores over time,\nfor both the numerical public feedback and the feed-\nback predicted from the written feedback, are plotted\n                   Filippas, Horton, and Golden: Reputation Inflation\n           Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\nin Figure 6. As expected, the two scores match up dur-\ning the training period. Going forward, both scores\nincrease, but the predicted feedback score increases at\na much slower rate. On average, numerical feedback\nincreases from 3.96 stars in the beginning of 2006 to\n4.86 stars at the beginning of 2016. In contrast, the\naverage score predicted from the written feedback\nonly goes to 4.25 stars. The divergence between the\nwritten sentiment and the numerical feedback implies\nthat a substantial amount of the increase in numerical\nfeedback scores is due to lower rater standards. Our\napproach also allows us to quantify the degree of infla-\ntion: the point estimate is that 67.7% of the increase in\nfeedback scores is due to inflation.\n   One might be concerned that some kind of selection\nbias might be driving the divergence between public\nfeedback scores and written text. We explore this pos-\nsibility in the online appendix, Section A.2, finding no\nevidence of such bias.\n\n5. Discussion\nAlthough we provided strong evidence that our alter-\nnative     feedback      measures—private           feedback      and\nwritten feedback—inflated at a slower rate (if at all)\ncompared with public numerical feedback, we offered\nlittle explanation as to why this might be the case. A\nrelated     question      is  precisely     why      any    measure\ninflates. In this section, we offer some thoughts on\nboth questions, with an eye toward future work that\nmight be more definitive. We also discuss whether\nreputation inflation is likely to matter to the function-\ning of the reputation system.\n\n5.1. Why Might Different Measures of Feedback\n       Inflate at Different Rates?\nTo quantify the importance of reputation inflation, we\nused two different alternative measures of rater satis-\nfaction, private feedback and written feedback. We\nshow that numerical public feedback inflates at a\ngreater rate than these two measures, but a natural\nquestion is, why the difference? We should note that\nour evidence cannot rule out that these other measures\nare    also   inflating—written        feedback      can    certainly\nbecome inflated, with work that would have elicited a\n“good” now garnering a            “great.”6 And, of course, the\nprivate numerical feedback rating could also become\ninflated.    However,      it  is  important     to  reiterate    that\nour method does not require there be no inflation in\nour alternative measure if the goal is to provide a lower\nbound.\n   A parsimonious explanation for why private scores\nwere less prone to inflation is that these scores did not\nmatter to the rated worker outcomes. As such, nega-\ntive feedback by employers could not harm the rated\nworker. If employers wanted to avoid harm—either",
    "md": "# 10\n\nThe crucial observation for our approach is that we can use the alternative-to-primary mapping learned from the period t data to estimate what the primary feedback average should have been in period t′. In particular, because employers leave alternative feedback scores “good” and “OK” with equal frequency at time t′, we can estimate that the primary feedback score should have been 0.75. If, instead, the observed primary feedback average at time t′ is 0.9, then the remaining 0.15 increase cannot be explained by improvements in fundamentals. We then say that 0:15=0:4 37:5% of the observed increase in the primary feedback between time t and t′ is attributable to reputation inflation. This allows us to disentangle changes in fundamentals from reputation inflation in our data.\n\nIt is important to note that if the alternative feedback measure also inflates, this approach will yield a lower bound on the magnitude of reputation inflation. For example, assume that when employers experience utility uM at time t′, they inflate their alternative feedback, generating “good” and “OK” with equal probabilities. We would then estimate that the primary feedback average should have been equal to 0.875, and hence conservatively estimate that 0:025=0:4 6:25% of the observed increase in the primary feedback is due to inflation.\n\n# 4.2. Using Written Feedback to Estimate the Degree of Reputation Inflation\n\nUsing written feedback, we fit a predictive model, ˆs(·), that predicts numerical feedback scores from the feedback text. The predictive model is fitted on a narrow time window, using employer written feedback as the training set and the associated numerical scores as the set of labels. One advantage of the written feedback is that, unlike private feedback, we have access to written feedback over the entire platform history. Each written feedback left by an employer after a transaction is one instance in our data.\n\nTo learn the predictive model, we use a standard natural language processing pipeline. For the preprocessing step, the text of each employer-on-worker review is stripped of accents and special characters and is lowercased, and stop words are removed. A matrix of token counts (up to three-grams) is created and is weighed using the term frequency-inverse document frequency (TFIDF) method. To find the best-performing algorithm, we conduct an extensive grid search, evaluating each configuration of hyper-parameters using a fivefold cross validation in terms of average squared error. We then use the fitted model to estimate out-of-sample feedback scores of the written feedback for the entire sample.\n\nThe average quarterly feedback scores over time, for both the numerical public feedback and the feedback predicted from the written feedback, are plotted in Figure 6. As expected, the two scores match up during the training period. Going forward, both scores increase, but the predicted feedback score increases at a much slower rate. On average, numerical feedback increases from 3.96 stars in the beginning of 2006 to 4.86 stars at the beginning of 2016. In contrast, the average score predicted from the written feedback only goes to 4.25 stars. The divergence between the written sentiment and the numerical feedback implies that a substantial amount of the increase in numerical feedback scores is due to lower rater standards. Our approach also allows us to quantify the degree of inflation: the point estimate is that 67.7% of the increase in feedback scores is due to inflation.\n\nOne might be concerned that some kind of selection bias might be driving the divergence between public feedback scores and written text. We explore this possibility in the online appendix, Section A.2, finding no evidence of such bias.\n\n# 5. Discussion\n\nAlthough we provided strong evidence that our alternative feedback measures—private feedback and written feedback—inflated at a slower rate (if at all) compared with public numerical feedback, we offered little explanation as to why this might be the case. A related question is precisely why any measure inflates. In this section, we offer some thoughts on both questions, with an eye toward future work that might be more definitive. We also discuss whether reputation inflation is likely to matter to the functioning of the reputation system.\n\n# 5.1. Why Might Different Measures of Feedback Inflate at Different Rates?\n\nTo quantify the importance of reputation inflation, we used two different alternative measures of rater satisfaction, private feedback and written feedback. We show that numerical public feedback inflates at a greater rate than these two measures, but a natural question is, why the difference? We should note that our evidence cannot rule out that these other measures are also inflating—written feedback can certainly become inflated, with work that would have elicited a “good” now garnering a “great.” And, of course, the private numerical feedback rating could also become inflated. However, it is important to reiterate that our method does not require there be no inflation in our alternative measure if the goal is to provide a lower bound.\n\nA parsimonious explanation for why private scores were less prone to inflation is that these scores did not matter to the rated worker outcomes. As such, negative feedback by employers could not harm the rated worker. If employers wanted to avoid harm—either",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "10",
        "md": "# 10",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 9.962600000000002,
          "h": 9.962600000000002
        }
      },
      {
        "type": "text",
        "value": "The crucial observation for our approach is that we can use the alternative-to-primary mapping learned from the period t data to estimate what the primary feedback average should have been in period t′. In particular, because employers leave alternative feedback scores “good” and “OK” with equal frequency at time t′, we can estimate that the primary feedback score should have been 0.75. If, instead, the observed primary feedback average at time t′ is 0.9, then the remaining 0.15 increase cannot be explained by improvements in fundamentals. We then say that 0:15=0:4 37:5% of the observed increase in the primary feedback between time t and t′ is attributable to reputation inflation. This allows us to disentangle changes in fundamentals from reputation inflation in our data.\n\nIt is important to note that if the alternative feedback measure also inflates, this approach will yield a lower bound on the magnitude of reputation inflation. For example, assume that when employers experience utility uM at time t′, they inflate their alternative feedback, generating “good” and “OK” with equal probabilities. We would then estimate that the primary feedback average should have been equal to 0.875, and hence conservatively estimate that 0:025=0:4 6:25% of the observed increase in the primary feedback is due to inflation.",
        "md": "The crucial observation for our approach is that we can use the alternative-to-primary mapping learned from the period t data to estimate what the primary feedback average should have been in period t′. In particular, because employers leave alternative feedback scores “good” and “OK” with equal frequency at time t′, we can estimate that the primary feedback score should have been 0.75. If, instead, the observed primary feedback average at time t′ is 0.9, then the remaining 0.15 increase cannot be explained by improvements in fundamentals. We then say that 0:15=0:4 37:5% of the observed increase in the primary feedback between time t and t′ is attributable to reputation inflation. This allows us to disentangle changes in fundamentals from reputation inflation in our data.\n\nIt is important to note that if the alternative feedback measure also inflates, this approach will yield a lower bound on the magnitude of reputation inflation. For example, assume that when employers experience utility uM at time t′, they inflate their alternative feedback, generating “good” and “OK” with equal probabilities. We would then estimate that the primary feedback average should have been equal to 0.875, and hence conservatively estimate that 0:025=0:4 6:25% of the observed increase in the primary feedback is due to inflation.",
        "bBox": {
          "x": 45,
          "y": 76.87199999999996,
          "w": 238.3175458,
          "h": 13.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "4.2. Using Written Feedback to Estimate the Degree of Reputation Inflation",
        "md": "# 4.2. Using Written Feedback to Estimate the Degree of Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 345.87199999999996,
          "w": 330.8267,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "Using written feedback, we fit a predictive model, ˆs(·), that predicts numerical feedback scores from the feedback text. The predictive model is fitted on a narrow time window, using employer written feedback as the training set and the associated numerical scores as the set of labels. One advantage of the written feedback is that, unlike private feedback, we have access to written feedback over the entire platform history. Each written feedback left by an employer after a transaction is one instance in our data.\n\nTo learn the predictive model, we use a standard natural language processing pipeline. For the preprocessing step, the text of each employer-on-worker review is stripped of accents and special characters and is lowercased, and stop words are removed. A matrix of token counts (up to three-grams) is created and is weighed using the term frequency-inverse document frequency (TFIDF) method. To find the best-performing algorithm, we conduct an extensive grid search, evaluating each configuration of hyper-parameters using a fivefold cross validation in terms of average squared error. We then use the fitted model to estimate out-of-sample feedback scores of the written feedback for the entire sample.\n\nThe average quarterly feedback scores over time, for both the numerical public feedback and the feedback predicted from the written feedback, are plotted in Figure 6. As expected, the two scores match up during the training period. Going forward, both scores increase, but the predicted feedback score increases at a much slower rate. On average, numerical feedback increases from 3.96 stars in the beginning of 2006 to 4.86 stars at the beginning of 2016. In contrast, the average score predicted from the written feedback only goes to 4.25 stars. The divergence between the written sentiment and the numerical feedback implies that a substantial amount of the increase in numerical feedback scores is due to lower rater standards. Our approach also allows us to quantify the degree of inflation: the point estimate is that 67.7% of the increase in feedback scores is due to inflation.\n\nOne might be concerned that some kind of selection bias might be driving the divergence between public feedback scores and written text. We explore this possibility in the online appendix, Section A.2, finding no evidence of such bias.",
        "md": "Using written feedback, we fit a predictive model, ˆs(·), that predicts numerical feedback scores from the feedback text. The predictive model is fitted on a narrow time window, using employer written feedback as the training set and the associated numerical scores as the set of labels. One advantage of the written feedback is that, unlike private feedback, we have access to written feedback over the entire platform history. Each written feedback left by an employer after a transaction is one instance in our data.\n\nTo learn the predictive model, we use a standard natural language processing pipeline. For the preprocessing step, the text of each employer-on-worker review is stripped of accents and special characters and is lowercased, and stop words are removed. A matrix of token counts (up to three-grams) is created and is weighed using the term frequency-inverse document frequency (TFIDF) method. To find the best-performing algorithm, we conduct an extensive grid search, evaluating each configuration of hyper-parameters using a fivefold cross validation in terms of average squared error. We then use the fitted model to estimate out-of-sample feedback scores of the written feedback for the entire sample.\n\nThe average quarterly feedback scores over time, for both the numerical public feedback and the feedback predicted from the written feedback, are plotted in Figure 6. As expected, the two scores match up during the training period. Going forward, both scores increase, but the predicted feedback score increases at a much slower rate. On average, numerical feedback increases from 3.96 stars in the beginning of 2006 to 4.86 stars at the beginning of 2016. In contrast, the average score predicted from the written feedback only goes to 4.25 stars. The divergence between the written sentiment and the numerical feedback implies that a substantial amount of the increase in numerical feedback scores is due to lower rater standards. Our approach also allows us to quantify the degree of inflation: the point estimate is that 67.7% of the increase in feedback scores is due to inflation.\n\nOne might be concerned that some kind of selection bias might be driving the divergence between public feedback scores and written text. We explore this possibility in the online appendix, Section A.2, finding no evidence of such bias.",
        "bBox": {
          "x": 45,
          "y": 88.87199999999996,
          "w": 493.6030287,
          "h": 10.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "5. Discussion",
        "md": "# 5. Discussion",
        "bBox": {
          "x": 301,
          "y": 320.87199999999996,
          "w": 78.05843443000003,
          "h": 11.955100000000016
        }
      },
      {
        "type": "text",
        "value": "Although we provided strong evidence that our alternative feedback measures—private feedback and written feedback—inflated at a slower rate (if at all) compared with public numerical feedback, we offered little explanation as to why this might be the case. A related question is precisely why any measure inflates. In this section, we offer some thoughts on both questions, with an eye toward future work that might be more definitive. We also discuss whether reputation inflation is likely to matter to the functioning of the reputation system.",
        "md": "Although we provided strong evidence that our alternative feedback measures—private feedback and written feedback—inflated at a slower rate (if at all) compared with public numerical feedback, we offered little explanation as to why this might be the case. A related question is precisely why any measure inflates. In this section, we offer some thoughts on both questions, with an eye toward future work that might be more definitive. We also discuss whether reputation inflation is likely to matter to the functioning of the reputation system.",
        "bBox": {
          "x": 45,
          "y": 100.87199999999996,
          "w": 313.9261263799999,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "5.1. Why Might Different Measures of Feedback Inflate at Different Rates?",
        "md": "# 5.1. Why Might Different Measures of Feedback Inflate at Different Rates?",
        "bBox": {
          "x": 121,
          "y": 100.87199999999996,
          "w": 226.40805508000017,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "To quantify the importance of reputation inflation, we used two different alternative measures of rater satisfaction, private feedback and written feedback. We show that numerical public feedback inflates at a greater rate than these two measures, but a natural question is, why the difference? We should note that our evidence cannot rule out that these other measures are also inflating—written feedback can certainly become inflated, with work that would have elicited a “good” now garnering a “great.” And, of course, the private numerical feedback rating could also become inflated. However, it is important to reiterate that our method does not require there be no inflation in our alternative measure if the goal is to provide a lower bound.\n\nA parsimonious explanation for why private scores were less prone to inflation is that these scores did not matter to the rated worker outcomes. As such, negative feedback by employers could not harm the rated worker. If employers wanted to avoid harm—either",
        "md": "To quantify the importance of reputation inflation, we used two different alternative measures of rater satisfaction, private feedback and written feedback. We show that numerical public feedback inflates at a greater rate than these two measures, but a natural question is, why the difference? We should note that our evidence cannot rule out that these other measures are also inflating—written feedback can certainly become inflated, with work that would have elicited a “good” now garnering a “great.” And, of course, the private numerical feedback rating could also become inflated. However, it is important to reiterate that our method does not require there be no inflation in our alternative measure if the goal is to provide a lower bound.\n\nA parsimonious explanation for why private scores were less prone to inflation is that these scores did not matter to the rated worker outcomes. As such, negative feedback by employers could not harm the rated worker. If employers wanted to avoid harm—either",
        "bBox": {
          "x": 45,
          "y": 100.87199999999996,
          "w": 313.54026432,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 12,
    "text": "             Filippas, Horton, and Golden: Reputation Inflation\n             Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS                                                                                 11\n\n             Figure 6.    (Color online) Numerical Public Feedback Score and Predicted Score from Textual Feedback\n\n                                                                                                    avg. public feedback                4.86\n                                      4.75   Training setFeedback score\n                                      4.50\n\n                                      4.25                                                                                              4.25\n                                                                                                    avg. predicted feedback\n                                      4.00\n\n                                                  200 7    200 8    200 9   201 0    201 1    201 2    201 3    201 4    201 5    2016\n                                                                                          Year\n             Notes. This figure shows the evolution of average public feedback scores (solid line) versus the average predicted score of textual feedback\n             (dashed line) assigned by employers to workers. A 95% interval is depicted for every point estimate. The shaded area indicates the quarter from\n             which training data were obtained for the predictive model.\n\nfor altruistic reasons of because they expect some neg-\native blowback—they were relatively freer to give\nnegative private feedback. Furthermore, because the\nrated worker did not know the score, they could not\ncomplain.\n   Written feedback was of course public, but it might\nbe    less  subject     to   less   inflationary     pressure      than\nnumerical feedback. First, it is harder for workers to\ncomplain about textual tone than it is to complain\nabout a nonperfect star rating. Second, the platform\ndoes not aggregate written feedback or put it on a\nscale, making it harder to use than average numerical\nfeedback       for  cross-worker        comparisons        by    future\nemployers;       these    comparisons         are   precisely     what\nmakes feedback consequential for workers. Third, the\nwritten feedback history is not presented in the work-\ner’s profile page in our focal marketplace, which is\ntypically accessed         by employers during the initial\nworker      screening      phase—only          average     numerical\nfeedback scores are presented, and written feedback is\nharder to access.\n\n5.2. Causes of Reputation Inflation\nAlthough we have strong evidence that reputation\ninflation exists, our data only hint at what the causes\nmight be. The evidence suggests that rater unwilling-\nness to give negative public feedback plays a role.\nNegative feedback is harmful to a rated party, and\nraters might want to avoid that harm—either because\nthey do not want to deal with retaliation (even just in\nthe form of a complaint) or because they simply do\nnot want to harm the rated party out of altruism.\nRecall that 28.4% of those employers who                      privately\nreported that they would definitely not hire the same\nworker in the future          publicly    assigned them four or\nmore stars out of          five. Because private feedback is\nanonymously given, workers cannot retaliate against\nemployers following a bad private feedback score. At\nthe same time, a bad public rating would be conse-\nquential in our setting, but a bad private rating would\nnot.\n   Although fear of retaliation or avoidance of harm—\nwhat we might think of as the cost of giving bad feed-\nback—could explain a bias toward higher ratings,\nhow does it explain the trends we observe? Although\nwe do not model the process formally, it is easy to see\nhow a kind of ratchet effect could happen in practice,\nwith the cost of bad feedback rising over time. Sup-\npose most raters want to rate truthfully; that is, they\nwant to match the percentile of their rating to the per-\ncentile of their subjective utility. If raters think their\nexperience gave them the median level of utility, they\nwould want to give the median feedback score; if they\nthink they got the 25th percentile in utility, they want\nto give the 25th percentile score, and so on, even if\nthis truthfulness can be harmful in the case of bad per-\nformance. But now suppose that some raters always\njust give the highest possible score and avoid the costs\nof being truthful. These always-five-stars raters will\nshift the distribution of feedback scores, requiring\neven truthful raters to rate higher. This, in turn, will\nmake any previous score fall in the distribution (e.g.,\nfour stars used to be the 80th percentile, and now it is\nthe 25th), effectively raising the cost of giving that\nscore.\n\n5.3. Does Reputation Inflation Matter?\nReputation systems exist to affect decisions in the\nmarketplace and, indirectly, to create effective incen-\ntives. A natural question is whether reputation inflation\nactually affects these system goals. It is not obvious\nthat it would—for example, a certain amount of mone-\ntary inflation is desirable and creates no large loss\nin   information      so   long    as  parties    know      to  adjust.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\n# Figure 6. (Color online) Numerical Public Feedback Score and Predicted Score from Textual Feedback\n\n|Year| |avg. public feedback|Feedback score|avg. predicted feedback| |\n|---|---|---|---|---|---|\n|2007| | |4.86|4.75|4.25|\n|2008| | | | | |\n|2009|4.50| | | | |\n|2010|4.25| | | | |\n|2011|4.00| | | | |\n|2012| | | | | |\n|2013| | | | | |\n|2014| | | | | |\n|2015| | | | | |\n| | | |2016| | |\n\nNotes. This figure shows the evolution of average public feedback scores (solid line) versus the average predicted score of textual feedback (dashed line) assigned by employers to workers. A 95% interval is depicted for every point estimate. The shaded area indicates the quarter from which training data were obtained for the predictive model.\n\n# 5.2. Causes of Reputation Inflation\n\nAlthough we have strong evidence that reputation inflation exists, our data only hint at what the causes might be. The evidence suggests that rater unwillingness to give negative public feedback plays a role. Negative feedback is harmful to a rated party, and raters might want to avoid that harm—either because they do not want to deal with retaliation (even just in the form of a complaint) or because they simply do not want to harm the rated party out of altruism. Recall that 28.4% of those employers who privately reported that they would definitely not hire the same worker in the future publicly assigned them four or more stars out of five. Because private feedback is anonymously given, workers cannot retaliate against employers following a bad private feedback score. At the same time, a bad public rating would be consequential in our setting, but a bad private rating would not.\n\nAlthough fear of retaliation or avoidance of harm—what we might think of as the cost of giving bad feedback—could explain a bias toward higher ratings, how does it explain the trends we observe? Although we do not model the process formally, it is easy to see how a kind of ratchet effect could happen in practice, with the cost of bad feedback rising over time. Suppose most raters want to rate truthfully; that is, they want to match the percentile of their rating to the percentile of their subjective utility. If raters think their experience gave them the median level of utility, they would want to give the median feedback score; if they think they got the 25th percentile in utility, they want to give the 25th percentile score, and so on, even if this truthfulness can be harmful in the case of bad performance. But now suppose that some raters always just give the highest possible score and avoid the costs of being truthful. These always-five-stars raters will shift the distribution of feedback scores, requiring even truthful raters to rate higher. This, in turn, will make any previous score fall in the distribution (e.g., four stars used to be the 80th percentile, and now it is the 25th), effectively raising the cost of giving that score.\n\n# 5.3. Does Reputation Inflation Matter?\n\nReputation systems exist to affect decisions in the marketplace and, indirectly, to create effective incentives. A natural question is whether reputation inflation actually affects these system goals. It is not obvious that it would—for example, a certain amount of monetary inflation is desirable and creates no large loss in information so long as parties know to adjust.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 7.471800000000002
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 9.962599999999952
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Figure 6. (Color online) Numerical Public Feedback Score and Predicted Score from Textual Feedback",
        "md": "# Figure 6. (Color online) Numerical Public Feedback Score and Predicted Score from Textual Feedback",
        "bBox": {
          "x": 45,
          "y": 75.87199999999996,
          "w": 353.0238534899995,
          "h": 10.2274
        }
      },
      {
        "type": "table",
        "rows": [
          [
            "Year",
            "",
            "avg. public feedback",
            "Feedback score",
            "avg. predicted feedback",
            ""
          ],
          ["2007", "", "", "4.86", "4.75", "4.25"],
          ["2008", "", "", "", "", ""],
          ["2009", "4.50", "", "", "", ""],
          ["2010", "4.25", "", "", "", ""],
          ["2011", "4.00", "", "", "", ""],
          ["2012", "", "", "", "", ""],
          ["2013", "", "", "", "", ""],
          ["2014", "", "", "", "", ""],
          ["2015", "", "", "", "", ""],
          ["", "", "", "2016", "", ""]
        ],
        "md": "|Year| |avg. public feedback|Feedback score|avg. predicted feedback| |\n|---|---|---|---|---|---|\n|2007| | |4.86|4.75|4.25|\n|2008| | | | | |\n|2009|4.50| | | | |\n|2010|4.25| | | | |\n|2011|4.00| | | | |\n|2012| | | | | |\n|2013| | | | | |\n|2014| | | | | |\n|2015| | | | | |\n| | | |2016| | |",
        "isPerfectTable": true,
        "csv": "\"Year\",\"\",\"avg. public feedback\",\"Feedback score\",\"avg. predicted feedback\",\"\"\n\"2007\",\"\",\"\",\"4.86\",\"4.75\",\"4.25\"\n\"2008\",\"\",\"\",\"\",\"\",\"\"\n\"2009\",\"4.50\",\"\",\"\",\"\",\"\"\n\"2010\",\"4.25\",\"\",\"\",\"\",\"\"\n\"2011\",\"4.00\",\"\",\"\",\"\",\"\"\n\"2012\",\"\",\"\",\"\",\"\",\"\"\n\"2013\",\"\",\"\",\"\",\"\",\"\"\n\"2014\",\"\",\"\",\"\",\"\",\"\"\n\"2015\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"2016\",\"\",\"\""
      },
      {
        "type": "text",
        "value": "Notes. This figure shows the evolution of average public feedback scores (solid line) versus the average predicted score of textual feedback (dashed line) assigned by employers to workers. A 95% interval is depicted for every point estimate. The shaded area indicates the quarter from which training data were obtained for the predictive model.",
        "md": "Notes. This figure shows the evolution of average public feedback scores (solid line) versus the average predicted score of textual feedback (dashed line) assigned by employers to workers. A 95% interval is depicted for every point estimate. The shaded area indicates the quarter from which training data were obtained for the predictive model.",
        "bBox": {
          "x": null,
          "y": 120,
          "w": 493.6914530399997,
          "h": 10.2274
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "5.2. Causes of Reputation Inflation",
        "md": "# 5.2. Causes of Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 564.872,
          "w": 166.18911938000005,
          "h": 9.962599999999952
        }
      },
      {
        "type": "text",
        "value": "Although we have strong evidence that reputation inflation exists, our data only hint at what the causes might be. The evidence suggests that rater unwillingness to give negative public feedback plays a role. Negative feedback is harmful to a rated party, and raters might want to avoid that harm—either because they do not want to deal with retaliation (even just in the form of a complaint) or because they simply do not want to harm the rated party out of altruism. Recall that 28.4% of those employers who privately reported that they would definitely not hire the same worker in the future publicly assigned them four or more stars out of five. Because private feedback is anonymously given, workers cannot retaliate against employers following a bad private feedback score. At the same time, a bad public rating would be consequential in our setting, but a bad private rating would not.\n\nAlthough fear of retaliation or avoidance of harm—what we might think of as the cost of giving bad feedback—could explain a bias toward higher ratings, how does it explain the trends we observe? Although we do not model the process formally, it is easy to see how a kind of ratchet effect could happen in practice, with the cost of bad feedback rising over time. Suppose most raters want to rate truthfully; that is, they want to match the percentile of their rating to the percentile of their subjective utility. If raters think their experience gave them the median level of utility, they would want to give the median feedback score; if they think they got the 25th percentile in utility, they want to give the 25th percentile score, and so on, even if this truthfulness can be harmful in the case of bad performance. But now suppose that some raters always just give the highest possible score and avoid the costs of being truthful. These always-five-stars raters will shift the distribution of feedback scores, requiring even truthful raters to rate higher. This, in turn, will make any previous score fall in the distribution (e.g., four stars used to be the 80th percentile, and now it is the 25th), effectively raising the cost of giving that score.",
        "md": "Although we have strong evidence that reputation inflation exists, our data only hint at what the causes might be. The evidence suggests that rater unwillingness to give negative public feedback plays a role. Negative feedback is harmful to a rated party, and raters might want to avoid that harm—either because they do not want to deal with retaliation (even just in the form of a complaint) or because they simply do not want to harm the rated party out of altruism. Recall that 28.4% of those employers who privately reported that they would definitely not hire the same worker in the future publicly assigned them four or more stars out of five. Because private feedback is anonymously given, workers cannot retaliate against employers following a bad private feedback score. At the same time, a bad public rating would be consequential in our setting, but a bad private rating would not.\n\nAlthough fear of retaliation or avoidance of harm—what we might think of as the cost of giving bad feedback—could explain a bias toward higher ratings, how does it explain the trends we observe? Although we do not model the process formally, it is easy to see how a kind of ratchet effect could happen in practice, with the cost of bad feedback rising over time. Suppose most raters want to rate truthfully; that is, they want to match the percentile of their rating to the percentile of their subjective utility. If raters think their experience gave them the median level of utility, they would want to give the median feedback score; if they think they got the 25th percentile in utility, they want to give the 25th percentile score, and so on, even if this truthfulness can be harmful in the case of bad performance. But now suppose that some raters always just give the highest possible score and avoid the costs of being truthful. These always-five-stars raters will shift the distribution of feedback scores, requiring even truthful raters to rate higher. This, in turn, will make any previous score fall in the distribution (e.g., four stars used to be the 80th percentile, and now it is the 25th), effectively raising the cost of giving that score.",
        "bBox": {
          "x": null,
          "y": 120,
          "w": 238.45710978000005,
          "h": 10.2274
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "5.3. Does Reputation Inflation Matter?",
        "md": "# 5.3. Does Reputation Inflation Matter?",
        "bBox": {
          "x": 301,
          "y": 649.872,
          "w": 181.67398856000005,
          "h": 9.962599999999952
        }
      },
      {
        "type": "text",
        "value": "Reputation systems exist to affect decisions in the marketplace and, indirectly, to create effective incentives. A natural question is whether reputation inflation actually affects these system goals. It is not obvious that it would—for example, a certain amount of monetary inflation is desirable and creates no large loss in information so long as parties know to adjust.",
        "md": "Reputation systems exist to affect decisions in the marketplace and, indirectly, to create effective incentives. A natural question is whether reputation inflation actually affects these system goals. It is not obvious that it would—for example, a certain amount of monetary inflation is desirable and creates no large loss in information so long as parties know to adjust.",
        "bBox": {
          "x": 92,
          "y": 367.87199999999996,
          "w": 237.72843060000002,
          "h": 9.962600000000009
        }
      }
    ]
  },
  {
    "page": 13,
    "text": "                                                                                                                                   Filippas, Horton, and Golden: Reputation Inflation\n12                                                                                                                   Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\nHowever, this is a misleading analogy. Unlike in mone-                                             could emphasize reviewers as performing a service\ntary systems where there is no highest price, ratings                                              for fellow consumers, or provide other incentives for\nsystems are always on a top-censored scale: for the                                                honest reviews; Yelp employs mechanisms such as\nquestion “rate on a scale from one to X,” the value of X                                           badges for top reviewers, and makes the feedback\nmust be prespecified. This is why reputation inflation                                             score distribution of each reviewer publicly accessible.\ndiffers from monetary inflation: a sandwich that used                                              Mandatory grading curves are often employed in non-\n                                                                                                   digital reputation systems.8\nto cost $0.50 and may now cost $12. However, this nec-\nessary increase could not happen if price was mechani-                                                  Whether reputation systems less prone to inflation\ncally restricted to be below $1. As such, reputation                                               can be designed remains an open research question\ninflation leads to pooling of feedback scores in the                                               (Garg and Johari 2020). One of their problems seems\nhighest feedback bin. This pooling makes it difficult to                                           to be the manifestation of Campbell’s (1979, p. 83)\ndistinguish “excellent”                            from simply “good,” and with                    law, which may be challenging to fully transcend:\nsufficient inflation, the reputation system could become                                           “the more any quantitative social indicator is used for\nnearly binary, with the only possible signals being                                                social decision-making, the more subject it will be to\n“terrible” and “not terrible.” In our context, 85% of the                                          corruption pressures and the more apt it will be to dis-\nusers receive a perfect rating at the end of our sample                                            tort and corrupt the social processes it is intended to\n(see Figure 1(c)), even though it is highly unlikely that                                          monitor.”\n85% of transactions result in the exact same employer\nsatisfaction.                                                                                      Acknowledgments\n     Even if market participants and the platform are                                              The authors thank David Holtz, Ramesh Johari, Nico Lace-\naware of the inflation, pooling is difficult or even                                               tra, Xiao Ma, Foster Provost, Aaron Sojourner, and Richard\nimpossible to correct statistically. With pooling, the                                             Zeckhauser for very helpful comments and suggestions.\nstrictly monotone relationship between rater satisfac-                                             Endnotes\ntion and scores is lost and presents the same problem                                              1 We use the terms “employer” and “worker” for consistency with\nas grade inflation (Babcock 2010, Butcher et al. 2014).                                            the literature, and not as a comment on the legal relationship of the\nFurthermore, a strong rate of inflation—even if episo-                                             transacting parties.\ndic and then contained—can cause individual reputa-                                                2 We use the present tense here to describe the reputation system\ntions to vary based on when a feedback score was                                                   before the introduction of private feedback. Although our focus is\nassigned, in turn undermining the usefulness of com-                                               on employer-on-worker feedback, our claims carry through to the\nparisons of feedback scores across different time peri-                                            equally important case of worker-on-employer feedback (Benson\nods. Aside from the effects on market participants,                                                et al. 2020).\nreputation inflation causes the platform itself to lose a                                          3 We use this $10 restriction throughout the paper to remove mis-\n                                                                                                   taken, trial, and erroneous transactions.\nyardstick for measuring its own performance.                                                       4 If this was not the case, then there would be some u′ such that\n                                                                                                   E[η | u′ ]  k and k ≠ 0. But E[η | u]  E[η | α−1(u)]  k, which contra-′          ′\n6. Conclusion                                                                                      dicts Assumption 3.\n                                                                                                   5 Other approaches—which we do not take in this paper—would\nThis paper documents that the reputation system in\nan online marketplace was subject to inflation—we                                                  be to reduce bias consumer satisfaction estimates directly (Huang\nobserve systematically higher scores over time, which                                              and Sudhir 2019) or to estimate structural models of the value of\ncannot be fully explained by improvements in funda-                                                reputation across different time periods (Yoganarasimhan 2013).\nmentals. Data from four other marketplaces exhibit                                                 6 One written feedback in our data reads, “This is the most impres-\n                                                                                                   sive piece of coding in the history of software development!”\nthe same trend, suggesting that reputation inflation is                                            7 See also https://www.youtube.com/watch?v=KOO5S4vxi0o.\na widespread problem.                                                                              8 For example, officer evaluation reports in the U.S. Army limit\n     For        would-be               marketplace                   designers,  our        paper  senior raters to indicating only 50% or less of the officers they rate\nillustrates a core market design problem. The diver-                                               as “most qualified.” However, it may be challenging to force a dis-\ngence of public and private feedback scores in our                                                 tribution in settings where buyers evaluate sellers as a “flow.”\ndata suggests that a possible mechanism driving repu-\ntation inflation is that raters incur a greater personal                                           References\ncost—or guilt—the greater the harm they impose on                                                  Aperjis C, Johari R (2010) Optimal windows for aggregating ratings\nthe rated worker. An interesting next step would be to                                                     in electronic marketplaces. Management Sci. 56(5):864–880.\nelucidate the root causes of reputation inflation.                                                 Athey S, Castillo JC, Chandar B (2019) Service quality in the gig\n     Whether there are effective market design respo-                                                      economy: Empirical evidence about driving quality at Uber.\nnses to reputation inflation is an open question. Ch-                                                      Preprint, submitted December 28, http://dx.doi.org/10.2139/\nanges in the reputation system, such as adding a                                                           ssrn.3499781.\n                                                                                                   Babcock P (2010) Real costs of nominal grade inflation? New evi-\nhigher ceiling in the feedback scores, may temporarily\nmitigate—but do not solve—the problem.7                                                                    dence from student course evaluations.                                  Econom. Inquiry  48(4):\n                                                                                   Platforms               983–996.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\nMarketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\nHowever, this is a misleading analogy. Unlike in monetary systems where there is no highest price, ratings systems are always on a top-censored scale: for the question “rate on a scale from one to X,” the value of X must be prespecified. This is why reputation inflation differs from monetary inflation: a sandwich that used to cost $0.50 and may now cost $12. However, this necessary increase could not happen if price was mechanically restricted to be below $1. As such, reputation inflation leads to pooling of feedback scores in the highest feedback bin. This pooling makes it difficult to distinguish “excellent” from simply “good,” and with sufficient inflation, the reputation system could become nearly binary, with the only possible signals being “terrible” and “not terrible.” In our context, 85% of the users receive a perfect rating at the end of our sample (see Figure 1(c)), even though it is highly unlikely that 85% of transactions result in the exact same employer satisfaction.\n\nEven if market participants and the platform are aware of the inflation, pooling is difficult or even impossible to correct statistically. With pooling, the strictly monotone relationship between rater satisfaction and scores is lost and presents the same problem as grade inflation (Babcock 2010, Butcher et al. 2014). Furthermore, a strong rate of inflation—even if episodic and then contained—can cause individual reputations to vary based on when a feedback score was assigned, in turn undermining the usefulness of comparisons of feedback scores across different time periods. Aside from the effects on market participants, reputation inflation causes the platform itself to lose a yardstick for measuring its own performance.\n\n# 6. Conclusion\n\nThis paper documents that the reputation system in an online marketplace was subject to inflation—we observe systematically higher scores over time, which cannot be fully explained by improvements in fundamentals. Data from four other marketplaces exhibit the same trend, suggesting that reputation inflation is a widespread problem.\n\nFor would-be marketplace designers, our paper illustrates a core market design problem. The divergence of public and private feedback scores in our data suggests that a possible mechanism driving reputation inflation is that raters incur a greater personal cost—or guilt—the greater the harm they impose on the rated worker. An interesting next step would be to elucidate the root causes of reputation inflation.\n\nWhether there are effective market design responses to reputation inflation is an open question. Changes in the reputation system, such as adding a higher ceiling in the feedback scores, may temporarily mitigate—but do not solve—the problem.\n\n# Acknowledgments\n\nThe authors thank David Holtz, Ramesh Johari, Nico Lace-tra, Xiao Ma, Foster Provost, Aaron Sojourner, and Richard Zeckhauser for very helpful comments and suggestions.\n\n# Endnotes\n\n1. We use the terms “employer” and “worker” for consistency with the literature, and not as a comment on the legal relationship of the transacting parties.\n2. We use the present tense here to describe the reputation system before the introduction of private feedback. Although our focus is on employer-on-worker feedback, our claims carry through to the equally important case of worker-on-employer feedback (Benson et al. 2020).\n3. We use this $10 restriction throughout the paper to remove mistaken, trial, and erroneous transactions.\n4. If this was not the case, then there would be some u′ such that E[η | u′ ]  k and k ≠ 0. But E[η | u]  E[η | α−1(u)]  k, which contradicts Assumption 3.\n5. Other approaches—which we do not take in this paper—would be to reduce bias consumer satisfaction estimates directly (Huang and Sudhir 2019) or to estimate structural models of the value of reputation across different time periods (Yoganarasimhan 2013).\n6. One written feedback in our data reads, “This is the most impressive piece of coding in the history of software development!”\n7. See also this link.\n8. For example, officer evaluation reports in the U.S. Army limit senior raters to indicating only 50% or less of the officers they rate as “most qualified.” However, it may be challenging to force a distribution in settings where buyers evaluate sellers as a “flow.”\n\n# References\n\nAperjis C, Johari R (2010) Optimal windows for aggregating ratings in electronic marketplaces. Management Sci. 56(5):864–880.\n\nAthey S, Castillo JC, Chandar B (2019) Service quality in the gig economy: Empirical evidence about driving quality at Uber. Preprint, submitted December 28, http://dx.doi.org/10.2139/ssrn.3499781.\n\nBabcock P (2010) Real costs of nominal grade inflation? New evidence from student course evaluations. Econom. Inquiry 48(4): 983–996.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 365,
          "y": 42.87199999999996,
          "w": 173.77079265999998,
          "h": 7.471800000000002
        }
      },
      {
        "type": "text",
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\nHowever, this is a misleading analogy. Unlike in monetary systems where there is no highest price, ratings systems are always on a top-censored scale: for the question “rate on a scale from one to X,” the value of X must be prespecified. This is why reputation inflation differs from monetary inflation: a sandwich that used to cost $0.50 and may now cost $12. However, this necessary increase could not happen if price was mechanically restricted to be below $1. As such, reputation inflation leads to pooling of feedback scores in the highest feedback bin. This pooling makes it difficult to distinguish “excellent” from simply “good,” and with sufficient inflation, the reputation system could become nearly binary, with the only possible signals being “terrible” and “not terrible.” In our context, 85% of the users receive a perfect rating at the end of our sample (see Figure 1(c)), even though it is highly unlikely that 85% of transactions result in the exact same employer satisfaction.\n\nEven if market participants and the platform are aware of the inflation, pooling is difficult or even impossible to correct statistically. With pooling, the strictly monotone relationship between rater satisfaction and scores is lost and presents the same problem as grade inflation (Babcock 2010, Butcher et al. 2014). Furthermore, a strong rate of inflation—even if episodic and then contained—can cause individual reputations to vary based on when a feedback score was assigned, in turn undermining the usefulness of comparisons of feedback scores across different time periods. Aside from the effects on market participants, reputation inflation causes the platform itself to lose a yardstick for measuring its own performance.",
        "md": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\nHowever, this is a misleading analogy. Unlike in monetary systems where there is no highest price, ratings systems are always on a top-censored scale: for the question “rate on a scale from one to X,” the value of X must be prespecified. This is why reputation inflation differs from monetary inflation: a sandwich that used to cost $0.50 and may now cost $12. However, this necessary increase could not happen if price was mechanically restricted to be below $1. As such, reputation inflation leads to pooling of feedback scores in the highest feedback bin. This pooling makes it difficult to distinguish “excellent” from simply “good,” and with sufficient inflation, the reputation system could become nearly binary, with the only possible signals being “terrible” and “not terrible.” In our context, 85% of the users receive a perfect rating at the end of our sample (see Figure 1(c)), even though it is highly unlikely that 85% of transactions result in the exact same employer satisfaction.\n\nEven if market participants and the platform are aware of the inflation, pooling is difficult or even impossible to correct statistically. With pooling, the strictly monotone relationship between rater satisfaction and scores is lost and presents the same problem as grade inflation (Babcock 2010, Butcher et al. 2014). Furthermore, a strong rate of inflation—even if episodic and then contained—can cause individual reputations to vary based on when a feedback score was assigned, in turn undermining the usefulness of comparisons of feedback scores across different time periods. Aside from the effects on market participants, reputation inflation causes the platform itself to lose a yardstick for measuring its own performance.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 238.4443516,
          "h": 9.962600000000009
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "6. Conclusion",
        "md": "# 6. Conclusion",
        "bBox": {
          "x": 45,
          "y": 491.87199999999996,
          "w": 79.4177293,
          "h": 12
        }
      },
      {
        "type": "text",
        "value": "This paper documents that the reputation system in an online marketplace was subject to inflation—we observe systematically higher scores over time, which cannot be fully explained by improvements in fundamentals. Data from four other marketplaces exhibit the same trend, suggesting that reputation inflation is a widespread problem.\n\nFor would-be marketplace designers, our paper illustrates a core market design problem. The divergence of public and private feedback scores in our data suggests that a possible mechanism driving reputation inflation is that raters incur a greater personal cost—or guilt—the greater the harm they impose on the rated worker. An interesting next step would be to elucidate the root causes of reputation inflation.\n\nWhether there are effective market design responses to reputation inflation is an open question. Changes in the reputation system, such as adding a higher ceiling in the feedback scores, may temporarily mitigate—but do not solve—the problem.",
        "md": "This paper documents that the reputation system in an online marketplace was subject to inflation—we observe systematically higher scores over time, which cannot be fully explained by improvements in fundamentals. Data from four other marketplaces exhibit the same trend, suggesting that reputation inflation is a widespread problem.\n\nFor would-be marketplace designers, our paper illustrates a core market design problem. The divergence of public and private feedback scores in our data suggests that a possible mechanism driving reputation inflation is that raters incur a greater personal cost—or guilt—the greater the harm they impose on the rated worker. An interesting next step would be to elucidate the root causes of reputation inflation.\n\nWhether there are effective market design responses to reputation inflation is an open question. Changes in the reputation system, such as adding a higher ceiling in the feedback scores, may temporarily mitigate—but do not solve—the problem.",
        "bBox": {
          "x": 45,
          "y": 503.87199999999996,
          "w": 238.29376790000003,
          "h": 9.9626
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Acknowledgments",
        "md": "# Acknowledgments",
        "bBox": {
          "x": 301,
          "y": 292.87199999999996,
          "w": 88.01957100000004,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "The authors thank David Holtz, Ramesh Johari, Nico Lace-tra, Xiao Ma, Foster Provost, Aaron Sojourner, and Richard Zeckhauser for very helpful comments and suggestions.",
        "md": "The authors thank David Holtz, Ramesh Johari, Nico Lace-tra, Xiao Ma, Foster Provost, Aaron Sojourner, and Richard Zeckhauser for very helpful comments and suggestions.",
        "bBox": {
          "x": 55,
          "y": 303.87199999999996,
          "w": 483.52356340999995,
          "h": 9.962599999999952
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Endnotes",
        "md": "# Endnotes",
        "bBox": {
          "x": 301,
          "y": 348.87199999999996,
          "w": 45.38960559999998,
          "h": 9.962600000000009
        }
      },
      {
        "type": "text",
        "value": "1. We use the terms “employer” and “worker” for consistency with the literature, and not as a comment on the legal relationship of the transacting parties.\n2. We use the present tense here to describe the reputation system before the introduction of private feedback. Although our focus is on employer-on-worker feedback, our claims carry through to the equally important case of worker-on-employer feedback (Benson et al. 2020).\n3. We use this $10 restriction throughout the paper to remove mistaken, trial, and erroneous transactions.\n4. If this was not the case, then there would be some u′ such that E[η | u′ ]  k and k ≠ 0. But E[η | u]  E[η | α−1(u)]  k, which contradicts Assumption 3.\n5. Other approaches—which we do not take in this paper—would be to reduce bias consumer satisfaction estimates directly (Huang and Sudhir 2019) or to estimate structural models of the value of reputation across different time periods (Yoganarasimhan 2013).\n6. One written feedback in our data reads, “This is the most impressive piece of coding in the history of software development!”\n7. See also this link.\n8. For example, officer evaluation reports in the U.S. Army limit senior raters to indicating only 50% or less of the officers they rate as “most qualified.” However, it may be challenging to force a distribution in settings where buyers evaluate sellers as a “flow.”",
        "md": "1. We use the terms “employer” and “worker” for consistency with the literature, and not as a comment on the legal relationship of the transacting parties.\n2. We use the present tense here to describe the reputation system before the introduction of private feedback. Although our focus is on employer-on-worker feedback, our claims carry through to the equally important case of worker-on-employer feedback (Benson et al. 2020).\n3. We use this $10 restriction throughout the paper to remove mistaken, trial, and erroneous transactions.\n4. If this was not the case, then there would be some u′ such that E[η | u′ ]  k and k ≠ 0. But E[η | u]  E[η | α−1(u)]  k, which contradicts Assumption 3.\n5. Other approaches—which we do not take in this paper—would be to reduce bias consumer satisfaction estimates directly (Huang and Sudhir 2019) or to estimate structural models of the value of reputation across different time periods (Yoganarasimhan 2013).\n6. One written feedback in our data reads, “This is the most impressive piece of coding in the history of software development!”\n7. See also this link.\n8. For example, officer evaluation reports in the U.S. Army limit senior raters to indicating only 50% or less of the officers they rate as “most qualified.” However, it may be challenging to force a distribution in settings where buyers evaluate sellers as a “flow.”",
        "bBox": {
          "x": 55,
          "y": 372.87199999999996,
          "w": 483.59405305999996,
          "h": 9.9626
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "References",
        "md": "# References",
        "bBox": {
          "x": 301,
          "y": 643.872,
          "w": 53.70837660000001,
          "h": 9.962599999999952
        }
      },
      {
        "type": "text",
        "value": "Aperjis C, Johari R (2010) Optimal windows for aggregating ratings in electronic marketplaces. Management Sci. 56(5):864–880.\n\nAthey S, Castillo JC, Chandar B (2019) Service quality in the gig economy: Empirical evidence about driving quality at Uber. Preprint, submitted December 28, http://dx.doi.org/10.2139/ssrn.3499781.\n\nBabcock P (2010) Real costs of nominal grade inflation? New evidence from student course evaluations. Econom. Inquiry 48(4): 983–996.",
        "md": "Aperjis C, Johari R (2010) Optimal windows for aggregating ratings in electronic marketplaces. Management Sci. 56(5):864–880.\n\nAthey S, Castillo JC, Chandar B (2019) Service quality in the gig economy: Empirical evidence about driving quality at Uber. Preprint, submitted December 28, http://dx.doi.org/10.2139/ssrn.3499781.\n\nBabcock P (2010) Real costs of nominal grade inflation? New evidence from student course evaluations. Econom. Inquiry 48(4): 983–996.",
        "bBox": {
          "x": 55,
          "y": 588.872,
          "w": 237.6020232599999,
          "h": 9.962599999999952
        }
      }
    ]
  },
  {
    "page": 14,
    "text": "                Filippas, Horton, and Golden: Reputation Inflation\n                Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS         13\n\nBenson A, Sojourner A, Umyarov A (2020) Can reputation discipline\n      the gig economy? Experimental evidence from an online labor\n      market. Management Sci. 66(5):1802–1825.\nBerentsen A, Menzio G, Wright R (2011) Inflation and unemploy-\n      ment in the long run. Amer. Econom. Rev. 101(1):371–398.\nBolton G, Greiner B, Ockenfels A (2013) Engineering trust: Reci-\n      procity in the production of reputation information.                   Manage-\n      ment Sci. 59(2):265–285.\nButcher KF, McEwan PJ, Weerapana A (2014) The effects of an anti-\n      grade-inflation policy at Wellesley College. J. Econom. Perspect.\n      28(3):189–204.\nCampbell DT (1979) Assessing the impact of planned social change.\n      Evaluation Program Planning 2(1):67–90.\nDellarocas C (2005) Reputation mechanism design in online trading\n      environments with pure moral hazard.                   Inform. Systems Res.\n      16(2):209–230.\nDellarocas C, Wood CA (2008) The sound of silence in online feed-\n      back: Estimating trading risks in the presence of reporting bias.\n      Management Sci. 54(3):460–476.\nDiewert WE (1998) Index number issues in the consumer price\n      index. J. Econom. Perspect. 12(1):47–58.\nFilippas A, Horton JJ (2018) The tragedy of your upstairs neighbors:\n      Externalities of home-sharing. Technical report.\nFilippas A, Horton JJ, Zeckhauser RJ (2020) Owning, using, and\n      renting: Some simple economics of the                  “sharing economy.”\n      Management Sci. 66(9):4152–4172.\nFilippas A, Jagabathula S, Sundararajan A (2021) The limits of cen-\n      tralized pricing in online marketplaces and the value of user\n      control. Working paper.\nFradkin A, Grewal E, Holtz D (2019) Reciprocity in two-sided\n      reputation systems: Evidence from an experiment on Airbnb.\n      Working paper, https://pubsonline.informs.org/doi/abs/10.\n      1287/mksc.2021.1311.\nFriedman M (1977) Nobel lecture: Inflation and unemployment.\n      J. Political Econom.    85(3):451–472.\nGarg N, Johari R (2020) Designing informative rating systems: Evidence\n      from an online labor market.         Bir´o P (chair), Hartline J (chair), eds.\n      Proc. 21st ACM Conf. Econom. Comput. (Association for Computing\n      Machinery, New York), 71.\nGhose A, Ipeirotis PG, Beibei Li (2014) Examining the impact of\n      ranking     on consumer        behavior     and search      engine    revenue.\n      Management Sci. 60(7):1632–1654.\nHall JV, Krueger AB (2018) An analysis of the labor market for\n      Uber’s driver-partners in the United States.              Indust. Labor Rela-\n      tions Rev. 71(3):705–732.\nHall JV, Horton JJ, Knoepfle DT (2021) Pricing in designed markets:\n      The case of ride-sharing. Working paper, Uber Technologies,\n      San Francisco.\nHorton J (2010) Online labor markets. Saberi A, eds. Internet and\n      Network Economics (Springer, Berlin), 515–522.\nHorton JJ, Tambe P (2015) Labor economists get their microscope:\n      big data and labor market analysis. Big Data 3(3):130–137.\nHorton JJ, Rand DG, Zeckhauser RJ (2011) The online laboratory:\n      Conducting experiments in a real labor market.                     Experiment.\n      Econom. 14(3):399–425.\nHu N, Pavlou PA, Zhang J (2017) On self-selection biases in online\n      product reviews. MIS Quart. 41(2):449–471.\nHuang G, Sudhir K (2019) The causal effect of service satisfaction\n      on customer loyalty. Preprint, submitted June 6, http://dx.doi.\n      org/10.2139/ssrn.3391242.\nJin GZ, Leslie P (2003) The effect of information on product quality:\n      Evidence from restaurant hygiene grade cards. Quart. J. Econom.\n      118(2):409–451.\nKatz LF, Krueger AB (2019) The rise and nature of alternative work\n      arrangements       in the United        States,  1995–2015.      Indust.   Labor\n      Relations Rev. 72(2):382–416.\nLevin JD (2011) The economics of internet markets. NBER Working\n      Paper No. 16852, National Bureau of Economic Research, Cam-\n      bridge, MA.\nLiu Q (2011) Information acquisition and reputation dynamics. Rev.\n      Econom. Stud. 78(4):1400–1425.\nLuca M (2016) Reviews, reputation, and revenue: The case of Yelp.\n      com. Working paper, Harvard Business School, Boston, MA.\nLuca M, Zervas G (2016) Fake it till you make it: Reputation, compe-\n      tition, and Yelp review fraud. Management Sci. 62(12):3412–3427.\nMayzlin D, Dover Y, Chevalier J (2014) Promotional reviews: An\n      empirical investigation of online review manipulation.                    Amer.\n      Econom. Rev. 104(8):2421–2455.\nMishkin FS (2000) Inflation targeting in emerging-market countries.\n      Amer. Econom. Rev. 90(2):105–109.\nNosko C, Tadelis S (2015) The limits of reputation in platform mar-\n      kets: An empirical analysis and field experiment. NBER Work-\n      ing Paper No. 20830, National Bureau of Economic Research,\n      Cambridge, MA.\nResnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputa-\n      tion systems. Commu ACM 43(12):45–48.\nResnick P, Zeckhauser R, Swanson J, Lockwood K (2006) The value\n      of reputation on eBay: A controlled experiment.                    Experiment.\n      Econom. 9(2):79–101.\nSidrauski     M    (1967)   Inflation    and    economic      growth.     J.  Political\n      Econom. 75(6):796–810.\nSundararajan A (2013) From Zipcar to the sharing economy. Harvard\n      Bus. Rev. https://hbr.org/2013/01/from-zipcar-to-the-sharing-eco.\nTadelis S (2016) Reputation and feedback systems in online platform\n      markets. Annual Rev. Econom. 8:321–340.\nTadelis S, Zettelmeyer F (2015) Information disclosure as a matching\n      mechanism:       Theory     and   evidence     from    a   field  experiment.\n      Amer. Econom. Rev. 105(2):886–905.\nYoganarasimhan H (2013) The value of reputation in an online free-\n      lance marketplace. Marketing Sci. 32(6):860–891.",
    "md": "# Filippas, Horton, and Golden: Reputation Inflation\n\n# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS\n\n# References\n\n- Benson A, Sojourner A, Umyarov A (2020) Can reputation discipline the gig economy? Experimental evidence from an online labor market. Management Sci. 66(5):1802–1825.\n- Berentsen A, Menzio G, Wright R (2011) Inflation and unemployment in the long run. Amer. Econom. Rev. 101(1):371–398.\n- Bolton G, Greiner B, Ockenfels A (2013) Engineering trust: Reciprocity in the production of reputation information. Management Sci. 59(2):265–285.\n- Butcher KF, McEwan PJ, Weerapana A (2014) The effects of an anti-grade-inflation policy at Wellesley College. J. Econom. Perspect. 28(3):189–204.\n- Campbell DT (1979) Assessing the impact of planned social change. Evaluation Program Planning 2(1):67–90.\n- Dellarocas C (2005) Reputation mechanism design in online trading environments with pure moral hazard. Inform. Systems Res. 16(2):209–230.\n- Dellarocas C, Wood CA (2008) The sound of silence in online feedback: Estimating trading risks in the presence of reporting bias. Management Sci. 54(3):460–476.\n- Diewert WE (1998) Index number issues in the consumer price index. J. Econom. Perspect. 12(1):47–58.\n- Filippas A, Horton JJ (2018) The tragedy of your upstairs neighbors: Externalities of home-sharing. Technical report.\n- Filippas A, Horton JJ, Zeckhauser RJ (2020) Owning, using, and renting: Some simple economics of the “sharing economy.” Management Sci. 66(9):4152–4172.\n- Filippas A, Jagabathula S, Sundararajan A (2021) The limits of centralized pricing in online marketplaces and the value of user control. Working paper.\n- Fradkin A, Grewal E, Holtz D (2019) Reciprocity in two-sided reputation systems: Evidence from an experiment on Airbnb. Working paper, https://pubsonline.informs.org/doi/abs/10.1287/mksc.2021.1311.\n- Friedman M (1977) Nobel lecture: Inflation and unemployment. J. Political Econom. 85(3):451–472.\n- Garg N, Johari R (2020) Designing informative rating systems: Evidence from an online labor market. Biró P (chair), Hartline J (chair), eds. Proc. 21st ACM Conf. Econom. Comput. (Association for Computing Machinery, New York), 71.\n- Ghose A, Ipeirotis PG, Beibei Li (2014) Examining the impact of ranking on consumer behavior and search engine revenue. Management Sci. 60(7):1632–1654.\n- Hall JV, Krueger AB (2018) An analysis of the labor market for Uber’s driver-partners in the United States. Indust. Labor Relations Rev. 71(3):705–732.\n- Hall JV, Horton JJ, Knoepfle DT (2021) Pricing in designed markets: The case of ride-sharing. Working paper, Uber Technologies, San Francisco.\n- Horton J (2010) Online labor markets. Saberi A, eds. Internet and Network Economics (Springer, Berlin), 515–522.\n- Horton JJ, Tambe P (2015) Labor economists get their microscope: big data and labor market analysis. Big Data 3(3):130–137.\n- Horton JJ, Rand DG, Zeckhauser RJ (2011) The online laboratory: Conducting experiments in a real labor market. Experiment. Econom. 14(3):399–425.\n- Hu N, Pavlou PA, Zhang J (2017) On self-selection biases in online product reviews. MIS Quart. 41(2):449–471.\n- Huang G, Sudhir K (2019) The causal effect of service satisfaction on customer loyalty. Preprint, submitted June 6, http://dx.doi.org/10.2139/ssrn.3391242.\n- Jin GZ, Leslie P (2003) The effect of information on product quality: Evidence from restaurant hygiene grade cards. Quart. J. Econom. 118(2):409–451.\n- Katz LF, Krueger AB (2019) The rise and nature of alternative work arrangements in the United States, 1995–2015. Indust. Labor Relations Rev. 72(2):382–416.\n- Levin JD (2011) The economics of internet markets. NBER Working Paper No. 16852, National Bureau of Economic Research, Cambridge, MA.\n- Liu Q (2011) Information acquisition and reputation dynamics. Rev. Econom. Stud. 78(4):1400–1425.\n- Luca M (2016) Reviews, reputation, and revenue: The case of Yelp.com. Working paper, Harvard Business School, Boston, MA.\n- Luca M, Zervas G (2016) Fake it till you make it: Reputation, competition, and Yelp review fraud. Management Sci. 62(12):3412–3427.\n- Mayzlin D, Dover Y, Chevalier J (2014) Promotional reviews: An empirical investigation of online review manipulation. Amer. Econom. Rev. 104(8):2421–2455.\n- Mishkin FS (2000) Inflation targeting in emerging-market countries. Amer. Econom. Rev. 90(2):105–109.\n- Nosko C, Tadelis S (2015) The limits of reputation in platform markets: An empirical analysis and field experiment. NBER Working Paper No. 20830, National Bureau of Economic Research, Cambridge, MA.\n- Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commu ACM 43(12):45–48.\n- Resnick P, Zeckhauser R, Swanson J, Lockwood K (2006) The value of reputation on eBay: A controlled experiment. Experiment. Econom. 9(2):79–101.\n- Sidrauski M (1967) Inflation and economic growth. J. Political Econom. 75(6):796–810.\n- Sundararajan A (2013) From Zipcar to the sharing economy. Harvard Bus. Rev. https://hbr.org/2013/01/from-zipcar-to-the-sharing-eco.\n- Tadelis S (2016) Reputation and feedback systems in online platform markets. Annual Rev. Econom. 8:321–340.\n- Tadelis S, Zettelmeyer F (2015) Information disclosure as a matching mechanism: Theory and evidence from a field experiment. Amer. Econom. Rev. 105(2):886–905.\n- Yoganarasimhan H (2013) The value of reputation in an online freelance marketplace. Marketing Sci. 32(6):860–891.",
    "images": [],
    "items": [
      {
        "type": "heading",
        "lvl": 1,
        "value": "Filippas, Horton, and Golden: Reputation Inflation",
        "md": "# Filippas, Horton, and Golden: Reputation Inflation",
        "bBox": {
          "x": 45,
          "y": 42.87199999999996,
          "w": 173.7827475399999,
          "h": 7.970199999999977
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "md": "# Marketing Science, Articles in Advance, pp. 1–13, © 2022 INFORMS",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 200.50380013000014,
          "h": 9.9626
        }
      },
      {
        "type": "heading",
        "lvl": 1,
        "value": "References",
        "md": "# References",
        "bBox": {
          "x": 0,
          "y": 0,
          "w": 584.674
        }
      },
      {
        "type": "text",
        "value": "- Benson A, Sojourner A, Umyarov A (2020) Can reputation discipline the gig economy? Experimental evidence from an online labor market. Management Sci. 66(5):1802–1825.\n- Berentsen A, Menzio G, Wright R (2011) Inflation and unemployment in the long run. Amer. Econom. Rev. 101(1):371–398.\n- Bolton G, Greiner B, Ockenfels A (2013) Engineering trust: Reciprocity in the production of reputation information. Management Sci. 59(2):265–285.\n- Butcher KF, McEwan PJ, Weerapana A (2014) The effects of an anti-grade-inflation policy at Wellesley College. J. Econom. Perspect. 28(3):189–204.\n- Campbell DT (1979) Assessing the impact of planned social change. Evaluation Program Planning 2(1):67–90.\n- Dellarocas C (2005) Reputation mechanism design in online trading environments with pure moral hazard. Inform. Systems Res. 16(2):209–230.\n- Dellarocas C, Wood CA (2008) The sound of silence in online feedback: Estimating trading risks in the presence of reporting bias. Management Sci. 54(3):460–476.\n- Diewert WE (1998) Index number issues in the consumer price index. J. Econom. Perspect. 12(1):47–58.\n- Filippas A, Horton JJ (2018) The tragedy of your upstairs neighbors: Externalities of home-sharing. Technical report.\n- Filippas A, Horton JJ, Zeckhauser RJ (2020) Owning, using, and renting: Some simple economics of the “sharing economy.” Management Sci. 66(9):4152–4172.\n- Filippas A, Jagabathula S, Sundararajan A (2021) The limits of centralized pricing in online marketplaces and the value of user control. Working paper.\n- Fradkin A, Grewal E, Holtz D (2019) Reciprocity in two-sided reputation systems: Evidence from an experiment on Airbnb. Working paper, https://pubsonline.informs.org/doi/abs/10.1287/mksc.2021.1311.\n- Friedman M (1977) Nobel lecture: Inflation and unemployment. J. Political Econom. 85(3):451–472.\n- Garg N, Johari R (2020) Designing informative rating systems: Evidence from an online labor market. Biró P (chair), Hartline J (chair), eds. Proc. 21st ACM Conf. Econom. Comput. (Association for Computing Machinery, New York), 71.\n- Ghose A, Ipeirotis PG, Beibei Li (2014) Examining the impact of ranking on consumer behavior and search engine revenue. Management Sci. 60(7):1632–1654.\n- Hall JV, Krueger AB (2018) An analysis of the labor market for Uber’s driver-partners in the United States. Indust. Labor Relations Rev. 71(3):705–732.\n- Hall JV, Horton JJ, Knoepfle DT (2021) Pricing in designed markets: The case of ride-sharing. Working paper, Uber Technologies, San Francisco.\n- Horton J (2010) Online labor markets. Saberi A, eds. Internet and Network Economics (Springer, Berlin), 515–522.\n- Horton JJ, Tambe P (2015) Labor economists get their microscope: big data and labor market analysis. Big Data 3(3):130–137.\n- Horton JJ, Rand DG, Zeckhauser RJ (2011) The online laboratory: Conducting experiments in a real labor market. Experiment. Econom. 14(3):399–425.\n- Hu N, Pavlou PA, Zhang J (2017) On self-selection biases in online product reviews. MIS Quart. 41(2):449–471.\n- Huang G, Sudhir K (2019) The causal effect of service satisfaction on customer loyalty. Preprint, submitted June 6, http://dx.doi.org/10.2139/ssrn.3391242.\n- Jin GZ, Leslie P (2003) The effect of information on product quality: Evidence from restaurant hygiene grade cards. Quart. J. Econom. 118(2):409–451.\n- Katz LF, Krueger AB (2019) The rise and nature of alternative work arrangements in the United States, 1995–2015. Indust. Labor Relations Rev. 72(2):382–416.\n- Levin JD (2011) The economics of internet markets. NBER Working Paper No. 16852, National Bureau of Economic Research, Cambridge, MA.\n- Liu Q (2011) Information acquisition and reputation dynamics. Rev. Econom. Stud. 78(4):1400–1425.\n- Luca M (2016) Reviews, reputation, and revenue: The case of Yelp.com. Working paper, Harvard Business School, Boston, MA.\n- Luca M, Zervas G (2016) Fake it till you make it: Reputation, competition, and Yelp review fraud. Management Sci. 62(12):3412–3427.\n- Mayzlin D, Dover Y, Chevalier J (2014) Promotional reviews: An empirical investigation of online review manipulation. Amer. Econom. Rev. 104(8):2421–2455.\n- Mishkin FS (2000) Inflation targeting in emerging-market countries. Amer. Econom. Rev. 90(2):105–109.\n- Nosko C, Tadelis S (2015) The limits of reputation in platform markets: An empirical analysis and field experiment. NBER Working Paper No. 20830, National Bureau of Economic Research, Cambridge, MA.\n- Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commu ACM 43(12):45–48.\n- Resnick P, Zeckhauser R, Swanson J, Lockwood K (2006) The value of reputation on eBay: A controlled experiment. Experiment. Econom. 9(2):79–101.\n- Sidrauski M (1967) Inflation and economic growth. J. Political Econom. 75(6):796–810.\n- Sundararajan A (2013) From Zipcar to the sharing economy. Harvard Bus. Rev. https://hbr.org/2013/01/from-zipcar-to-the-sharing-eco.\n- Tadelis S (2016) Reputation and feedback systems in online platform markets. Annual Rev. Econom. 8:321–340.\n- Tadelis S, Zettelmeyer F (2015) Information disclosure as a matching mechanism: Theory and evidence from a field experiment. Amer. Econom. Rev. 105(2):886–905.\n- Yoganarasimhan H (2013) The value of reputation in an online freelance marketplace. Marketing Sci. 32(6):860–891.",
        "md": "- Benson A, Sojourner A, Umyarov A (2020) Can reputation discipline the gig economy? Experimental evidence from an online labor market. Management Sci. 66(5):1802–1825.\n- Berentsen A, Menzio G, Wright R (2011) Inflation and unemployment in the long run. Amer. Econom. Rev. 101(1):371–398.\n- Bolton G, Greiner B, Ockenfels A (2013) Engineering trust: Reciprocity in the production of reputation information. Management Sci. 59(2):265–285.\n- Butcher KF, McEwan PJ, Weerapana A (2014) The effects of an anti-grade-inflation policy at Wellesley College. J. Econom. Perspect. 28(3):189–204.\n- Campbell DT (1979) Assessing the impact of planned social change. Evaluation Program Planning 2(1):67–90.\n- Dellarocas C (2005) Reputation mechanism design in online trading environments with pure moral hazard. Inform. Systems Res. 16(2):209–230.\n- Dellarocas C, Wood CA (2008) The sound of silence in online feedback: Estimating trading risks in the presence of reporting bias. Management Sci. 54(3):460–476.\n- Diewert WE (1998) Index number issues in the consumer price index. J. Econom. Perspect. 12(1):47–58.\n- Filippas A, Horton JJ (2018) The tragedy of your upstairs neighbors: Externalities of home-sharing. Technical report.\n- Filippas A, Horton JJ, Zeckhauser RJ (2020) Owning, using, and renting: Some simple economics of the “sharing economy.” Management Sci. 66(9):4152–4172.\n- Filippas A, Jagabathula S, Sundararajan A (2021) The limits of centralized pricing in online marketplaces and the value of user control. Working paper.\n- Fradkin A, Grewal E, Holtz D (2019) Reciprocity in two-sided reputation systems: Evidence from an experiment on Airbnb. Working paper, https://pubsonline.informs.org/doi/abs/10.1287/mksc.2021.1311.\n- Friedman M (1977) Nobel lecture: Inflation and unemployment. J. Political Econom. 85(3):451–472.\n- Garg N, Johari R (2020) Designing informative rating systems: Evidence from an online labor market. Biró P (chair), Hartline J (chair), eds. Proc. 21st ACM Conf. Econom. Comput. (Association for Computing Machinery, New York), 71.\n- Ghose A, Ipeirotis PG, Beibei Li (2014) Examining the impact of ranking on consumer behavior and search engine revenue. Management Sci. 60(7):1632–1654.\n- Hall JV, Krueger AB (2018) An analysis of the labor market for Uber’s driver-partners in the United States. Indust. Labor Relations Rev. 71(3):705–732.\n- Hall JV, Horton JJ, Knoepfle DT (2021) Pricing in designed markets: The case of ride-sharing. Working paper, Uber Technologies, San Francisco.\n- Horton J (2010) Online labor markets. Saberi A, eds. Internet and Network Economics (Springer, Berlin), 515–522.\n- Horton JJ, Tambe P (2015) Labor economists get their microscope: big data and labor market analysis. Big Data 3(3):130–137.\n- Horton JJ, Rand DG, Zeckhauser RJ (2011) The online laboratory: Conducting experiments in a real labor market. Experiment. Econom. 14(3):399–425.\n- Hu N, Pavlou PA, Zhang J (2017) On self-selection biases in online product reviews. MIS Quart. 41(2):449–471.\n- Huang G, Sudhir K (2019) The causal effect of service satisfaction on customer loyalty. Preprint, submitted June 6, http://dx.doi.org/10.2139/ssrn.3391242.\n- Jin GZ, Leslie P (2003) The effect of information on product quality: Evidence from restaurant hygiene grade cards. Quart. J. Econom. 118(2):409–451.\n- Katz LF, Krueger AB (2019) The rise and nature of alternative work arrangements in the United States, 1995–2015. Indust. Labor Relations Rev. 72(2):382–416.\n- Levin JD (2011) The economics of internet markets. NBER Working Paper No. 16852, National Bureau of Economic Research, Cambridge, MA.\n- Liu Q (2011) Information acquisition and reputation dynamics. Rev. Econom. Stud. 78(4):1400–1425.\n- Luca M (2016) Reviews, reputation, and revenue: The case of Yelp.com. Working paper, Harvard Business School, Boston, MA.\n- Luca M, Zervas G (2016) Fake it till you make it: Reputation, competition, and Yelp review fraud. Management Sci. 62(12):3412–3427.\n- Mayzlin D, Dover Y, Chevalier J (2014) Promotional reviews: An empirical investigation of online review manipulation. Amer. Econom. Rev. 104(8):2421–2455.\n- Mishkin FS (2000) Inflation targeting in emerging-market countries. Amer. Econom. Rev. 90(2):105–109.\n- Nosko C, Tadelis S (2015) The limits of reputation in platform markets: An empirical analysis and field experiment. NBER Working Paper No. 20830, National Bureau of Economic Research, Cambridge, MA.\n- Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commu ACM 43(12):45–48.\n- Resnick P, Zeckhauser R, Swanson J, Lockwood K (2006) The value of reputation on eBay: A controlled experiment. Experiment. Econom. 9(2):79–101.\n- Sidrauski M (1967) Inflation and economic growth. J. Political Econom. 75(6):796–810.\n- Sundararajan A (2013) From Zipcar to the sharing economy. Harvard Bus. Rev. https://hbr.org/2013/01/from-zipcar-to-the-sharing-eco.\n- Tadelis S (2016) Reputation and feedback systems in online platform markets. Annual Rev. Econom. 8:321–340.\n- Tadelis S, Zettelmeyer F (2015) Information disclosure as a matching mechanism: Theory and evidence from a field experiment. Amer. Econom. Rev. 105(2):886–905.\n- Yoganarasimhan H (2013) The value of reputation in an online freelance marketplace. Marketing Sci. 32(6):860–891.",
        "bBox": {
          "x": 45,
          "y": 51.87199999999996,
          "w": 493.9626,
          "h": 9.9626
        }
      }
    ]
  }
]
